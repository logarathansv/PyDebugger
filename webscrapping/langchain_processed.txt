page: https://python.langchain.com/docs/versions/v0_3/
missing_data
page: https://python.langchain.com/v0.2/api_reference/openai/index.html
langchain-openai: 0.1.24 # chat_models # class chat_models.azure.azurechatopenai azure openai chat model integration. chat_models.base.basechatopenai field : chat_models.base.chatopenai openai chat model integration. chat_models.base.openairefusalerror error raised openai structured output api return refusal. embeddings # class embeddings.azure.azureopenaiembeddings azureopenai embedding model integration. embeddings.base.openaiembeddings openai embedding model integration. llm # class llms.azure.azureopenai azure-specific openai large language models. llms.base.baseopenai base openai large language model class. llms.base.openai openai completion model integration.
page: https://python.langchain.com/v0.2/api_reference/openai/chat_models.html#langchain-openai-chat-models
chat_models # class chat_models.azure.azurechatopenai azure openai chat model integration. chat_models.base.basechatopenai field : chat_models.base.chatopenai openai chat model integration. chat_models.base.openairefusalerror error raised openai structured output api return refusal.
page: https://python.langchain.com/v0.2/api_reference/openai/chat_models/langchain_openai.chat_models.azure.azurechatopenai.html#langchain_openai.chat_models.azure.azurechatopenai
azurechatopenai # class langchain_openai.chat_models.azure. azurechatopenai [source] # bases: basechatopenai azure openai chat model integration. setup: head https://learn.microsoft.com/en-us/azure/ai-services/openai/chatgpt-quickstart?tabs=command-line%2cpython-new&pivots=programming-language-python create azure openai deployment. install langchain-openai set environment variable azure_openai_api_key azure_openai_endpoint : pip install -u langchain-openai export azure_openai_api_key = "your-api-key" export azure_openai_endpoint = "https://your-endpoint.openai.azure.com/" key init args — completion params: azure_deployment: str name azure openai deployment use. temperature: float sampling temperature. max_tokens: optional[int] max number token generate. logprobs: optional[bool] whether return logprobs. key init args — client params: api_version: str azure openai api version use. see different version here: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning timeout: union[float, tuple[float, float], any, none] timeout requests. max_retries: int max number retries. organization: optional[str] openai organization id. passed read env
var openai_org_id. model: optional[str] name underlying openai model. used tracing token
counting. affect completion. e.g. “gpt-4”, “gpt-35-turbo”, etc. model_version: optional[str] version underlying openai model. used tracing token
counting. affect completion. e.g., “0125”, “0125-preview”, etc. see full list supported init args description params section. instantiate: langchain_openai import azurechatopenai llm = azurechatopenai ( azure_deployment = "your-deployment" , api_version = "2024-05-01-preview" , temperature = 0 , max_tokens = none , timeout = none , max_retries = 2 , # organization="...", # model="gpt-35-turbo", # model_version="0125", # params... ) note : param explicitly supported passed directly openai.azureopenai.chat.completions.create(...) api every time model
invoked. example: langchain_openai import azurechatopenai import openai azurechatopenai ( ... , logprobs = true ) . invoke ( ... ) # result underlying api call of: openai . azureopenai ( .. ) . chat . completion . create ( ... , logprobs = true ) # also equivalent to: azurechatopenai ( ... ) . invoke ( ... , logprobs = true ) invoke: message = [ ( "system" , "you helpful translator. translate user sentence french." , ), ( "human" , "i love programming." ), ] llm . invoke ( message ) aimessage ( content = "j'adore programmer." , usage_metadata = { "input_tokens" : 28 , "output_tokens" : 6 , "total_tokens" : 34 }, response_metadata = { "token_usage" : { "completion_tokens" : 6 , "prompt_tokens" : 28 , "total_tokens" : 34 , }, "model_name" : "gpt-4" , "system_fingerprint" : "fp_7ec89fabc6" , "prompt_filter_results" : [ { "prompt_index" : 0 , "content_filter_results" : { "hate" : { "filtered" : false , "severity" : "safe" }, "self_harm" : { "filtered" : false , "severity" : "safe" }, "sexual" : { "filtered" : false , "severity" : "safe" }, "violence" : { "filtered" : false , "severity" : "safe" }, }, } ], "finish_reason" : "stop" , "logprobs" : none , "content_filter_results" : { "hate" : { "filtered" : false , "severity" : "safe" }, "self_harm" : { "filtered" : false , "severity" : "safe" }, "sexual" : { "filtered" : false , "severity" : "safe" }, "violence" : { "filtered" : false , "severity" : "safe" }, }, }, id = "run-6d7a5282-0de0-4f27-9cc0-82a9db9a3ce9-0" , ) stream: chunk llm . stream ( message ): print ( chunk ) aimessagechunk ( content = "" , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = "j" , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = "'" , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = "ad" , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = "ore" , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = " la" , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = " programm" , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = "ation" , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = "." , id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" ) aimessagechunk ( content = "" , response_metadata = { "finish_reason" : "stop" , "model_name" : "gpt-4" , "system_fingerprint" : "fp_811936bd4f" , }, id = "run-a6f294d3-0700-4f6a-abc2-c6ef1178c37f" , ) stream = llm . stream ( message ) full = next ( stream ) chunk stream : full += chunk full aimessagechunk ( content = "j'adore la programmation." , response_metadata = { "finish_reason" : "stop" , "model_name" : "gpt-4" , "system_fingerprint" : "fp_811936bd4f" , }, id = "run-ba60e41c-9258-44b8-8f3a-2f10599643b3" , ) async: await llm . ainvoke ( message ) # stream: # async chunk (await llm.astream(messages)) # batch: # await llm.abatch([messages]) tool calling: langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getpopulation ( basemodel ): '''get current population given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) llm_with_tools = llm . bind_tools ([ getweather , getpopulation ]) ai_msg = llm_with_tools . invoke ( "which city hotter today bigger: la ny?" ) ai_msg . tool_calls [ { "name" : "getweather" , "args" : { "location" : "los angeles, ca" }, "id" : "call_6xswgd5pqk8tt5atyr7tfenu" , }, { "name" : "getweather" , "args" : { "location" : "new york, ny" }, "id" : "call_zvl15va8y7kxqoy3dtmqgeci" , }, { "name" : "getpopulation" , "args" : { "location" : "los angeles, ca" }, "id" : "call_49cfw8zqc9w7mh7hbmlsirxw" , }, { "name" : "getpopulation" , "args" : { "location" : "new york, ny" }, "id" : "call_6ghfkxv264jefe1mriks3pe7" , }, ] structured output: typing import optional langchain_core.pydantic_v1 import basemodel , field class joke ( basemodel ): '''joke tell user.''' setup : str = field ( description = "the setup joke" ) punchline : str = field ( description = "the punchline joke" ) rating : optional [ int ] = field ( description = "how funny joke is, 1 10" ) structured_llm = llm . with_structured_output ( joke ) structured_llm . invoke ( "tell joke cats" ) joke ( setup = "why cat sitting computer?" , punchline = "to keep eye mouse!" , rating = none , ) see azurechatopenai.with_structured_output() more. json mode: json_llm = llm . bind ( response_format = { "type" : "json_object" }) ai_msg = json_llm . invoke ( "return json object key 'random_ints' value 10 random ints [0-99]" ) ai_msg . content ' \n { \n "random_ints": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67] \n }' image input: import base64 import httpx langchain_core.messages import humanmessage image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-gfp-wisconsin-madison-the-nature-boardwalk.jpg" image_data = base64 . b64encode ( httpx . get ( image_url ) . content ) . decode ( "utf-8" ) message = humanmessage ( content = [ { "type" : "text" , "text" : "describe weather image" }, { "type" : "image_url" , "image_url" : { "url" : f "data:image/jpeg;base64, { image_data } " }, }, ] ) ai_msg = llm . invoke ([ message ]) ai_msg . content "the weather image appears quite pleasant. sky mostly clear" token usage: ai_msg = llm . invoke ( message ) ai_msg . usage_metadata { "input_tokens" : 28 , "output_tokens" : 5 , "total_tokens" : 33 } logprobs: logprobs_llm = llm . bind ( logprobs = true ) ai_msg = logprobs_llm . invoke ( message ) ai_msg . response_metadata [ "logprobs" ] { "content" : [ { "token" : "j" , "bytes" : [ 74 ], "logprob" : - 4.9617593e-06 , "top_logprobs" : [], }, { "token" : "'adore" , "bytes" : [ 39 , 97 , 100 , 111 , 114 , 101 ], "logprob" : - 0.25202933 , "top_logprobs" : [], }, { "token" : " la" , "bytes" : [ 32 , 108 , 97 ], "logprob" : - 0.20141791 , "top_logprobs" : [], }, { "token" : " programmation" , "bytes" : [ 32 , 112 , 114 , 111 , 103 , 114 , 97 , 109 , 109 , 97 , 116 , 105 , 111 , 110 , ], "logprob" : - 1.9361265e-07 , "top_logprobs" : [], }, { "token" : "." , "bytes" : [ 46 ], "logprob" : - 1.2233183e-05 , "top_logprobs" : [], }, ] } response metadata ai_msg = llm . invoke ( message ) ai_msg . response_metadata { "token_usage" : { "completion_tokens" : 6 , "prompt_tokens" : 28 , "total_tokens" : 34 , }, "model_name" : "gpt-35-turbo" , "system_fingerprint" : none , "prompt_filter_results" : [ { "prompt_index" : 0 , "content_filter_results" : { "hate" : { "filtered" : false , "severity" : "safe" }, "self_harm" : { "filtered" : false , "severity" : "safe" }, "sexual" : { "filtered" : false , "severity" : "safe" }, "violence" : { "filtered" : false , "severity" : "safe" }, }, } ], "finish_reason" : "stop" , "logprobs" : none , "content_filter_results" : { "hate" : { "filtered" : false , "severity" : "safe" }, "self_harm" : { "filtered" : false , "severity" : "safe" }, "sexual" : { "filtered" : false , "severity" : "safe" }, "violence" : { "filtered" : false , "severity" : "safe" }, }, } note azurechatopenai implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param azure_ad_token : secretstr | none [optional] # azure active directory token. automatically inferred env var azure_openai_ad_token provided. more: https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id . constraint : type = string writeonly = true format = password param azure_ad_token_provider : callable [ [ ] , str ] | none = none # function return azure active directory token. invoked every request. param azure_endpoint : str | none [optional] # azure endpoint, including resource. automatically inferred env var azure_openai_endpoint provided. example: https://example-resource.azure.openai.com/ param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param default_headers : mapping [ str , str ] | none = none # param default_query : mapping [ str , object ] | none = none # param deployment_name : str | none = none (alias 'azure_deployment') # model deployment. given set base client url include /deployments/{azure_deployment} .
note: mean won’t able use non-deployment endpoints. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param extra_body : mapping [ str , ] | none = none # optional additional json property include request parameter
making request openai compatible apis, vllm. param frequency_penalty : float | none = none # penalizes repeated token according frequency. param http_async_client : | none = none # optional httpx.asyncclient. used async invocations. must specify
http_client well you’d like custom client sync invocations. param http_client : | none = none # optional httpx.client. used sync invocations. must specify
http_async_client well you’d like custom client async invocations. param include_response_headers : bool = false # whether include response header output message response_metadata. param logit_bias : dict [ int , int ] | none = none # modify likelihood specified token appearing completion. param logprobs : bool | none = none # whether return logprobs. param max_retries : int = 2 # maximum number retries make generating. param max_tokens : int | none = none # maximum number token generate. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_kwargs : dict [ str , ] [optional] # hold model parameter valid create call explicitly specified. param model_name : str | none = none (alias 'model') # name deployed openai model, e.g. “gpt-4o”, “gpt-35-turbo”, etc. distinct azure deployment name, set azure user.
used tracing token counting. affect completion. param model_version : str = '' # version model (e.g. “0125” gpt-3.5-0125). azure openai doesn’t return model version response default must
manually specified want use information downstream, e.g.
calculating costs. specify version, appended model name
response. setting correct version help calculate cost properly.
model version validated, make sure set correctly get
correct cost. param n : int = 1 # number chat completion generate prompt. param openai_api_base : str | none = none (alias 'base_url') # base url path api requests, leave blank using proxy service
emulator. param openai_api_key : secretstr | none [optional] (alias 'api_key') # automatically inferred env var azure_openai_api_key provided. constraint : type = string writeonly = true format = password param openai_api_type : str | none [optional] # legacy, openai<1.0.0 support. param openai_api_version : str | none [optional] (alias 'api_version') # automatically inferred env var openai_api_version provided. param openai_organization : str | none = none (alias 'organization') # automatically inferred env var openai_org_id provided. param openai_proxy : str | none = none # param presence_penalty : float | none = none # penalizes repeated tokens. param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param request_timeout : float | tuple [ float , float ] | | none = none (alias 'timeout') # timeout request openai completion api. float, httpx.timeout
none. param seed : int | none = none # seed generation param stop : list [ str ] | str | none = none (alias 'stop_sequences') # default stop sequences. param streaming : bool = false # whether stream result not. param tag : list [ str ] | none = none # tag add run trace. param temperature : float = 0.7 # sampling temperature use. param tiktoken_model_name : str | none = none # model name pas tiktoken using class.
tiktoken used count number token document constrain
certain limit. default, set none,
embedding model name. however, case
may want use embedding class model name
supported tiktoken. include using azure embeddings
using one many model provider expose openai-like
api different models. cases, order avoid erroring
tiktoken called, specify model name use here. param top_logprobs : int | none = none # number likely token return token position,
associated log probability. logprobs must set true
parameter used. param top_p : float | none = none # total probability mass token consider step. param validate_base_url : bool = true # legacy arg openai_api_base passed in, try infer base_url
azure_endpoint update client params accordingly. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
e.g., underlying runnable us api support batch mode. parameter : input ( list [ input ] ) – list input runnable. config ( runnableconfig | list [ runnableconfig ] | none ) – config use invoking runnable.
config support standard key like ‘tags’, ‘metadata’ tracing
purposes, ‘max_concurrency’ controlling much work
parallel, keys. please refer runnableconfig
details. default none. return_exceptions ( bool ) – whether return exception instead raising them.
default false. kwargs ( | none ) – additional keyword argument pas runnable. return : list output runnable. return type : list [ output ] async abatch_as_completed ( input : sequence [ input ] , config : runnableconfig | sequence [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → asynciterator [ tuple [ int , output | exception ] ] # run ainvoke parallel list inputs,
yielding result complete. parameter : input ( sequence [ input ] ) – list input runnable. config ( runnableconfig | sequence [ runnableconfig ] | none ) – config use invoking runnable.
details. default none. default none. return_exceptions ( bool ) – whether return exception instead raising them.
default false. kwargs ( | none ) – additional keyword argument pas runnable. yield : tuple index input output runnable. return type : asynciterator [ tuple [int, output | exception]] async agenerate ( message : list [ list [ basemessage ] ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , run_name : str | none = none , run_id : uuid | none = none , ** kwargs : ) → llmresult # asynchronously pas sequence prompt model return generations. method make use batched call model expose batched
api. use method want to: take advantage batched calls, need output model top generated value, building chain agnostic underlying language model type (e.g., pure text completion model v chat models). parameter : message ( list [ list [ basemessage ] ] ) – list list messages. stop ( list [ str ] | none ) – stop word use generating. model output cut
first occurrence substrings. callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – callback pas through. used executing additional
functionality, logging streaming, throughout generation. **kwargs ( ) – arbitrary additional keyword arguments. usually passed
model provider api call. tag ( list [ str ] | none ) – metadata ( dict [ str , ] | none ) – run_name ( str | none ) – run_id ( uuid | none ) – **kwargs – return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult async agenerate_prompt ( prompt : list [ promptvalue ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → llmresult # asynchronously pas sequence prompt return model generations. method make use batched call model expose batched
api. use method want to: take advantage batched calls, need output model top generated value, building chain agnostic underlying language model type (e.g., pure text completion model v chat models). parameter : prompt ( list [ promptvalue ] ) – list promptvalues. promptvalue object
converted match format language model (string pure
text generation model basemessages chat models). stop ( list [ str ] | none ) – stop word use generating. model output cut
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult async ainvoke ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → basemessage # default implementation ainvoke, call invoke thread. default implementation allows usage async code even
runnable implement native async version invoke. subclass override method run asynchronously. parameter : input ( languagemodelinput ) – config ( optional [ runnableconfig ] ) – stop ( optional [ list [ str ] ] ) – kwargs ( ) – return type : basemessage async apredict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use ainvoke instead. parameter : text ( str ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : str async apredict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use ainvoke instead. parameter : message ( list [ basemessage ] ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : basemessage async astream ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → asynciterator [ basemessagechunk ] # default implementation astream, call ainvoke.
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : asynciterator[ basemessagechunk ] astream_events ( input : , config : runnableconfig | none = none , * , version : literal [ 'v1' , 'v2' ] , include_names : sequence [ str ] | none = none , include_types : sequence [ str ] | none = none , include_tags : sequence [ str ] | none = none , exclude_names : sequence [ str ] | none = none , exclude_types : sequence [ str ] | none = none , exclude_tags : sequence [ str ] | none = none , ** kwargs : ) → asynciterator [ standardstreamevent | customstreamevent ] # beta api beta may change future. generate stream events. use create iterator streamevents provide real-time information
progress runnable, including streamevents intermediate
results. streamevent dictionary following schema: event : str - event name format: on_[runnable_type]_(start|stream|end). name : str - name runnable generated event. run_id : str - randomly generated id associated given execution runnable emitted event.
child runnable get invoked part execution
parent runnable assigned unique id. parent_ids : list[str] - id parent runnables generated event. root runnable empty list.
order parent id root immediate parent.
available v2 version api. v1 version api
return empty list. tag : optional[list[str]] - tag runnable generated event. metadata : optional[dict[str, any]] - metadata runnable generated event. data : dict[str, any] table illustrates even might emitted various
chains. metadata field omitted table brevity.
chain definition included table. attention reference table v2 version schema. event name chunk input output on_chat_model_start [model name] {“messages”: [[systemmessage, humanmessage]]} on_chat_model_stream [model name] aimessagechunk(content=”hello”) on_chat_model_end [model name] {“messages”: [[systemmessage, humanmessage]]} aimessagechunk(content=”hello world”) on_llm_start [model name] {‘input’: ‘hello’} on_llm_stream [model name] ‘hello’ on_llm_end [model name] ‘hello human!’ on_chain_start format_docs on_chain_stream format_docs “hello world!, goodbye world!” on_chain_end format_docs [document(…)] “hello world!, goodbye world!” on_tool_start some_tool {“x”: 1, “y”: “2”} on_tool_end some_tool {“x”: 1, “y”: “2”} on_retriever_start [retriever name] {“query”: “hello”} on_retriever_end [retriever name] {“query”: “hello”} [document(…), ..] on_prompt_start [template_name] {“question”: “hello”} on_prompt_end [template_name] {“question”: “hello”} chatpromptvalue(messages: [systemmessage, …]) addition standard events, user also dispatch custom event (see example below). custom event surfaced v2 version api! custom event following format: attribute type description name str user defined name event. data data associated event. anything, though suggest making json serializable. declaration associated standard event shown above: format_docs : def format_docs ( doc : list [ document ]) -> str : '''format docs.''' return ", " . join ([ doc . page_content doc doc ]) format_docs = runnablelambda ( format_docs ) some_tool : @tool def some_tool ( x : int , : str ) -> dict : '''some_tool.''' return { "x" : x , "y" : } prompt : template = chatprompttemplate . from_messages ( [( "system" , "you cat agent 007" ), ( "human" , " {question} " )] ) . with_config ({ "run_name" : "my_template" , "tags" : [ "my_template" ]}) example: langchain_core.runnables import runnablelambda async def reverse ( : str ) -> str : return [:: - 1 ] chain = runnablelambda ( func = reverse ) event = [ event async event chain . astream_events ( "hello" , version = "v2" ) ] # produce following event (run_id, parent_ids # omitted brevity): [ { "data" : { "input" : "hello" }, "event" : "on_chain_start" , "metadata" : {}, "name" : "reverse" , "tags" : [], }, { "data" : { "chunk" : "olleh" }, "event" : "on_chain_stream" , "metadata" : {}, "name" : "reverse" , "tags" : [], }, { "data" : { "output" : "olleh" }, "event" : "on_chain_end" , "metadata" : {}, "name" : "reverse" , "tags" : [], }, ] example: dispatch custom event langchain_core.callbacks.manager import ( adispatch_custom_event , ) langchain_core.runnables import runnablelambda , runnableconfig import asyncio async def slow_thing ( some_input : str , config : runnableconfig ) -> str : """do something take long time.""" await asyncio . sleep ( 1 ) # placeholder slow operation await adispatch_custom_event ( "progress_event" , { "message" : "finished step 1 3" }, config = config # must included python < 3.10 ) await asyncio . sleep ( 1 ) # placeholder slow operation await adispatch_custom_event ( "progress_event" , { "message" : "finished step 2 3" }, config = config # must included python < 3.10 ) await asyncio . sleep ( 1 ) # placeholder slow operation return "done" slow_thing = runnablelambda ( slow_thing ) async event slow_thing . astream_events ( "some_input" , version = "v2" ): print ( event ) parameter : input ( ) – input runnable. config ( runnableconfig | none ) – config use runnable. version ( literal [ 'v1' , 'v2' ] ) – version schema use either v2 v1 .
user use v2 . v1 backwards compatibility deprecated
0.4.0.
default assigned api stabilized.
custom event surfaced v2 . include_names ( sequence [ str ] | none ) – include event runnables matching names. include_types ( sequence [ str ] | none ) – include event runnables matching types. include_tags ( sequence [ str ] | none ) – include event runnables matching tags. exclude_names ( sequence [ str ] | none ) – exclude event runnables matching names. exclude_types ( sequence [ str ] | none ) – exclude event runnables matching types. exclude_tags ( sequence [ str ] | none ) – exclude event runnables matching tags. kwargs ( ) – additional keyword argument pas runnable.
passed astream_log implementation
astream_events built top astream_log. yield : async stream streamevents. raise : notimplementederror – version v1 v2 . return type : asynciterator [ standardstreamevent | customstreamevent ] batch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run invoke parallel using thread pool executor. default implementation batch work well io bound runnables. subclass override method batch efficiently;
e.g., underlying runnable us api support batch mode. parameter : input ( list [ input ] ) – config ( runnableconfig | list [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : list [ output ] batch_as_completed ( input : sequence [ input ] , config : runnableconfig | sequence [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → iterator [ tuple [ int , output | exception ] ] # run invoke parallel list inputs,
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_functions ( function : sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] , function_call : _functioncall | str | literal [ 'auto' , 'none' ] | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] # bind function (and objects) chat model. assumes model compatible openai function-calling api. note: using bind_tools recommended instead, function function_call request parameter officially marked deprecated
openai. parameter : function ( sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] ) – list function definition bind chat model.
dictionary, pydantic model, callable. pydantic
model callables automatically converted
schema dictionary representation. function_call ( _functioncall | str | literal [ 'auto' , 'none' ] | none ) – function require model call.
must name single provided function
“auto” automatically determine function call
(if any). **kwargs ( ) – additional parameter pas runnable constructor. return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , * , tool_choice : dict | str | literal [ 'auto' , 'none' , 'any' , 'required' ] | bool | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model. assumes model compatible openai tool-calling api. changed version 0.1.21: support strict argument added. parameter : tool ( sequence [ dict [ str , ] | type | callable | basetool ] ) – list tool definition bind chat model.
support tool definition handled langchain_core.utils.function_calling.convert_to_openai_tool() . tool_choice ( dict | str | literal [ 'auto' , 'none' , 'any' , 'required' ] | bool | none ) – tool require model call.
option are: str form ">" : call > tool. "auto" : automatically selects tool (including tool). "none" : call tool. "any" "required" true : force least one tool called. dict form {"type": "function", "function": {"name": >}} : call > tool. false none : effect, default openai behavior. strict – true, model output guaranteed exactly match json schema
provided tool definition. true, input schema
validated according https://platform.openai.com/docs/guides/structured-outputs/supported-schemas .
false, input schema validated model output
validated.
none, strict argument passed model. new version 0.1.21. kwargs ( ) – additional parameter passed directly self.bind(**kwargs) . return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) – stop ( list [ str ] | none ) – kwargs ( ) – return type : str configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) → runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) – configurablefield instance used select
alternative. default_key ( str ) – default key use alternative selected.
default “default”. prefix_keys ( bool ) – whether prefix key configurablefield id.
default false. **kwargs ( runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) – dictionary key runnable instance callables
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) → runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) – dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) generate ( message : list [ list [ basemessage ] ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , run_name : str | none = none , run_id : uuid | none = none , ** kwargs : ) → llmresult # pas sequence prompt model return model generations. method make use batched call model expose batched
model provider api call. tag ( list [ str ] | none ) – metadata ( dict [ str , ] | none ) – run_name ( str | none ) – run_id ( uuid | none ) – **kwargs – return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult generate_prompt ( prompt : list [ promptvalue ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → llmresult # pas sequence prompt model return model generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int # get number token present text. useful checking input fit model’s context window. parameter : text ( str ) – string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # calculate num token gpt-3.5-turbo gpt-4 tiktoken package. requirement : must pillow installed want count
image token specifying image base64 string, must
pillow httpx installed specifying image
url. aren’t installed image input ignored token
counting. openai reference: openai/openai-cookbook main/examples/how_to_format_inputs_to_chatgpt_models.ipynb parameter : message ( list [ basemessage ] ) – return type : int get_token_ids ( text : str ) → list [ int ] # get token present text tiktoken package. parameter : text ( str ) – return type : list [int] invoke ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → basemessage # transform single input output. override implement. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use invoking runnable.
details. stop ( optional [ list [ str ] ] ) – kwargs ( ) – return : output runnable. return type : basemessage predict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : text ( str ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : str predict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : basemessage stream ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → iterator [ basemessagechunk ] # default implementation stream, call invoke.
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict [ str , ] | type [ _bm ] | none = none , * , method : literal [ 'function_calling' , 'json_mode' ] = 'function_calling' , include_raw : literal [ true ] = true , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , _allreturntype ] [source] # with_structured_output ( schema : dict [ str , ] | type [ _bm ] | none = none , * , method : literal [ 'function_calling' , 'json_mode' ] = 'function_calling' , include_raw : literal [ false ] = false , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | _bm ] model wrapper return output formatted match given schema. args: schema: output schema. passed as: openai function/tool schema, json schema, typeddict class, pydantic class. schema pydantic class model output
pydantic instance class, model-generated field
validated pydantic class. otherwise model output
dict validated. see langchain_core.utils.function_calling.convert_to_openai_tool() properly specify type description
schema field specifying pydantic typeddict class. method: method steering model generation, either “function_calling”
“json_mode”. “function_calling” schema converted
openai function returned model make use
function-calling api. “json_mode” openai’s json mode
used. note using “json_mode” must include instruction
formatting output desired schema model call. include_raw: false parsed structured output returned.
error occurs model output parsing raised. true
raw model response (a basemessage) parsed model
response returned. error occurs output parsing
caught returned well. final output always dict
key “raw”, “parsed”, “parsing_error”. returns: runnable take input langchain_core.language_models.chat.basechatmodel . include_raw false schema pydantic class, runnable output
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] example: schema=pydantic class, method=”function_calling”, include_raw=false: typing import optional langchain_openai import azurechatopenai langchain_core.pydantic_v1 import basemodel , field class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str # provide default value and/or description fields, passed # model. important part improving model's ability # correctly return structured outputs. justification : optional [ str ] = field ( default = none , description = "a justification answer." ) llm = azurechatopenai ( model = "gpt-3.5-turbo-0125" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: schema=pydantic class, method=”function_calling”, include_raw=true: langchain_openai import azurechatopenai langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = azurechatopenai ( model = "gpt-3.5-turbo-0125" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: schema=typeddict class, method=”function_calling”, include_raw=false: # important: using python { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example: schema=openai function schema, method=”function_calling”, include_raw=false: langchain_openai import azurechatopenai oai_schema = { 'name' : 'answerwithjustification' , 'description' : 'an answer user question along justification answer.' , 'parameters' : { 'type' : 'object' , 'properties' : { 'answer' : { 'type' : 'string' }, 'justification' : { 'description' : 'a justification answer.' , 'type' : 'string' } }, 'required' : [ 'answer' ] } } llm = azurechatopenai ( model = "gpt-3.5-turbo-0125" , temperature = 0 ) structured_llm = llm . with_structured_output ( oai_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example: schema=pydantic class, method=”json_mode”, include_raw=true: langchain_openai import azurechatopenai langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): answer : str justification : str llm = azurechatopenai ( model = "gpt-3.5-turbo-0125" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , method = "json_mode" , include_raw = true ) structured_llm . invoke ( "answer following question. " "make sure return json blob key 'answer' 'justification'. “ “what’s heavier pound brick pound feathers?” )
# -> {
# ‘raw’: aimessage(content=’{ “answer”: “they weight.”,
“justification”: “both pound brick pound feather weigh one pound. difference lie volume density materials, weight.” }’), # ‘parsed’: answerwithjustification(answer=’they weight.’, justification=’both pound brick pound feather weigh one pound. difference lie volume density materials, weight.’),
# ‘parsing_error’: none
# } example: schema=none, method=”json_mode”, include_raw=true: structured_llm = llm . with_structured_output ( method = "json_mode" , include_raw = true ) structured_llm . invoke ( "answer following question. " "make sure return json blob key 'answer' 'justification'. “ “what’s heavier pound brick pound feathers?” )
“justification”: “both pound brick pound feather weigh one pound. difference lie volume density materials, weight.” }’), # ‘parsed’: {
# ‘answer’: ‘they weight.’,
# ‘justification’: ‘both pound brick pound feather weigh one pound. difference lie volume density materials, weight.’
# },
# } example using azurechatopenai azure cosmos db apache gremlin azurechatopenai bing search microsoft openai
page: https://python.langchain.com/v0.2/api_reference/openai/chat_models/langchain_openai.chat_models.base.basechatopenai.html#langchain_openai.chat_models.base.basechatopenai
basechatopenai # class langchain_openai.chat_models.base. basechatopenai [source] # bases: basechatmodel note basechatopenai implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param default_headers : mapping [ str , str ] | none = none # param default_query : mapping [ str , object ] | none = none # param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
http_async_client well you’d like custom client async invocations. param include_response_headers : bool = false # whether include response header output message response_metadata. param logit_bias : dict [ int , int ] | none = none # modify likelihood specified token appearing completion. param logprobs : bool | none = none # whether return logprobs. param max_retries : int = 2 # maximum number retries make generating. param max_tokens : int | none = none # maximum number token generate. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_kwargs : dict [ str , ] [optional] # hold model parameter valid create call explicitly specified. param model_name : str = 'gpt-3.5-turbo' (alias 'model') # model name use. param n : int = 1 # number chat completion generate prompt. param openai_api_base : str | none = none (alias 'base_url') # base url path api requests, leave blank using proxy service
emulator. param openai_api_key : secretstr | none = none (alias 'api_key') # automatically inferred env var openai_api_key provided. constraint : type = string writeonly = true format = password param openai_organization : str | none = none (alias 'organization') # automatically inferred env var openai_org_id provided. param openai_proxy : str | none = none # param presence_penalty : float | none = none # penalizes repeated tokens. param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param request_timeout : float | tuple [ float , float ] | | none = none (alias 'timeout') # timeout request openai completion api. float, httpx.timeout
parameter used. param top_p : float | none = none # total probability mass token consider step. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_functions ( function : sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] , function_call : _functioncall | str | literal [ 'auto' , 'none' ] | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind function (and objects) chat model. assumes model compatible openai function-calling api. note: using bind_tools recommended instead, function function_call request parameter officially marked deprecated
(if any). **kwargs ( ) – additional parameter pas runnable constructor. return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , * , tool_choice : dict | str | literal [ 'auto' , 'none' , 'any' , 'required' ] | bool | none = none , strict : bool | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model. assumes model compatible openai tool-calling api. changed version 0.1.21: support strict argument added. parameter : tool ( sequence [ dict [ str , ] | type | callable | basetool ] ) – list tool definition bind chat model.
option are: str form ">" : call > tool. "auto" : automatically selects tool (including tool). "none" : call tool. "any" "required" true : force least one tool called. dict form {"type": "function", "function": {"name": >}} : call > tool. false none : effect, default openai behavior. strict ( bool | none ) – true, model output guaranteed exactly match json schema
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int # get number token present text. useful checking input fit model’s context window. parameter : text ( str ) – string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int [source] # calculate num token gpt-3.5-turbo gpt-4 tiktoken package. requirement : must pillow installed want count
counting. openai reference: openai/openai-cookbook main/examples/how_to_format_inputs_to_chatgpt_models.ipynb parameter : message ( list [ basemessage ] ) – return type : int get_token_ids ( text : str ) → list [ int ] [source] # get token present text tiktoken package. parameter : text ( str ) – return type : list [int] invoke ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → basemessage # transform single input output. override implement. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use invoking runnable.
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict [ str , ] | type [ _bm ] | type | none = none , * , method : literal [ 'function_calling' , 'json_mode' , 'json_schema' ] = 'function_calling' , include_raw : bool = false , strict : bool | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | _bm ] [source] # model wrapper return output formatted match given schema. parameter : schema ( dict [ str , ] | type [ _bm ] | type | none ) – output schema. passed as: openai function/tool schema, json schema, typeddict class (support added 0.1.20), pydantic class. schema pydantic class model output
schema field specifying pydantic typeddict class. method ( literal [ 'function_calling' , 'json_mode' , 'json_schema' ] ) – method steering model generation, one of: ”function_calling”: us openai’s tool-calling (formerly called function calling)
api: https://platform.openai.com/docs/guides/function-calling ”json_schema”: us openai’s structured output api: https://platform.openai.com/docs/guides/structured-outputs supported “gpt-4o-mini”, “gpt-4o-2024-08-06”, later
models. ”json_mode”: us openai’s json mode. note using json mode
must include instruction formatting output
desired schema model call: https://platform.openai.com/docs/guides/structured-outputs/json-mode learn difference method model
support method here: https://platform.openai.com/docs/guides/structured-outputs/structured-outputs-vs-json-mode https://platform.openai.com/docs/guides/structured-outputs/function-calling-vs-response-format include_raw ( bool ) – false parsed structured output returned.
key “raw”, “parsed”, “parsing_error”. strict ( bool | none ) – true: model output guaranteed exactly match schema.
input schema also validated according https://platform.openai.com/docs/guides/structured-outputs/supported-schemas false: input schema validated model output
validated. none: strict argument passed model. method “json_schema” default true. method
“function_calling” “json_mode” default none.
non-null method “function_calling” “json_schema”. kwargs ( ) – additional keyword args aren’t supported. return : runnable take input langchain_core.language_models.chat.basechatmodel . include_raw false schema pydantic class, runnable output instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: ”raw”: basemessage ”parsed”: none parsing error, otherwise type depends schema described above. ”parsing_error”: optional[baseexception] return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | _bm ] changed version 0.1.20: added support typeddict class schema . changed version 0.1.21: support strict argument added.
support method = “json_schema” added. note planned breaking change version 0.2.0 method default changed “json_schema” “function_calling”. strict default true method “function_calling” version 0.2.0 . example: schema=pydantic class, method=”function_calling”, include_raw=false, strict=true note, openai number restriction type schema
provided strict = true. using pydantic, model cannot
specify field metadata (like min/max constraints) field cannot
default values. see constraint here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas typing import optional langchain_openai import chatopenai langchain_core.pydantic_v1 import basemodel , field class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : optional [ str ] = field ( default =... , description = "a justification answer." ) llm = chatopenai ( model = "gpt-4o" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , strict = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: schema=pydantic class, method=”function_calling”, include_raw=true langchain_openai import chatopenai langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatopenai ( model = "gpt-4o" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: schema=typeddict class, method=”function_calling”, include_raw=false # important: using python { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example: schema=openai function schema, method=”function_calling”, include_raw=false langchain_openai import chatopenai oai_schema = { 'name' : 'answerwithjustification' , 'description' : 'an answer user question along justification answer.' , 'parameters' : { 'type' : 'object' , 'properties' : { 'answer' : { 'type' : 'string' }, 'justification' : { 'description' : 'a justification answer.' , 'type' : 'string' } }, 'required' : [ 'answer' ] } } llm = chatopenai ( model = "gpt-4o" , temperature = 0 ) structured_llm = llm . with_structured_output ( oai_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example: schema=pydantic class, method=”json_mode”, include_raw=true langchain_openai import chatopenai langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): answer : str justification : str llm = chatopenai ( model = "gpt-4o" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , method = "json_mode" , include_raw = true ) structured_llm . invoke ( "answer following question. " "make sure return json blob key 'answer' 'justification'. \n\n " "what's heavier pound brick pound feathers?" ) # -> { # 'raw': aimessage(content='{\n "answer": "they weight.",\n "justification": "both pound brick pound feather weigh one pound. difference lie volume density materials, weight." \n}'), # 'parsed': answerwithjustification(answer='they weight.', justification='both pound brick pound feather weigh one pound. difference lie volume density materials, weight.'), # 'parsing_error': none # } example: schema=none, method=”json_mode”, include_raw=true structured_llm = llm . with_structured_output ( method = "json_mode" , include_raw = true ) structured_llm . invoke ( "answer following question. " "make sure return json blob key 'answer' 'justification'. \n\n " "what's heavier pound brick pound feathers?" ) # -> { # 'raw': aimessage(content='{\n "answer": "they weight.",\n "justification": "both pound brick pound feather weigh one pound. difference lie volume density materials, weight." \n}'), # 'parsed': { # 'answer': 'they weight.', # 'justification': 'both pound brick pound feather weigh one pound. difference lie volume density materials, weight.' # }, # 'parsing_error': none # }
page: https://python.langchain.com/v0.2/api_reference/openai/chat_models/langchain_openai.chat_models.base.chatopenai.html#langchain_openai.chat_models.base.chatopenai
chatopenai # class langchain_openai.chat_models.base. chatopenai [source] # bases: basechatopenai openai chat model integration. setup install langchain-openai set environment variable openai_api_key . pip install -u langchain-openai export openai_api_key = "your-api-key" key init args — completion params model: str name openai model use. temperature: float sampling temperature. max_tokens: optional[int] max number token generate. logprobs: optional[bool] whether return logprobs. stream_options: dict configure streaming outputs, like whether return token usage
streaming ( {"include_usage": true} ). see full list supported init args description params section. key init args — client params timeout: union[float, tuple[float, float], any, none] timeout requests. max_retries: int max number retries. api_key: optional[str] openai api key. passed read env var openai_api_key. base_url: optional[str] base url api requests. specify using proxy service
emulator. organization: optional[str] openai organization id. passed read env
var openai_org_id. see full list supported init args description params section. instantiate langchain_openai import chatopenai llm = chatopenai ( model = "gpt-4o" , temperature = 0 , max_tokens = none , timeout = none , max_retries = 2 , # api_key="...", # base_url="...", # organization="...", # params... ) note : param explicitly supported passed directly openai.openai.chat.completions.create(...) api every time model
invoked. example: langchain_openai import chatopenai import openai chatopenai ( ... , frequency_penalty = 0.2 ) . invoke ( ... ) # result underlying api call of: openai . openai ( .. ) . chat . completion . create ( ... , frequency_penalty = 0.2 ) # also equivalent to: chatopenai ( ... ) . invoke ( ... , frequency_penalty = 0.2 ) invoke message = [ ( "system" , "you helpful translator. translate user sentence french." , ), ( "human" , "i love programming." ), ] llm . invoke ( message ) aimessage( content="j'adore la programmation.", response_metadata={ "token_usage": { "completion_tokens": 5, "prompt_tokens": 31, "total_tokens": 36, }, "model_name": "gpt-4o", "system_fingerprint": "fp_43dfabdef1", "finish_reason": "stop", "logprobs": none, }, id="run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0", usage_metadata={"input_tokens": 31, "output_tokens": 5, "total_tokens": 36}, ) stream chunk llm . stream ( message ): print ( chunk ) aimessagechunk ( content = "" , id = "run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0" ) aimessagechunk ( content = "j" , id = "run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0" ) aimessagechunk ( content = "'adore" , id = "run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0" ) aimessagechunk ( content = " la" , id = "run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0" ) aimessagechunk ( content = " programmation" , id = "run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0" ) aimessagechunk ( content = "." , id = "run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0" ) aimessagechunk ( content = "" , response_metadata = { "finish_reason" : "stop" }, id = "run-9e1517e3-12bf-48f2-bb1b-2e824f7cd7b0" , ) stream = llm . stream ( message ) full = next ( stream ) chunk stream : full += chunk full aimessagechunk ( content = "j'adore la programmation." , response_metadata = { "finish_reason" : "stop" }, id = "run-bf917526-7f58-4683-84f7-36a6b671d140" , ) async await llm . ainvoke ( message ) # stream: # async chunk (await llm.astream(messages)) # batch: # await llm.abatch([messages]) aimessage ( content = "j'adore la programmation." , response_metadata = { "token_usage" : { "completion_tokens" : 5 , "prompt_tokens" : 31 , "total_tokens" : 36 , }, "model_name" : "gpt-4o" , "system_fingerprint" : "fp_43dfabdef1" , "finish_reason" : "stop" , "logprobs" : none , }, id = "run-012cffe2-5d3d-424d-83b5-51c6d4a593d1-0" , usage_metadata = { "input_tokens" : 31 , "output_tokens" : 5 , "total_tokens" : 36 }, ) tool calling langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getpopulation ( basemodel ): '''get current population given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) llm_with_tools = llm . bind_tools ( [ getweather , getpopulation ] # strict = true # enforce tool args schema respected ) ai_msg = llm_with_tools . invoke ( "which city hotter today bigger: la ny?" ) ai_msg . tool_calls [ { "name" : "getweather" , "args" : { "location" : "los angeles, ca" }, "id" : "call_6xswgd5pqk8tt5atyr7tfenu" , }, { "name" : "getweather" , "args" : { "location" : "new york, ny" }, "id" : "call_zvl15va8y7kxqoy3dtmqgeci" , }, { "name" : "getpopulation" , "args" : { "location" : "los angeles, ca" }, "id" : "call_49cfw8zqc9w7mh7hbmlsirxw" , }, { "name" : "getpopulation" , "args" : { "location" : "new york, ny" }, "id" : "call_6ghfkxv264jefe1mriks3pe7" , }, ] note openai >= 1.32 support parallel_tool_calls parameter
default true . parameter set false
disable parallel tool calls: ai_msg = llm_with_tools . invoke ( "what weather la ny?" , parallel_tool_calls = false ) ai_msg . tool_calls [ { "name" : "getweather" , "args" : { "location" : "los angeles, ca" }, "id" : "call_4ooy0zr99ievc7fevsh8uhtz" , } ] like runtime parameters, parallel_tool_calls bound model
using llm.bind(parallel_tool_calls=false) instantiation
setting model_kwargs . see chatopenai.bind_tools() method more. structured output typing import optional langchain_core.pydantic_v1 import basemodel , field class joke ( basemodel ): '''joke tell user.''' setup : str = field ( description = "the setup joke" ) punchline : str = field ( description = "the punchline joke" ) rating : optional [ int ] = field ( description = "how funny joke is, 1 10" ) structured_llm = llm . with_structured_output ( joke ) structured_llm . invoke ( "tell joke cats" ) joke ( setup = "why cat sitting computer?" , punchline = "to keep eye mouse!" , rating = none , ) see chatopenai.with_structured_output() more. json mode json_llm = llm . bind ( response_format = { "type" : "json_object" }) ai_msg = json_llm . invoke ( "return json object key 'random_ints' value 10 random ints [0-99]" ) ai_msg . content ' \n { \n "random_ints": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67] \n }' image input import base64 import httpx langchain_core.messages import humanmessage image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-gfp-wisconsin-madison-the-nature-boardwalk.jpg" image_data = base64 . b64encode ( httpx . get ( image_url ) . content ) . decode ( "utf-8" ) message = humanmessage ( content = [ { "type" : "text" , "text" : "describe weather image" }, { "type" : "image_url" , "image_url" : { "url" : f "data:image/jpeg;base64, { image_data } " }, }, ] ) ai_msg = llm . invoke ([ message ]) ai_msg . content "the weather image appears clear pleasant. sky mostly blue scattered, light clouds, suggesting sunny day minimal cloud cover. indication rain strong winds, overall scene look bright calm. lush green grass clear visibility indicate good weather conditions." token usage ai_msg = llm . invoke ( message ) ai_msg . usage_metadata { "input_tokens" : 28 , "output_tokens" : 5 , "total_tokens" : 33 } streaming, set stream_usage kwarg: stream = llm . stream ( message , stream_usage = true ) full = next ( stream ) chunk stream : full += chunk full . usage_metadata { "input_tokens" : 28 , "output_tokens" : 5 , "total_tokens" : 33 } alternatively, setting stream_usage instantiating model
useful incorporating chatopenai lcel chains– using
method like .with_structured_output , generate chain
hood. llm = chatopenai ( model = "gpt-4o" , stream_usage = true ) structured_llm = llm . with_structured_output ( ... ) logprobs logprobs_llm = llm . bind ( logprobs = true ) ai_msg = logprobs_llm . invoke ( message ) ai_msg . response_metadata [ "logprobs" ] { "content" : [ { "token" : "j" , "bytes" : [ 74 ], "logprob" : - 4.9617593e-06 , "top_logprobs" : [], }, { "token" : "'adore" , "bytes" : [ 39 , 97 , 100 , 111 , 114 , 101 ], "logprob" : - 0.25202933 , "top_logprobs" : [], }, { "token" : " la" , "bytes" : [ 32 , 108 , 97 ], "logprob" : - 0.20141791 , "top_logprobs" : [], }, { "token" : " programmation" , "bytes" : [ 32 , 112 , 114 , 111 , 103 , 114 , 97 , 109 , 109 , 97 , 116 , 105 , 111 , 110 , ], "logprob" : - 1.9361265e-07 , "top_logprobs" : [], }, { "token" : "." , "bytes" : [ 46 ], "logprob" : - 1.2233183e-05 , "top_logprobs" : [], }, ] } response metadata ai_msg = llm . invoke ( message ) ai_msg . response_metadata { "token_usage" : { "completion_tokens" : 5 , "prompt_tokens" : 28 , "total_tokens" : 33 , }, "model_name" : "gpt-4o" , "system_fingerprint" : "fp_319be4768e" , "finish_reason" : "stop" , "logprobs" : none , } note chatopenai implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param default_headers : mapping [ str , str ] | none = none # param default_query : mapping [ str , object ] | none = none # param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
none. param seed : int | none = none # seed generation param stop : list [ str ] | str | none = none (alias 'stop_sequences') # default stop sequences. param stream_usage : bool = false # whether include usage metadata streaming output. true, additional
message chunk generated stream including usage metadata. param streaming : bool = false # whether stream result not. param tag : list [ str ] | none = none # tag add run trace. param temperature : float = 0.7 # sampling temperature use. param tiktoken_model_name : str | none = none # model name pas tiktoken using class.
(if any). **kwargs ( ) – additional parameter pas runnable constructor. return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , * , tool_choice : dict | str | literal [ 'auto' , 'none' , 'any' , 'required' ] | bool | none = none , strict : bool | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] # bind tool-like object chat model. assumes model compatible openai tool-calling api. changed version 0.1.21: support strict argument added. parameter : tool ( sequence [ dict [ str , ] | type | callable | basetool ] ) – list tool definition bind chat model.
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict [ str , ] | type [ _bm ] | type | none = none , * , method : literal [ 'function_calling' , 'json_mode' , 'json_schema' ] = 'function_calling' , include_raw : bool = false , strict : bool | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | _bm ] # model wrapper return output formatted match given schema. parameter : schema ( dict [ str , ] | type [ _bm ] | type | none ) – output schema. passed as: openai function/tool schema, json schema, typeddict class (support added 0.1.20), pydantic class. schema pydantic class model output
default values. see constraint here: https://platform.openai.com/docs/guides/structured-outputs/supported-schemas typing import optional langchain_openai import chatopenai langchain_core.pydantic_v1 import basemodel , field class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : optional [ str ] = field ( default =... , description = "a justification answer." ) llm = chatopenai ( model = "gpt-4o" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , strict = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: schema=pydantic class, method=”function_calling”, include_raw=true langchain_openai import chatopenai langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatopenai ( model = "gpt-4o" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: schema=typeddict class, method=”function_calling”, include_raw=false # important: using python { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example: schema=openai function schema, method=”function_calling”, include_raw=false langchain_openai import chatopenai oai_schema = { 'name' : 'answerwithjustification' , 'description' : 'an answer user question along justification answer.' , 'parameters' : { 'type' : 'object' , 'properties' : { 'answer' : { 'type' : 'string' }, 'justification' : { 'description' : 'a justification answer.' , 'type' : 'string' } }, 'required' : [ 'answer' ] } } llm = chatopenai ( model = "gpt-4o" , temperature = 0 ) structured_llm = llm . with_structured_output ( oai_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example: schema=pydantic class, method=”json_mode”, include_raw=true langchain_openai import chatopenai langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): answer : str justification : str llm = chatopenai ( model = "gpt-4o" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , method = "json_mode" , include_raw = true ) structured_llm . invoke ( "answer following question. " "make sure return json blob key 'answer' 'justification'. \n\n " "what's heavier pound brick pound feathers?" ) # -> { # 'raw': aimessage(content='{\n "answer": "they weight.",\n "justification": "both pound brick pound feather weigh one pound. difference lie volume density materials, weight." \n}'), # 'parsed': answerwithjustification(answer='they weight.', justification='both pound brick pound feather weigh one pound. difference lie volume density materials, weight.'), # 'parsing_error': none # } example: schema=none, method=”json_mode”, include_raw=true structured_llm = llm . with_structured_output ( method = "json_mode" , include_raw = true ) structured_llm . invoke ( "answer following question. " "make sure return json blob key 'answer' 'justification'. \n\n " "what's heavier pound brick pound feathers?" ) # -> { # 'raw': aimessage(content='{\n "answer": "they weight.",\n "justification": "both pound brick pound feather weigh one pound. difference lie volume density materials, weight." \n}'), # 'parsed': { # 'answer': 'they weight.', # 'justification': 'both pound brick pound feather weigh one pound. difference lie volume density materials, weight.' # }, # 'parsing_error': none # } example using chatopenai # example # legacy ainetwork toolkit aws dynamodb activeloop deep memory amadeus toolkit amazon neptune cypher apache age apache cassandra arxiv arangodb arthur asknews azure container apps dynamic session azureaisearchretriever bearly code interpreter browserbase build query analysis system build question answering application graph database build simple llm application lcel cassandra database toolkit chatgpt plugins chatopenai classify text label cnosdb cogniswitch toolkit conceptual guide connery toolkit connery toolkit tool context conversational rag couchbase dataherald diffbot discord e2b data analysis elasticsearchretriever exa search facebook messenger falkordb figma financialdatasets toolkit flashrank reranker fleet ai context flyte generate synthetic data hippo deal high cardinality categoricals query analysis add semantic layer graph database add chat history add default invocation args runnable add example prompt query analysis add fallback runnable add memory chatbots add retrieval chatbots add score retriever result add tool chatbots add value chain’s state best prompt graph-rag bind model-specific tool configure runtime chain internals construct knowledge graph convert tool openai function disable parallel tool calling “self-querying” retrieval retrieval contextual compression get log probability handle case query generated handle multiple query query analysis handle multiple retriever query analysis inspect runnables invoke runnables parallel map value graph database migrate legacy langchain agent langgraph parse json output parse yaml output pas multimodal data directly model pas argument one step next retry parsing error occurs run custom function save load langchain object stream tool call track token usage chatmodels trim message use langchain different pydantic version use shot example chat model use few-shot prompting tool calling use multimodal prompt use reference example extraction use multiqueryretriever use output-fixing parser hugegraph human tool hybrid search image caption infino infobip jaguar vector database jina reranker kdb.ai kay.ai kuzu llmlingua document compressor llmonitor label studio langsmith chat datasets langsmith llm run load doc log, trace, monitor log10 mlflow memgraph milvus hybrid search retriever model cache momento vector index (mvi) mongodb multion toolkit myscale nebulagraph neo4j neo4j vector index ontotext graphdb openai openai metadata tagger openapi toolkit outline panda dataframe passio nutritionai polygon io toolkit polygon io toolkit tool portkey powerbi toolkit promptlayer ragatouille rdflib rankllm reranker rephrasequery reddit search redis rememberizer remembrall request toolkit response metadata robocorp toolkit sap hana cloud vector engine sec filing sql (sqlalchemy) sqlite semantic scholar api tool shell (bash) slack slack toolkit spark sql toolkit streamlit tavilysearchapiretriever telegram tencent cloud vectordb tidb timescale vector (postgres) trubrics uptrain upstash ratelimit callback vectara vectara self-querying wechat weaviate whatsapp xata yahoo finance news yellowbrick you.com you.com search youtube audio zepcloudchatmessagehistory imessage vllm chat 🦜️🏓 langserve
page: https://python.langchain.com/v0.2/api_reference/openai/chat_models/langchain_openai.chat_models.base.openairefusalerror.html#langchain_openai.chat_models.base.openairefusalerror
openairefusalerror # class langchain_openai.chat_models.base. openairefusalerror [source] # error raised openai structured output api return refusal. using openai’s structured output api user-generated input, model
may occasionally refuse fulfill request safety reasons. see refusals: https://platform.openai.com/docs/guides/structured-outputs/refusals new version 0.1.21.
page: https://python.langchain.com/v0.2/api_reference/openai/embeddings.html#langchain-openai-embeddings
embeddings # class embeddings.azure.azureopenaiembeddings azureopenai embedding model integration. embeddings.base.openaiembeddings openai embedding model integration.
page: https://python.langchain.com/v0.2/api_reference/openai/embeddings/langchain_openai.embeddings.azure.azureopenaiembeddings.html#langchain_openai.embeddings.azure.azureopenaiembeddings
azureopenaiembeddings # class langchain_openai.embeddings.azure. azureopenaiembeddings [source] # bases: openaiembeddings azureopenai embedding model integration. setup: access azureopenai embedding model you’ll need create azure account,
get api key, install langchain-openai integration package. you’ll need azure openai instance deployed.
deploy version azure portal following
[guide]( https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal ). instance running, make sure name
instance key. find key azure portal,
“keys endpoint” section instance. pip install -u langchain_openai # set environment variable (or pas directly model) export azure_openai_api_key = "your-api-key" export azure_openai_endpoint = "https://.openai.azure.com/" export azure_openai_api_version = "2024-02-01" key init args — completion params: model: str name azureopenai model use. dimensions: optional[int] number dimension embeddings. specified
underlying model support it. key init args — client params: api_key: optional[secretstr] see full list supported init args description params section. instantiate: langchain_openai import azureopenaiembeddings embeddings = azureopenaiembeddings ( model = "text-embedding-3-large" # dimensions: optional[int] = none, # specify dimension new text-embedding-3 model # azure_endpoint="https://.openai.azure.com/", provided, read env variable azure_openai_endpoint # api_key=... # provide api key directly. missing read env variable azure_openai_api_key # openai_api_version=..., # provided, read env variable azure_openai_api_version ) embed single text: input_text = "the meaning life 42" vector = embed . embed_query ( input_text ) print ( vector [: 3 ]) [ - 0.024603435769677162 , - 0.007543657906353474 , 0.0039630369283258915 ] embed multiple texts: input_texts = [ "document 1..." , "document 2..." ] vector = embed . embed_documents ( input_texts ) print ( len ( vector )) # first 3 coordinate first vector print ( vector [ 0 ][: 3 ]) 2 [ - 0.024603435769677162 , - 0.007543657906353474 , 0.0039630369283258915 ] async: vector = await embed . aembed_query ( input_text ) print ( vector [: 3 ]) # multiple: # await embed.aembed_documents(input_texts) [ - 0.009100092574954033 , 0.005071679595857859 , - 0.0029193938244134188 ] create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param allowed_special : literal [ 'all' ] | set [ str ] | none = none # param azure_ad_token : secretstr | none [optional] # azure active directory token. automatically inferred env var azure_openai_ad_token provided. more: https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id . constraint : type = string writeonly = true format = password param azure_ad_token_provider : callable [ [ ] , str ] | none = none # function return azure active directory token. invoked every request. param azure_endpoint : str | none [optional] # azure endpoint, including resource. automatically inferred env var azure_openai_endpoint provided. example: https://example-resource.azure.openai.com/ param check_embedding_ctx_length : bool = true # whether check token length input automatically split input
longer embedding_ctx_length. param chunk_size : int = 2048 # maximum number text embed batch param default_headers : mapping [ str , str ] | none = none # param default_query : mapping [ str , object ] | none = none # param deployment : str | none = none (alias 'azure_deployment') # model deployment. given set base client url include /deployments/{azure_deployment} .
note: mean won’t able use non-deployment endpoints. param dimension : int | none = none # number dimension resulting output embeddings have. supported text-embedding-3 later models. param disallowed_special : literal [ 'all' ] | set [ str ] | sequence [ str ] | none = none # param embedding_ctx_length : int = 8191 # maximum number token embed once. param header : = none # param http_async_client : | none = none # optional httpx.asyncclient. used async invocations. must specify
http_async_client well you’d like custom client async invocations. param max_retries : int = 2 # maximum number retries make generating. param model : str = 'text-embedding-ada-002' # param model_kwargs : dict [ str , ] [optional] # hold model parameter valid create call explicitly specified. param openai_api_base : str | none [optional] (alias 'base_url') # base url path api requests, leave blank using proxy service
emulator. param openai_api_key : secretstr | none [optional] (alias 'api_key') # automatically inferred env var azure_openai_api_key provided. constraint : type = string writeonly = true format = password param openai_api_type : str | none [optional] # param openai_api_version : str | none [optional] (alias 'api_version') # automatically inferred env var openai_api_version provided. set “2023-05-15” default env variable openai_api_version set. param openai_organization : str | none [optional] (alias 'organization') # automatically inferred env var openai_org_id provided. param openai_proxy : str | none [optional] # param request_timeout : float | tuple [ float , float ] | | none = none (alias 'timeout') # timeout request openai completion api. float, httpx.timeout
none. param retry_max_seconds : int = 20 # max number second wait retries param retry_min_seconds : int = 4 # min number second wait retries param show_progress_bar : bool = false # whether show progress bar embedding. param skip_empty : bool = false # whether skip empty string embedding raise error.
default skipping. param tiktoken_enabled : bool = true # set false non-openai implementation embeddings api, e.g.
–extensions openai extension text-generation-webui param tiktoken_model_name : str | none = none # model name pas tiktoken using class.
tiktoken called, specify model name use here. param validate_base_url : bool = true # async aembed_documents ( text : list [ str ] , chunk_size : int | none = 0 ) → list [ list [ float ] ] # call openai’s embedding endpoint async embedding search docs. parameter : text ( list [ str ] ) – list text embed. chunk_size ( int | none ) – chunk size embeddings. none, use chunk size
specified class. return : list embeddings, one text. return type : list [ list [float]] async aembed_query ( text : str ) → list [ float ] # call openai’s embedding endpoint async embedding query text. parameter : text ( str ) – text embed. return : embedding text. return type : list [float] embed_documents ( text : list [ str ] , chunk_size : int | none = 0 ) → list [ list [ float ] ] # call openai’s embedding endpoint embedding search docs. parameter : text ( list [ str ] ) – list text embed. chunk_size ( int | none ) – chunk size embeddings. none, use chunk size
specified class. return : list embeddings, one text. return type : list [ list [float]] embed_query ( text : str ) → list [ float ] # call openai’s embedding endpoint embedding query text. parameter : text ( str ) – text embed. return : embedding text. return type : list [float] example using azureopenaiembeddings azure ai search azure cosmos db sql azureaisearchretriever azureopenaiembeddings microsoft
page: https://python.langchain.com/v0.2/api_reference/openai/embeddings/langchain_openai.embeddings.base.openaiembeddings.html#langchain_openai.embeddings.base.openaiembeddings
openaiembeddings # class langchain_openai.embeddings.base. openaiembeddings [source] # bases: basemodel , embeddings openai embedding model integration. setup: install langchain_openai set environment variable openai_api_key . pip install -u langchain_openai export openai_api_key = "your-api-key" key init args — embedding params: model: str name openai model use. dimensions: optional[int] = none number dimension resulting output embeddings have.
supported text-embedding-3 later models. key init args — client params: api_key: optional[secretstr] = none openai api key. organization: optional[str] = none openai organization id. passed read
env var openai_org_id. max_retries: int = 2 maximum number retries make generating. request_timeout: optional[union[float, tuple[float, float], any]] = none timeout request openai completion api see full list supported init args description params section. instantiate: langchain_openai import openaiembeddings embed = openaiembeddings ( model = "text-embedding-3-large" # `text-embedding-3` class # models, specify size # embeddings want returned. # dimensions=1024 ) embed single text: input_text = "the meaning life 42" vector = embeddings . embed_query ( "hello" ) print ( vector [: 3 ]) [ - 0.024603435769677162 , - 0.007543657906353474 , 0.0039630369283258915 ] embed multiple texts: vector = embeddings . embed_documents ([ "hello" , "goodbye" ]) # showing first 3 coordinate print ( len ( vector )) print ( vector [ 0 ][: 3 ]) 2 [ - 0.024603435769677162 , - 0.007543657906353474 , 0.0039630369283258915 ] async: await embed . aembed_query ( input_text ) print ( vector [: 3 ]) # multiple: # await embed.aembed_documents(input_texts) [ - 0.009100092574954033 , 0.005071679595857859 , - 0.0029193938244134188 ] create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param allowed_special : literal [ 'all' ] | set [ str ] | none = none # param check_embedding_ctx_length : bool = true # whether check token length input automatically split input
longer embedding_ctx_length. param chunk_size : int = 1000 # maximum number text embed batch param default_headers : mapping [ str , str ] | none = none # param default_query : mapping [ str , object ] | none = none # param deployment : str | none = 'text-embedding-ada-002' # param dimension : int | none = none # number dimension resulting output embeddings have. supported text-embedding-3 later models. param disallowed_special : literal [ 'all' ] | set [ str ] | sequence [ str ] | none = none # param embedding_ctx_length : int = 8191 # maximum number token embed once. param header : = none # param http_async_client : | none = none # optional httpx.asyncclient. used async invocations. must specify
emulator. param openai_api_key : secretstr | none [optional] (alias 'api_key') # automatically inferred env var openai_api_key provided. constraint : type = string writeonly = true format = password param openai_api_type : str | none [optional] # param openai_api_version : str | none [optional] (alias 'api_version') # automatically inferred env var openai_api_version provided. param openai_organization : str | none [optional] (alias 'organization') # automatically inferred env var openai_org_id provided. param openai_proxy : str | none [optional] # param request_timeout : float | tuple [ float , float ] | | none = none (alias 'timeout') # timeout request openai completion api. float, httpx.timeout
tiktoken called, specify model name use here. async aembed_documents ( text : list [ str ] , chunk_size : int | none = 0 ) → list [ list [ float ] ] [source] # call openai’s embedding endpoint async embedding search docs. parameter : text ( list [ str ] ) – list text embed. chunk_size ( int | none ) – chunk size embeddings. none, use chunk size
specified class. return : list embeddings, one text. return type : list [ list [float]] async aembed_query ( text : str ) → list [ float ] [source] # call openai’s embedding endpoint async embedding query text. parameter : text ( str ) – text embed. return : embedding text. return type : list [float] embed_documents ( text : list [ str ] , chunk_size : int | none = 0 ) → list [ list [ float ] ] [source] # call openai’s embedding endpoint embedding search docs. parameter : text ( list [ str ] ) – list text embed. chunk_size ( int | none ) – chunk size embeddings. none, use chunk size
specified class. return : list embeddings, one text. return type : list [ list [float]] embed_query ( text : str ) → list [ float ] [source] # call openai’s embedding endpoint embedding query text. parameter : text ( str ) – text embed. return : embedding text. return type : list [float] example using openaiembeddings activeloop deep lake activeloop deep memory alibaba cloud opensearch amazon document db analyticdb apache cassandra apache doris apify dataset astra db (cassandra) azure ai search azure cosmos db mongo vcore azureaisearchretriever build pdf ingestion question/answering system build query analysis system build question/answering system sql data build retrieval augmented generation (rag) app build agent agentexecutor (legacy) caching china mobile ecloud elasticsearch vectorsearch chroma confident conversational rag databricks vector search deep lake dingodb docarray docarray hnswsearch docarray inmemorysearch docugami duckdb elasticsearch epsilla faiss (async) flashrank reranker fleet ai context hippo hologres deal high cardinality categoricals query analysis add chat history add retrieval chatbots add score retriever result add value chain’s state best prompt graph-rag better prompt sql question-answering combine result multiple retriever convert runnables tool create query vector store deal large database sql question-answering “self-querying” retrieval per-user retrieval retrieval contextual compression get rag application add citation get rag application return source handle case query generated handle long text extraction handle multiple query query analysis handle multiple retriever query analysis inspect runnables invoke runnables parallel load pdfs pas argument one step next retrieve using multiple vector per document route sub-chains select example maximal marginal relevance (mmr) select example similarity split text based semantic similarity stream result rag application stream runnables use time-weighted vector store retriever use vectorstore retriever use shot example use shot example chat model use langchain indexing api use multiqueryretriever use parent document retriever hybrid search identity-enabled rag using pebbloretrievalqa image caption jaguar vector database jaguardb vector database javelin ai gateway kdb.ai kinetica vectorstore api kinetica vectorstore based retriever llmlingua document compressor lotr (merger retriever) lancedb lantern load doc meilisearch milvus milvus hybrid search retriever model cache momento vector index (mvi) mongodb atlas myscale neo4j vector index openai openaiembeddings opensearch pgvector (postgres) pinecone pinecone hybrid search postgres embedding psychic qdrant ragatouille rankllm reranker rephrasequery redis rockset sap hana cloud vector engine svm singlestoredb starrocks supabase (postgres) tencent cloud vectordb text embedding model tidb vector tigris timescale vector (postgres) timescale vector (postgres) typesense usearch uptrain upstash vector vector store retriever weaviate xata yellowbrick youtube audio zilliz knn scikit-learn viking db
page: https://python.langchain.com/v0.2/api_reference/openai/llms.html#langchain-openai-llms
llm # class llms.azure.azureopenai azure-specific openai large language models. llms.base.baseopenai base openai large language model class. llms.base.openai openai completion model integration.
page: https://python.langchain.com/v0.2/api_reference/openai/llms/langchain_openai.llms.azure.azureopenai.html#langchain_openai.llms.azure.azureopenai
azureopenai # class langchain_openai.llms.azure. azureopenai [source] # bases: baseopenai azure-specific openai large language models. use, openai python package installed,
environment variable openai_api_key set api key. parameter valid passed openai.create call passed
in, even explicitly saved class. example langchain_openai import azureopenai openai = azureopenai ( model_name = "gpt-3.5-turbo-instruct" ) note azureopenai implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param allowed_special : literal [ 'all' ] | abstractset [ str ] = {} # set special token allowed。 param azure_ad_token : secretstr | none [optional] # azure active directory token. automatically inferred env var azure_openai_ad_token provided. more: https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id . constraint : type = string writeonly = true format = password param azure_ad_token_provider : callable [ [ ] , str ] | none = none # function return azure active directory token. invoked every request. param azure_endpoint : str | none [optional] # azure endpoint, including resource. automatically inferred env var azure_openai_endpoint provided. example: https://example-resource.azure.openai.com/ param batch_size : int = 20 # batch size use passing multiple document generate. param best_of : int = 1 # generates best_of completion server-side return “best”. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param default_headers : mapping [ str , str ] | none = none # param default_query : mapping [ str , object ] | none = none # param deployment_name : str | none = none (alias 'azure_deployment') # model deployment. given set base client url include /deployments/{azure_deployment} .
note: mean won’t able use non-deployment endpoints. param disallowed_special : literal [ 'all' ] | collection [ str ] = 'all' # set special token allowed。 param extra_body : mapping [ str , ] | none = none # optional additional json property include request parameter
making request openai compatible apis, vllm. param frequency_penalty : float = 0 # penalizes repeated token according frequency. param http_async_client : | none = none # optional httpx.asyncclient. used async invocations. must specify
http_async_client well you’d like custom client async invocations. param logit_bias : dict [ str , float ] | none [optional] # adjust probability specific token generated. param logprobs : int | none = none # include log probability logprobs likely output tokens,
well chosen tokens. param max_retries : int = 2 # maximum number retries make generating. param max_tokens : int = 256 # maximum number token generate completion.
-1 return many token possible given prompt
model maximal context size. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_kwargs : dict [ str , ] [optional] # hold model parameter valid create call explicitly specified. param model_name : str = 'gpt-3.5-turbo-instruct' (alias 'model') # model name use. param n : int = 1 # many completion generate prompt. param openai_api_base : str | none [optional] (alias 'base_url') # base url path api requests, leave blank using proxy service
emulator. param openai_api_key : secretstr | none [optional] (alias 'api_key') # automatically inferred env var openai_api_key provided. constraint : type = string writeonly = true format = password param openai_api_type : str | none [optional] # legacy, openai<1.0.0 support. param openai_api_version : str | none [optional] (alias 'api_version') # automatically inferred env var openai_api_version provided. param openai_organization : str | none [optional] (alias 'organization') # automatically inferred env var openai_org_id provided. param openai_proxy : str | none [optional] # param presence_penalty : float = 0 # penalizes repeated tokens. param request_timeout : float | tuple [ float , float ] | | none = none (alias 'timeout') # timeout request openai completion api. float, httpx.timeout
none. param seed : int | none = none # seed generation param streaming : bool = false # whether stream result not. param tag : list [ str ] | none = none # tag add run trace. param temperature : float = 0.7 # sampling temperature use. param tiktoken_model_name : str | none = none # model name pas tiktoken using class.
tiktoken called, specify model name use here. param top_p : float = 1 # total probability mass token consider step. param validate_base_url : bool = true # backwards compatibility. legacy val openai_api_base passed in, try
infer base_url azure_endpoint update accordingly. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) – prompt generate from. stop ( list [ str ] | none ) – stop word use generating. model output cut
functionality, logging streaming, throughout generation. tag ( list [ str ] | none ) – list tag associate prompt. metadata ( dict [ str , ] | none ) – metadata associate prompt. **kwargs ( ) – arbitrary additional keyword arguments. usually passed
model provider api call. return : generated text. raise : valueerror – prompt string. return type : str async abatch ( input : list [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : ) → list [ str ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
e.g., underlying runnable us api support batch mode. parameter : input ( list [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ] ) – list input runnable. config ( runnableconfig | list [ runnableconfig ] | none ) – config use invoking runnable.
default false. kwargs ( ) – additional keyword argument pas runnable. return : list output runnable. return type : list [str] async abatch_as_completed ( input : sequence [ input ] , config : runnableconfig | sequence [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → asynciterator [ tuple [ int , output | exception ] ] # run ainvoke parallel list inputs,
default false. kwargs ( | none ) – additional keyword argument pas runnable. yield : tuple index input output runnable. return type : asynciterator [ tuple [int, output | exception]] async agenerate ( prompt : list [ str ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , * , tag : list [ str ] | list [ list [ str ] ] | none = none , metadata : dict [ str , ] | list [ dict [ str , ] ] | none = none , run_name : str | list [ str ] | none = none , run_id : uuid | list [ uuid | none ] | none = none , ** kwargs : ) → llmresult # asynchronously pas sequence prompt model return generations. method make use batched call model expose batched
api. use method want to: take advantage batched calls, need output model top generated value, building chain agnostic underlying language model type (e.g., pure text completion model v chat models). parameter : prompt ( list [ str ] ) – list string prompts. stop ( list [ str ] | none ) – stop word use generating. model output cut
first occurrence substrings. callback ( list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] ) – callback pas through. used executing additional
functionality, logging streaming, throughout generation. tag ( list [ str ] | list [ list [ str ] ] | none ) – list tag associate prompt. provided, length
list must match length prompt list. metadata ( dict [ str , ] | list [ dict [ str , ] ] | none ) – list metadata dictionary associate prompt.
provided, length list must match length prompt
list. run_name ( str | list [ str ] | none ) – list run name associate prompt. provided,
length list must match length prompt list. run_id ( uuid | list [ uuid | none ] | none ) – list run id associate prompt. provided,
length list must match length prompt list. **kwargs ( ) – arbitrary additional keyword arguments. usually passed
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult async agenerate_prompt ( prompt : list [ promptvalue ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , ** kwargs : ) → llmresult # asynchronously pas sequence prompt return model generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult async ainvoke ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → str # default implementation ainvoke, call invoke thread. default implementation allows usage async code even
runnable implement native async version invoke. subclass override method run asynchronously. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – config ( runnableconfig | none ) – stop ( list [ str ] | none ) – kwargs ( ) – return type : str async apredict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use ainvoke instead. parameter : text ( str ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : str async apredict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use ainvoke instead. parameter : message ( list [ basemessage ] ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : basemessage async astream ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → asynciterator [ str ] # default implementation astream, call ainvoke.
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : asynciterator [str] astream_events ( input : , config : runnableconfig | none = none , * , version : literal [ 'v1' , 'v2' ] , include_names : sequence [ str ] | none = none , include_types : sequence [ str ] | none = none , include_tags : sequence [ str ] | none = none , exclude_names : sequence [ str ] | none = none , exclude_types : sequence [ str ] | none = none , exclude_tags : sequence [ str ] | none = none , ** kwargs : ) → asynciterator [ standardstreamevent | customstreamevent ] # beta api beta may change future. generate stream events. use create iterator streamevents provide real-time information
astream_events built top astream_log. yield : async stream streamevents. raise : notimplementederror – version v1 v2 . return type : asynciterator [ standardstreamevent | customstreamevent ] batch ( input : list [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : ) → list [ str ] # default implementation run invoke parallel using thread pool executor. default implementation batch work well io bound runnables. subclass override method batch efficiently;
e.g., underlying runnable us api support batch mode. parameter : input ( list [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ] ) – config ( runnableconfig | list [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( ) – return type : list [str] batch_as_completed ( input : sequence [ input ] , config : runnableconfig | sequence [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → iterator [ tuple [ int , output | exception ] ] # run invoke parallel list inputs,
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) → runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) – configurablefield instance used select
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) → runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) – dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) create_llm_result ( choice : , prompt : list [ str ] , params : dict [ str , ] , token_usage : dict [ str , int ] , * , system_fingerprint : str | none = none ) → llmresult # create llmresult choice prompts. parameter : choice ( ) – prompt ( list [ str ] ) – params ( dict [ str , ] ) – token_usage ( dict [ str , int ] ) – system_fingerprint ( str | none ) – return type : llmresult generate ( prompt : list [ str ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , * , tag : list [ str ] | list [ list [ str ] ] | none = none , metadata : dict [ str , ] | list [ dict [ str , ] ] | none = none , run_name : str | list [ str ] | none = none , run_id : uuid | list [ uuid | none ] | none = none , ** kwargs : ) → llmresult # pas sequence prompt model return generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult generate_prompt ( prompt : list [ promptvalue ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , ** kwargs : ) → llmresult # pas sequence prompt model return model generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int # get number token present text. useful checking input fit model’s context window. parameter : text ( str ) – string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # get number token messages. useful checking input fit model’s context window. parameter : message ( list [ basemessage ] ) – message input tokenize. return : sum number token across messages. return type : int get_sub_prompts ( params : dict [ str , ] , prompt : list [ str ] , stop : list [ str ] | none = none ) → list [ list [ str ] ] # get sub prompt llm call. parameter : params ( dict [ str , ] ) – prompt ( list [ str ] ) – stop ( list [ str ] | none ) – return type : list [ list [str]] get_token_ids ( text : str ) → list [ int ] # get token id using tiktoken package. parameter : text ( str ) – return type : list [int] invoke ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → str # transform single input output. override implement. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use invoking runnable.
details. stop ( list [ str ] | none ) – kwargs ( ) – return : output runnable. return type : str max_tokens_for_prompt ( prompt : str ) → int # calculate maximum number token possible generate prompt. parameter : prompt ( str ) – prompt pas model. return : maximum number token generate prompt. return type : int example max_tokens = openai . max_token_for_prompt ( "tell joke." ) static modelname_to_contextsize ( modelname : str ) → int # calculate maximum number token possible generate model. parameter : modelname ( str ) – modelname want know context size for. return : maximum context size return type : int example max_tokens = openai . modelname_to_contextsize ( "gpt-3.5-turbo-instruct" ) predict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : text ( str ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : str predict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : basemessage save ( file_path : path | str ) → none # save llm. parameter : file_path ( path | str ) – path file save llm to. raise : valueerror – file path string path object. return type : none example:
.. code-block:: python llm.save(file_path=”path/llm.yaml”) stream ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → iterator [ str ] # default implementation stream, call invoke.
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : iterator [str] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] property max_context_size : int # get max context size model. example using azureopenai azure openai microsoft openai
page: https://python.langchain.com/v0.2/api_reference/openai/llms/langchain_openai.llms.base.baseopenai.html#langchain_openai.llms.base.baseopenai
baseopenai # class langchain_openai.llms.base. baseopenai [source] # bases: basellm base openai large language model class. note baseopenai implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param allowed_special : literal [ 'all' ] | abstractset [ str ] = {} # set special token allowed。 param batch_size : int = 20 # batch size use passing multiple document generate. param best_of : int = 1 # generates best_of completion server-side return “best”. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param default_headers : mapping [ str , str ] | none = none # param default_query : mapping [ str , object ] | none = none # param disallowed_special : literal [ 'all' ] | collection [ str ] = 'all' # set special token allowed。 param extra_body : mapping [ str , ] | none = none # optional additional json property include request parameter
emulator. param openai_api_key : secretstr | none [optional] (alias 'api_key') # automatically inferred env var openai_api_key provided. constraint : type = string writeonly = true format = password param openai_organization : str | none [optional] (alias 'organization') # automatically inferred env var openai_org_id provided. param openai_proxy : str | none [optional] # param presence_penalty : float = 0 # penalizes repeated tokens. param request_timeout : float | tuple [ float , float ] | | none = none (alias 'timeout') # timeout request openai completion api. float, httpx.timeout
tiktoken called, specify model name use here. param top_p : float = 1 # total probability mass token consider step. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) – prompt generate from. stop ( list [ str ] | none ) – stop word use generating. model output cut
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) → runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) – dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) create_llm_result ( choice : , prompt : list [ str ] , params : dict [ str , ] , token_usage : dict [ str , int ] , * , system_fingerprint : str | none = none ) → llmresult [source] # create llmresult choice prompts. parameter : choice ( ) – prompt ( list [ str ] ) – params ( dict [ str , ] ) – token_usage ( dict [ str , int ] ) – system_fingerprint ( str | none ) – return type : llmresult generate ( prompt : list [ str ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , * , tag : list [ str ] | list [ list [ str ] ] | none = none , metadata : dict [ str , ] | list [ dict [ str , ] ] | none = none , run_name : str | list [ str ] | none = none , run_id : uuid | list [ uuid | none ] | none = none , ** kwargs : ) → llmresult # pas sequence prompt model return generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int # get number token present text. useful checking input fit model’s context window. parameter : text ( str ) – string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # get number token messages. useful checking input fit model’s context window. parameter : message ( list [ basemessage ] ) – message input tokenize. return : sum number token across messages. return type : int get_sub_prompts ( params : dict [ str , ] , prompt : list [ str ] , stop : list [ str ] | none = none ) → list [ list [ str ] ] [source] # get sub prompt llm call. parameter : params ( dict [ str , ] ) – prompt ( list [ str ] ) – stop ( list [ str ] | none ) – return type : list [ list [str]] get_token_ids ( text : str ) → list [ int ] [source] # get token id using tiktoken package. parameter : text ( str ) – return type : list [int] invoke ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → str # transform single input output. override implement. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use invoking runnable.
details. stop ( list [ str ] | none ) – kwargs ( ) – return : output runnable. return type : str max_tokens_for_prompt ( prompt : str ) → int [source] # calculate maximum number token possible generate prompt. parameter : prompt ( str ) – prompt pas model. return : maximum number token generate prompt. return type : int example max_tokens = openai . max_token_for_prompt ( "tell joke." ) static modelname_to_contextsize ( modelname : str ) → int [source] # calculate maximum number token possible generate model. parameter : modelname ( str ) – modelname want know context size for. return : maximum context size return type : int example max_tokens = openai . modelname_to_contextsize ( "gpt-3.5-turbo-instruct" ) predict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : text ( str ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : str predict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : basemessage save ( file_path : path | str ) → none # save llm. parameter : file_path ( path | str ) – path file save llm to. raise : valueerror – file path string path object. return type : none example:
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : iterator [str] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] property max_context_size : int # get max context size model.
page: https://python.langchain.com/v0.2/api_reference/openai/llms/langchain_openai.llms.base.openai.html#langchain_openai.llms.base.openai
openai # class langchain_openai.llms.base. openai [source] # bases: baseopenai openai completion model integration. setup: install langchain-openai set environment variable openai_api_key . pip install -u langchain-openai export openai_api_key = "your-api-key" key init args — completion params: model: str name openai model use. temperature: float sampling temperature. max_tokens: optional[int] max number token generate. logprobs: optional[bool] whether return logprobs. stream_options: dict configure streaming outputs, like whether return token usage
streaming ( {"include_usage": true} ). key init args — client params: timeout: union[float, tuple[float, float], any, none] timeout requests. max_retries: int max number retries. api_key: optional[str] openai api key. passed read env var openai_api_key. base_url: optional[str] base url api requests. specify using proxy service
var openai_org_id. see full list supported init args description params section. instantiate: langchain_openai import openai llm = openai ( model = "gpt-3.5-turbo-instruct" , temperature = 0 , max_retries = 2 , # api_key="...", # base_url="...", # organization="...", # params... ) invoke: input_text = "the meaning life " llm . invoke ( input_text ) "a philosophical question debated thinker scholar centuries." stream: chunk llm . stream ( input_text ): print ( chunk , end = "|" ) a| philosophical| question| that| has| been| debated| by| thinkers| and| scholars| for| centuries|. "" . join ( llm . stream ( input_text )) "a philosophical question debated thinker scholar centuries." async: await llm . ainvoke ( input_text ) # stream: # async chunk (await llm.astream(input_text)): # print(chunk) # batch: # await llm.abatch([input_text]) "a philosophical question debated thinker scholar centuries." note openai implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param allowed_special : literal [ 'all' ] | abstractset [ str ] = {} # set special token allowed。 param batch_size : int = 20 # batch size use passing multiple document generate. param best_of : int = 1 # generates best_of completion server-side return “best”. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param default_headers : mapping [ str , str ] | none = none # param default_query : mapping [ str , object ] | none = none # param disallowed_special : literal [ 'all' ] | collection [ str ] = 'all' # set special token allowed。 param extra_body : mapping [ str , ] | none = none # optional additional json property include request parameter
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : iterator [str] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] property max_context_size : int # get max context size model. example using openai # example # legacy aws lambda aim amazon document db amazon textract apache doris apify dataset argilla astra db (cassandra) azure ai service toolkit azure cognitive service toolkit chroma clearml clickup toolkit comet comet tracing confident dspy dall-e image generator databricks vector search deep lake dingodb docugami elasticsearch eleven lab text2speech fiddler generate synthetic data gitlab toolkit google drive google finance google job google serper gradio graphql helicone add fallback runnable cache llm response retrieval contextual compression reorder retrieved result mitigate “lost middle” effect retry parsing error occurs stream response llm track token usage llm use output parser parse llm response structured format human tool identity-enabled rag using pebbloretrievalqa infino ionic shopping tool json toolkit jaguar vector database jira toolkit llmonitor label studio langchain decorator ✨ layerup security lemon agent log10 marqo milvus model cache mongodb atlas motörhead nasa toolkit natural language api toolkits networkx office365 toolkit opaqueprompts openai openapi toolkit opensearch openweathermap pgvector (postgres) panda dataframe pinecone promptlayer psychic qdrant ray serve rebuff redis sagemaker tracking scenexplain searchapi serper - google search api shale protocol starrocks steam toolkit streamlit supabase (postgres) timescale vector (postgres) timescale vector (postgres) trubrics voyageai reranker wandb tracing weaviate weight & bias whylabs zapier natural language action zep cloud memory zep open source memory
page: https://python.langchain.com/v0.2/api_reference/anthropic/index.html
langchain-anthropic: 0.1.23 # chat_models # class chat_models.anthropictool anthropic tool definition. chat_models.chatanthropic anthropic chat models. function chat_models.convert_to_anthropic_tool (tool) convert tool-like object anthropic tool definition. deprecated class chat_models.chatanthropicmessages deprecated since version 0.1.0: use chatanthropic instead. experimental # function experimental.get_system_message (tools) generate system message describes available tools. deprecated class experimental.chatanthropictools deprecated since version 0.1.5: tool-calling officially supported anthropic api workaround longer needed. use chatanthropic instead. llm # class llms.anthropicllm anthropic large language model. deprecated class llms.anthropic deprecated since version 0.1.0: use anthropicllm instead. output_parsers # class output_parsers.toolsoutputparser output parser tool calls. function output_parsers.extract_tool_calls (content) extract tool call list content blocks.
page: https://python.langchain.com/v0.2/api_reference/anthropic/chat_models.html#langchain-anthropic-chat-models
chat_models # class chat_models.anthropictool anthropic tool definition. chat_models.chatanthropic anthropic chat models. function chat_models.convert_to_anthropic_tool (tool) convert tool-like object anthropic tool definition. deprecated class chat_models.chatanthropicmessages deprecated since version 0.1.0: use chatanthropic instead.
page: https://python.langchain.com/v0.2/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.anthropictool.html#langchain_anthropic.chat_models.anthropictool
anthropictool # class langchain_anthropic.chat_models. anthropictool [source] # anthropic tool definition. name : str # description : str # input_schema : dict [ str , ] # cache_control : dict [ str , str ] #
page: https://python.langchain.com/v0.2/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.chatanthropic.html#langchain_anthropic.chat_models.chatanthropic
chatanthropic # class langchain_anthropic.chat_models. chatanthropic [source] # bases: basechatmodel anthropic chat models. see https://docs.anthropic.com/en/docs/models-overview list latest models. setup: install langchain-anthropic set environment variable anthropic_api_key . pip install -u langchain-anthropic export anthropic_api_key = "your-api-key" key init args — completion params: model: str name anthropic model use. e.g. “claude-3-sonnet-20240229”. temperature: float sampling temperature. range 0.0 1.0. max_tokens: optional[int] max number token generate. key init args — client params: timeout: optional[float] timeout requests. max_retries: int max number retries request fails. api_key: optional[str] anthropic api key. passed read env var anthropic_api_key. base_url: optional[str] base url api requests. specify using proxy service
emulator. see full list supported init args description params section. instantiate: langchain_anthropic import chatanthropic llm = chatanthropic ( model = "claude-3-sonnet-20240229" , temperature = 0 , max_tokens = 1024 , timeout = none , max_retries = 2 , # api_key="...", # base_url="...", # params... ) note : param explicitly supported passed directly anthropic.anthropic.messages.create(...) api every time model
invoked. example: langchain_anthropic import chatanthropic import anthropic chatanthropic ( ... , extra_headers = {}) . invoke ( ... ) # result underlying api call of: anthropic . anthropic ( .. ) . message . create ( ... , extra_headers = {}) # also equivalent to: chatanthropic ( ... ) . invoke ( ... , extra_headers = {}) invoke: message = [ ( "system" , "you helpful translator. translate user sentence french." ), ( "human" , "i love programming." ), ] llm . invoke ( message ) aimessage ( content = "j'aime la programmation." , response_metadata = { 'id' : 'msg_01trik66aiq9z1higrd5xfx3' , 'model' : 'claude-3-sonnet-20240229' , 'stop_reason' : 'end_turn' , 'stop_sequence' : none , 'usage' : { 'input_tokens' : 25 , 'output_tokens' : 11 }}, id = 'run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0' , usage_metadata = { 'input_tokens' : 25 , 'output_tokens' : 11 , 'total_tokens' : 36 }) stream: chunk llm . stream ( message ): print ( chunk ) aimessagechunk ( content = 'j' , id = 'run-272ff5f9-8485-402c-b90d-eac8babc5b25' ) aimessagechunk ( content = "'" , id = 'run-272ff5f9-8485-402c-b90d-eac8babc5b25' ) aimessagechunk ( content = 'a' , id = 'run-272ff5f9-8485-402c-b90d-eac8babc5b25' ) aimessagechunk ( content = 'ime' , id = 'run-272ff5f9-8485-402c-b90d-eac8babc5b25' ) aimessagechunk ( content = ' la' , id = 'run-272ff5f9-8485-402c-b90d-eac8babc5b25' ) aimessagechunk ( content = ' programm' , id = 'run-272ff5f9-8485-402c-b90d-eac8babc5b25' ) aimessagechunk ( content = 'ation' , id = 'run-272ff5f9-8485-402c-b90d-eac8babc5b25' ) aimessagechunk ( content = '.' , id = 'run-272ff5f9-8485-402c-b90d-eac8babc5b25' ) stream = llm . stream ( message ) full = next ( stream ) chunk stream : full += chunk full aimessagechunk ( content = "j'aime la programmation." , id = 'run-b34faef0-882f-4869-a19c-ed2b856e6361' ) async: await llm . ainvoke ( message ) # stream: # async chunk (await llm.astream(messages)) # batch: # await llm.abatch([messages]) aimessage ( content = "j'aime la programmation." , response_metadata = { 'id' : 'msg_01trik66aiq9z1higrd5xfx3' , 'model' : 'claude-3-sonnet-20240229' , 'stop_reason' : 'end_turn' , 'stop_sequence' : none , 'usage' : { 'input_tokens' : 25 , 'output_tokens' : 11 }}, id = 'run-5886ac5f-3c2e-49f5-8a44-b1e92808c929-0' , usage_metadata = { 'input_tokens' : 25 , 'output_tokens' : 11 , 'total_tokens' : 36 }) tool calling: langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getpopulation ( basemodel ): '''get current population given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) llm_with_tools = llm . bind_tools ([ getweather , getpopulation ]) ai_msg = llm_with_tools . invoke ( "which city hotter today bigger: la ny?" ) ai_msg . tool_calls [{ 'name' : 'getweather' , 'args' : { 'location' : 'los angeles, ca' }, 'id' : 'toolu_01kzppeagzura7hpbqwhbwdo' }, { 'name' : 'getweather' , 'args' : { 'location' : 'new york, ny' }, 'id' : 'toolu_01jtgbvgvjbiswtzk3uycezx' }, { 'name' : 'getpopulation' , 'args' : { 'location' : 'los angeles, ca' }, 'id' : 'toolu_01429aygngesudv9ntbckguw' }, { 'name' : 'getpopulation' , 'args' : { 'location' : 'new york, ny' }, 'id' : 'toolu_01jpktyd44tvmebcppnfsejg' }] see chatanthropic.bind_tools() method more. structured output: typing import optional langchain_core.pydantic_v1 import basemodel , field class joke ( basemodel ): '''joke tell user.''' setup : str = field ( description = "the setup joke" ) punchline : str = field ( description = "the punchline joke" ) rating : optional [ int ] = field ( description = "how funny joke is, 1 10" ) structured_llm = llm . with_structured_output ( joke ) structured_llm . invoke ( "tell joke cats" ) joke ( setup = 'why cat sitting computer?' , punchline = 'to keep eye mouse!' , rating = none ) see chatanthropic.with_structured_output() more. image input: import base64 import httpx langchain_core.messages import humanmessage image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-gfp-wisconsin-madison-the-nature-boardwalk.jpg" image_data = base64 . b64encode ( httpx . get ( image_url ) . content ) . decode ( "utf-8" ) message = humanmessage ( content = [ { "type" : "text" , "text" : "describe weather image" }, { "type" : "image_url" , "image_url" : { "url" : f "data:image/jpeg;base64, { image_data } " }, }, ], ) ai_msg = llm . invoke ([ message ]) ai_msg . content "the image depicts sunny day partly cloudy sky. sky brilliant blue color scattered white cloud drifting across. lighting cloud pattern suggest pleasant, mild weather conditions. scene show grassy field meadow wooden boardwalk trail leading it, indicating outdoor setting nice day well-suited enjoying nature." token usage: ai_msg = llm . invoke ( message ) ai_msg . usage_metadata { 'input_tokens' : 25 , 'output_tokens' : 11 , 'total_tokens' : 36 } message chunk containing token usage included streaming
default: stream = llm . stream ( message ) full = next ( stream ) chunk stream : full += chunk full . usage_metadata { 'input_tokens' : 25 , 'output_tokens' : 11 , 'total_tokens' : 36 } disabled setting stream_usage=false stream method,
setting stream_usage=false initializing chatanthropic. response metadata ai_msg = llm . invoke ( message ) ai_msg . response_metadata { 'id' : 'msg_013xu6fhegeq76ap4rgfervt' , 'model' : 'claude-3-sonnet-20240229' , 'stop_reason' : 'end_turn' , 'stop_sequence' : none , 'usage' : { 'input_tokens' : 25 , 'output_tokens' : 11 }} note chatanthropic implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param anthropic_api_key : secretstr [optional] (alias 'api_key') # automatically read env var anthropic_api_key provided. constraint : type = string writeonly = true format = password param anthropic_api_url : str | none [optional] (alias 'base_url') # base url api requests. specify using proxy service emulator. value isn’t passed in, attempt read value first
anthropic_api_url set, anthropic_base_url.
neither set, default value ‘ https://api.anthropic.com ’
used. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param default_headers : mapping [ str , str ] | none = none # header pas anthropic clients, used every api call. param default_request_timeout : float | none = none (alias 'timeout') # timeout request anthropic completion api. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param max_retries : int = 2 # number retries allowed request sent anthropic completion api. param max_tokens : int = 1024 (alias 'max_tokens_to_sample') # denotes number token predict per generation. param metadata : dict [ str , ] | none = none # metadata add run trace. param model : str [required] (alias 'model_name') # model name use. param model_kwargs : dict [ str , ] [optional] # param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param stop_sequences : list [ str ] | none = none (alias 'stop') # default stop sequences. param stream_usage : bool = true # whether include usage metadata streaming output. true, additional
message chunk generated stream including usage metadata. param streaming : bool = false # whether use streaming not. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # non-negative float tune degree randomness generation. param top_k : int | none = none # number likely token consider step. param top_p : float | none = none # total probability mass token consider step. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , * , tool_choice : dict [ str , str ] | literal [ 'any' , 'auto' ] | str | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model. args: tools: list tool definition bind chat model. support anthropic format tool schema tool definition handled
langchain_core.utils.function_calling.convert_to_openai_tool() . tool_choice: tool require model call. option are: name tool (str): call corresponding tool; "auto" none: automatically selects tool (including tool); "any" : force least one tool called; dict form: {"type": "tool", "name": "tool_name"} ,
{"type: "any"} ,
{"type: "auto"} ; kwargs: additional parameter passed directly self.bind(**kwargs) . example: langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getprice ( basemodel ): '''get price specific product.''' product : str = field ( ... , description = "the product look up." ) llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) llm_with_tools = llm . bind_tools ([ getweather , getprice ]) llm_with_tools . invoke ( "what weather like san francisco" ,) # -> aimessage( # content=[ # {'text': ' based user’s question, relevant function call getweather, requires “location” parameter. user directly specified location “san francisco”. since san francisco well known city, reasonably infer mean san francisco, ca without needing state specified. required parameter provided, proceed api call.
’, ‘type’: ‘text’}, # {‘text’: none, ‘type’: ‘tool_use’, ‘id’: ‘toolu_01scgexkzq7eqskmhfygvyuu’, ‘name’: ‘getweather’, ‘input’: {‘location’: ‘san francisco, ca’}}
# ],
# response_metadata={‘id’: ‘msg_01gm3zqtofv8jgqmw7ablnhi’, ‘model’: ‘claude-3-opus-20240229’, ‘stop_reason’: ‘tool_use’, ‘stop_sequence’: none, ‘usage’: {‘input_tokens’: 487, ‘output_tokens’: 145}},
# id=’run-87b1331e-9251-4a68-acef-f0a018b639cc-0’
# ) example — force tool call tool_choice ‘any’: langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getprice ( basemodel ): '''get price specific product.''' product : str = field ( ... , description = "the product look up." ) llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) llm_with_tools = llm . bind_tools ([ getweather , getprice ], tool_choice = "any" ) llm_with_tools . invoke ( "what weather like san francisco" ,) example — force specific tool call tool_choice ‘’: langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getprice ( basemodel ): '''get price specific product.''' product : str = field ( ... , description = "the product look up." ) llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) llm_with_tools = llm . bind_tools ([ getweather , getprice ], tool_choice = "getweather" ) llm_with_tools . invoke ( "what weather like san francisco" ,) example — cache specific tools: langchain_anthropic import chatanthropic , convert_to_anthropic_tool langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getprice ( basemodel ): '''get price specific product.''' product : str = field ( ... , description = "the product look up." ) # convert pydantic class anthropic tool format # passing bind_tools set 'cache_control' # field tool. cached_price_tool = convert_to_anthropic_tool ( getprice ) # currently supported "cache_control" value # {"type": "ephemeral"}. cached_price_tool [ "cache_control" ] = { "type" : "ephemeral" } # need pas extra header enable use beta cache # control api. llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 , extra_headers = { "anthropic-beta" : "prompt-caching-2024-07-31" } ) llm_with_tools = llm . bind_tools ([ getweather , cached_price_tool ]) llm_with_tools . invoke ( "what weather like san francisco" ,) outputs:
.. code-block:: pycon aimessage(content=[{‘text’: “certainly! help find current weather san francisco. get information, i’ll use getweather function. let fetch data right away.”, ‘type’: ‘text’}, {‘id’: ‘toolu_01ts5h8lno7p5imcg7yriaum’, ‘input’: {‘location’: ‘san francisco, ca’}, ‘name’: ‘getweather’, ‘type’: ‘tool_use’}], response_metadata={‘id’: ‘msg_01xg7wr5infwgbxe5jh9rpro’, ‘model’: ‘claude-3-5-sonnet-20240620’, ‘stop_reason’: ‘tool_use’, ‘stop_sequence’: none, ‘usage’: {‘input_tokens’: 171, ‘output_tokens’: 96, ‘cache_creation_input_tokens’: 1470, ‘cache_read_input_tokens’: 0}}, id=’run-b36a5b54-5d69-470e-a1b0-b932d00b089e-0’, tool_calls=[{‘name’: ‘getweather’, ‘args’: {‘location’: ‘san francisco, ca’}, ‘id’: ‘toolu_01ts5h8lno7p5imcg7yriaum’, ‘type’: ‘tool_call’}], usage_metadata={‘input_tokens’: 171, ‘output_tokens’: 96, ‘total_tokens’: 267}) invoke tool again, see “usage” information aimessage.response_metadata show cache hit:
.. code-block:: pycon aimessage(content=[{‘text’: ‘to get current weather san francisco, use getweather function. let check you.’, ‘type’: ‘text’}, {‘id’: ‘toolu_01htvty1qhmfdpprx42qu2ea’, ‘input’: {‘location’: ‘san francisco, ca’}, ‘name’: ‘getweather’, ‘type’: ‘tool_use’}], response_metadata={‘id’: ‘msg_016rfwhrrvw6dagcdwb6ac64’, ‘model’: ‘claude-3-5-sonnet-20240620’, ‘stop_reason’: ‘tool_use’, ‘stop_sequence’: none, ‘usage’: {‘input_tokens’: 171, ‘output_tokens’: 82, ‘cache_creation_input_tokens’: 0, ‘cache_read_input_tokens’: 1470}}, id=’run-88b1f825-dcb7-4277-ac27-53df55d22001-0’, tool_calls=[{‘name’: ‘getweather’, ‘args’: {‘location’: ‘san francisco, ca’}, ‘id’: ‘toolu_01htvty1qhmfdpprx42qu2ea’, ‘type’: ‘tool_call’}], usage_metadata={‘input_tokens’: 171, ‘output_tokens’: 82, ‘total_tokens’: 253}) parameter : tool ( sequence [ dict [ str , ] | type | callable | basetool ] ) – tool_choice ( dict [ str , str ] | literal [ 'any' , 'auto' ] | str | none ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) – stop ( list [ str ] | none ) – kwargs ( ) – return type : str configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) → runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) – configurablefield instance used select
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int # get number token present text. useful checking input fit model’s context window. parameter : text ( str ) – string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # get number token messages. useful checking input fit model’s context window. parameter : message ( list [ basemessage ] ) – message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) → list [ int ] # return ordered id token text. parameter : text ( str ) – string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] invoke ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → basemessage # transform single input output. override implement. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use invoking runnable.
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , * , include_raw : bool = false , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] [source] # model wrapper return output formatted match given schema. parameter : schema ( dict | type [ basemodel ] ) – output schema. passed as: anthropic tool schema, openai function/tool schema, json schema, typeddict class (support added 0.1.22), pydantic class. schema pydantic class model output
schema field specifying pydantic typeddict class. changed version 0.1.22: added support typeddict class. include_raw ( bool ) – false parsed structured output returned.
key “raw”, “parsed”, “parsing_error”. kwargs ( ) – return : runnable take input langchain_core.language_models.chat.basechatmodel . include_raw false schema pydantic class, runnable output
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example: pydantic schema (include_raw=false): langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_anthropic import chatanthropic schema = { "name" : "answerwithjustification" , "description" : "an answer user question along justification answer." , "input_schema" : { "type" : "object" , "properties" : { "answer" : { "type" : "string" }, "justification" : { "type" : "string" }, }, "required" : [ "answer" , "justification" ] } } llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example using chatanthropic anthropic build agent chatanthropic conceptual guide add fallback runnable attach callback runnable configure runtime chain internals create custom output parser create dynamic (self-constructing) chain create custom callback handler filter message handle rate limit merge consecutive message type parse xml output pas callback runtime propagate callback constructor route sub-chains stream chat model response track token usage chatmodels use callback async environment use prompting alone (no tool calling) extraction log10 playwright browser toolkit response metadata riza code interpreter 🦜️🏓 langserve
page: https://python.langchain.com/v0.2/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.convert_to_anthropic_tool.html#langchain_anthropic.chat_models.convert_to_anthropic_tool
convert_to_anthropic_tool # langchain_anthropic.chat_models. convert_to_anthropic_tool ( tool : dict [ str , ] | type | callable | basetool ) → anthropictool [source] # convert tool-like object anthropic tool definition. parameter : tool ( dict [ str , ] | type | callable | basetool ) – return type : anthropictool
page: https://python.langchain.com/v0.2/api_reference/anthropic/chat_models/langchain_anthropic.chat_models.chatanthropicmessages.html#langchain_anthropic.chat_models.chatanthropicmessages
chatanthropicmessages # class langchain_anthropic.chat_models. chatanthropicmessages [source] # bases: chatanthropic deprecated since version 0.1.0: use chatanthropic instead. note chatanthropicmessages implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param anthropic_api_key : secretstr [optional] (alias 'api_key') # automatically read env var anthropic_api_key provided. constraint : type = string writeonly = true format = password param anthropic_api_url : str | none [optional] (alias 'base_url') # base url api requests. specify using proxy service emulator. value isn’t passed in, attempt read value first
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , * , tool_choice : dict [ str , str ] | literal [ 'any' , 'auto' ] | str | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] # bind tool-like object chat model. args: tools: list tool definition bind chat model. support anthropic format tool schema tool definition handled
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , * , include_raw : bool = false , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # model wrapper return output formatted match given schema. parameter : schema ( dict | type [ basemodel ] ) – output schema. passed as: anthropic tool schema, openai function/tool schema, json schema, typeddict class (support added 0.1.22), pydantic class. schema pydantic class model output
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example: pydantic schema (include_raw=false): langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_anthropic import chatanthropic schema = { "name" : "answerwithjustification" , "description" : "an answer user question along justification answer." , "input_schema" : { "type" : "object" , "properties" : { "answer" : { "type" : "string" }, "justification" : { "type" : "string" }, }, "required" : [ "answer" , "justification" ] } } llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # }
page: https://python.langchain.com/v0.2/api_reference/anthropic/experimental.html#langchain-anthropic-experimental
experimental # function experimental.get_system_message (tools) generate system message describes available tools. deprecated class experimental.chatanthropictools deprecated since version 0.1.5: tool-calling officially supported anthropic api workaround longer needed. use chatanthropic instead.
page: https://python.langchain.com/v0.2/api_reference/anthropic/experimental/langchain_anthropic.experimental.get_system_message.html#langchain_anthropic.experimental.get_system_message
get_system_message # langchain_anthropic.experimental. get_system_message ( tool : list [ dict ] ) → str [source] # generate system message describes available tools. parameter : tool ( list [ dict ] ) – return type : str
page: https://python.langchain.com/v0.2/api_reference/anthropic/experimental/langchain_anthropic.experimental.chatanthropictools.html#langchain_anthropic.experimental.chatanthropictools
chatanthropictools # class langchain_anthropic.experimental. chatanthropictools [source] # bases: chatanthropic deprecated since version 0.1.5: tool-calling officially supported anthropic api workaround longer needed. use chatanthropic instead. chat model interacting anthropic functions. note chatanthropictools implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param anthropic_api_key : secretstr [optional] (alias 'api_key') # automatically read env var anthropic_api_key provided. constraint : type = string writeonly = true format = password param anthropic_api_url : str | none [optional] (alias 'base_url') # base url api requests. specify using proxy service emulator. value isn’t passed in, attempt read value first
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example: pydantic schema (include_raw=false): langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_anthropic import chatanthropic langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_anthropic import chatanthropic schema = { "name" : "answerwithjustification" , "description" : "an answer user question along justification answer." , "input_schema" : { "type" : "object" , "properties" : { "answer" : { "type" : "string" }, "justification" : { "type" : "string" }, }, "required" : [ "answer" , "justification" ] } } llm = chatanthropic ( model = "claude-3-opus-20240229" , temperature = 0 ) structured_llm = llm . with_structured_output ( schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example using chatanthropictools [deprecated] experimental anthropic tool wrapper
page: https://python.langchain.com/v0.2/api_reference/anthropic/llms.html#langchain-anthropic-llms
llm # class llms.anthropicllm anthropic large language model. deprecated class llms.anthropic deprecated since version 0.1.0: use anthropicllm instead.
page: https://python.langchain.com/v0.2/api_reference/anthropic/llms/langchain_anthropic.llms.anthropicllm.html#langchain_anthropic.llms.anthropicllm
anthropicllm # class langchain_anthropic.llms. anthropicllm [source] # bases: llm , _anthropiccommon anthropic large language model. use, environment variable anthropic_api_key set api key, pas named parameter constructor. example langchain_anthropic import anthropicllm model = anthropicllm () note anthropicllm implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param ai_prompt : str | none = none # param human_prompt : str | none = none # param anthropic_api_key : secretstr [optional] (alias 'api_key') # automatically read env var anthropic_api_key provided. constraint : type = string writeonly = true format = password param anthropic_api_url : str | none [optional] (alias 'base_url') # base url api requests. specify using proxy service emulator. value isn’t passed in, attempt read value
anthropic_api_url. set, default value ‘ https://api.anthropic.com ’
used. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param count_tokens : callable [ [ str ] , int ] | none = none # param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param default_request_timeout : float | none = none # timeout request anthropic completion api. default 600 seconds. param max_retries : int = 2 # number retries allowed request sent anthropic completion api. param max_tokens_to_sample : int = 1024 (alias 'max_tokens') # denotes number token predict per generation. param metadata : dict [ str , ] | none = none # metadata add run trace. param model : str = 'claude-2' (alias 'model_name') # model name use. param model_kwargs : dict [ str , ] [optional] # param streaming : bool = false # whether stream results. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # non-negative float tune degree randomness generation. param top_k : int | none = none # number likely token consider step. param top_p : float | none = none # total probability mass token consider step. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) – prompt generate from. stop ( list [ str ] | none ) – stop word use generating. model output cut
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) → runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) – dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) convert_prompt ( prompt : promptvalue ) → str [source] # parameter : prompt ( promptvalue ) – return type : str generate ( prompt : list [ str ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , * , tag : list [ str ] | list [ list [ str ] ] | none = none , metadata : dict [ str , ] | list [ dict [ str , ] ] | none = none , run_name : str | list [ str ] | none = none , run_id : uuid | list [ uuid | none ] | none = none , ** kwargs : ) → llmresult # pas sequence prompt model return generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int [source] # calculate number tokens. parameter : text ( str ) – return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # get number token messages. useful checking input fit model’s context window. parameter : message ( list [ basemessage ] ) – message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) → list [ int ] # return ordered id token text. parameter : text ( str ) – string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] invoke ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → str # transform single input output. override implement. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use invoking runnable.
details. stop ( list [ str ] | none ) – kwargs ( ) – return : output runnable. return type : str predict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : text ( str ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : str predict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( sequence [ str ] | none ) – kwargs ( ) – return type : basemessage save ( file_path : path | str ) → none # save llm. parameter : file_path ( path | str ) – path file save llm to. raise : valueerror – file path string path object. return type : none example:
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : iterator [str] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example using anthropicllm anthropic anthropicllm
page: https://python.langchain.com/v0.2/api_reference/anthropic/llms/langchain_anthropic.llms.anthropic.html#langchain_anthropic.llms.anthropic
anthropic # class langchain_anthropic.llms. anthropic [source] # bases: anthropicllm deprecated since version 0.1.0: use anthropicllm instead. anthropic large language model. note anthropic implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param ai_prompt : str | none = none # param human_prompt : str | none = none # param anthropic_api_key : secretstr [optional] (alias 'api_key') # automatically read env var anthropic_api_key provided. constraint : type = string writeonly = true format = password param anthropic_api_url : str | none [optional] (alias 'base_url') # base url api requests. specify using proxy service emulator. value isn’t passed in, attempt read value
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) → runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) – dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) convert_prompt ( prompt : promptvalue ) → str # parameter : prompt ( promptvalue ) – return type : str generate ( prompt : list [ str ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , * , tag : list [ str ] | list [ list [ str ] ] | none = none , metadata : dict [ str , ] | list [ dict [ str , ] ] | none = none , run_name : str | list [ str ] | none = none , run_id : uuid | list [ uuid | none ] | none = none , ** kwargs : ) → llmresult # pas sequence prompt model return generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int # calculate number tokens. parameter : text ( str ) – return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # get number token messages. useful checking input fit model’s context window. parameter : message ( list [ basemessage ] ) – message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) → list [ int ] # return ordered id token text. parameter : text ( str ) – string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] invoke ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → str # transform single input output. override implement. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use invoking runnable.
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : iterator [str] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ]
page: https://python.langchain.com/v0.2/api_reference/anthropic/output_parsers.html#langchain-anthropic-output-parsers
output_parsers # class output_parsers.toolsoutputparser output parser tool calls. function output_parsers.extract_tool_calls (content) extract tool call list content blocks.
page: https://python.langchain.com/v0.2/api_reference/anthropic/output_parsers/langchain_anthropic.output_parsers.toolsoutputparser.html#langchain_anthropic.output_parsers.toolsoutputparser
toolsoutputparser # class langchain_anthropic.output_parsers. toolsoutputparser [source] # bases: basegenerationoutputparser output parser tool calls. note toolsoutputparser implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param args_only : bool = false # whether return argument tool calls. param first_tool_only : bool = false # whether return first tool call. param pydantic_schemas : list [ type [ basemodel ] ] | none = none # pydantic schema parse tool call into. async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
default false. kwargs ( | none ) – additional keyword argument pas runnable. yield : tuple index input output runnable. return type : asynciterator [ tuple [int, output | exception]] async ainvoke ( input : str | basemessage , config : runnableconfig | none = none , ** kwargs : | none ) → # default implementation ainvoke, call invoke thread. default implementation allows usage async code even
runnable implement native async version invoke. subclass override method run asynchronously. parameter : input ( str | basemessage ) – config ( runnableconfig | none ) – kwargs ( | none ) – return type : async aparse_result ( result : list [ generation ] , * , partial : bool = false ) → # async parse list candidate model generation specific format. parameter : result ( list [ generation ] ) – list generation parsed. generation assumed
different candidate output single model input. partial ( bool ) – whether parse output partial result. useful
parser parse partial results. default false. return : structured output. return type : async astream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) → asynciterator [ output ] # default implementation astream, call ainvoke.
subclass override method support streaming output. parameter : input ( input ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( | none ) – additional keyword argument pas runnable. yield : output runnable. return type : asynciterator [ output ] astream_events ( input : , config : runnableconfig | none = none , * , version : literal [ 'v1' , 'v2' ] , include_names : sequence [ str ] | none = none , include_types : sequence [ str ] | none = none , include_tags : sequence [ str ] | none = none , exclude_names : sequence [ str ] | none = none , exclude_types : sequence [ str ] | none = none , exclude_tags : sequence [ str ] | none = none , ** kwargs : ) → asynciterator [ standardstreamevent | customstreamevent ] # beta api beta may change future. generate stream events. use create iterator streamevents provide real-time information
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) → runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) – dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) invoke ( input : str | basemessage , config : runnableconfig | none = none ) → # transform single input output. override implement. parameter : input ( str | basemessage ) – input runnable. config ( runnableconfig | none ) – config use invoking runnable.
details. return : output runnable. return type : parse_result ( result : list [ generation ] , * , partial : bool = false ) → [source] # parse list candidate model generation specific format. parameter : result ( list [ generation ] ) – list generation parsed. generation assumed
different candidate output single model input. partial ( bool ) – return : structured output. return type : stream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) → iterator [ output ] # default implementation stream, call invoke.
subclass override method support streaming output. parameter : input ( input ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( | none ) – additional keyword argument pas runnable. yield : output runnable. return type : iterator [ output ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented
page: https://python.langchain.com/v0.2/api_reference/anthropic/output_parsers/langchain_anthropic.output_parsers.extract_tool_calls.html#langchain_anthropic.output_parsers.extract_tool_calls
extract_tool_calls # langchain_anthropic.output_parsers. extract_tool_calls ( content : str | list [ str | dict ] ) → list [ toolcall ] [source] # extract tool call list content blocks. parameter : content ( str | list [ str | dict ] ) – return type : list [ toolcall ]
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/index.html
langchain-google-vertexai: 1.0.10 # callback # class callbacks.vertexaicallbackhandler () callback handler track vertexai info. chain # function chains.create_structured_runnable (function, ...) create runnable sequence us openai functions. chains.get_output_parser (functions) get appropriate function output parser given user functions. chat_models # class chat_models.chatvertexai google cloud vertex ai chat model integration. embeddings # class embeddings.googleembeddingmodeltype (value[, ...]) embeddings.googleembeddingmodelversion (value) embeddings.vertexaiembeddings google cloud vertexai embedding models. evaluator # class evaluators.evaluation.vertexpairwisestringevaluator (...) evaluate perplexity predicted string. evaluators.evaluation.vertexstringevaluator (...) evaluate perplexity predicted string. functions_utils # class functions_utils.pydanticfunctionsoutputparser parse output pydantic object. gemma # class gemma.gemmachatlocalhf field : gemma.gemmachatlocalkaggle needed mypy typing recognize model_name valid arg. gemma.gemmachatvertexaimodelgarden needed mypy typing recognize model_name valid arg. gemma.gemmalocalhf local gemma model loaded huggingface. gemma.gemmalocalkaggle local gemma chat model loaded kaggle. gemma.gemmavertexaimodelgarden create new model parsing validating input data keyword arguments. function gemma.gemma_messages_to_prompt (history) convert list message chat prompt gemma. llm # class llms.vertexai google vertex ai large language models. model_garden # class model_garden.chatanthropicvertex create new model parsing validating input data keyword arguments. model_garden.vertexaimodelgarden large language model served vertex ai model garden. model_garden_maas # class model_garden_maas.llama.vertexmodelgardenllama integration llama 3.1 google cloud vertex ai model-as-a-service. model_garden_maas.mistral.vertexmodelgardenmistral create new model parsing validating input data keyword arguments. utils # function utils.create_context_cache (model, messages) creates cache content model. vectorstores # class vectorstores.document_storage.datastoredocumentstorage (...) store document google cloud datastore. vectorstores.document_storage.documentstorage () abstract interface key, text storage retrieving documents. vectorstores.document_storage.gcsdocumentstorage (bucket) store document google cloud storage. vectorstores.vectorstores.vectorsearchvectorstore (...) vertexai vectorstore handle search indexing using vector search store document google cloud storage. vectorstores.vectorstores.vectorsearchvectorstoredatastore (...) vectorsearch datastore document storage. vectorstores.vectorstores.vectorsearchvectorstoregcs (...) alias vectorsearchvectorstore consistency rest vector store different document storage backends. vision_models # class vision_models.vertexaiimagecaptioning implementation image captioning model llm. vision_models.vertexaiimagecaptioningchat implementation image captioning model chat. vision_models.vertexaiimageeditorchat given image prompt, edits image. vision_models.vertexaiimagegeneratorchat generates image prompt. vision_models.vertexaivisualqnachat chat implementation visual qna model
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/callbacks.html#langchain-google-vertexai-callbacks
callback # class callbacks.vertexaicallbackhandler () callback handler track vertexai info.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/callbacks/langchain_google_vertexai.callbacks.vertexaicallbackhandler.html#langchain_google_vertexai.callbacks.vertexaicallbackhandler
vertexaicallbackhandler # class langchain_google_vertexai.callbacks. vertexaicallbackhandler [source] # callback handler track vertexai info. attribute always_verbose whether call verbose callback even verbose false. completion_characters completion_tokens ignore_agent whether ignore agent callbacks. ignore_chain whether ignore chain callbacks. ignore_chat_model whether ignore chat model callbacks. ignore_custom_event ignore custom event. ignore_llm whether ignore llm callbacks. ignore_retriever whether ignore retriever callbacks. ignore_retry whether ignore retry callbacks. prompt_characters prompt_tokens raise_error whether raise error exception occurs. run_inline whether run callback inline. successful_requests method __init__ () on_agent_action (action, *, run_id[, ...]) run agent action. on_agent_finish (finish, *, run_id[, ...]) run agent end. on_chain_end (outputs, *, run_id[, parent_run_id]) run chain end running. on_chain_error (error, *, run_id[, parent_run_id]) run chain errors. on_chain_start (serialized, inputs, *, run_id) run chain start running. on_chat_model_start (serialized, messages, *, ...) run chat model start running. on_custom_event (name, data, *, run_id[, ...]) override define handler custom event. on_llm_end (response, **kwargs) collect token usage. on_llm_error (error, *, run_id[, parent_run_id]) run llm errors. on_llm_new_token (token, **kwargs) run new llm token. on_llm_start (serialized, prompts, **kwargs) run llm start running. on_retriever_end (documents, *, run_id[, ...]) run retriever end running. on_retriever_error (error, *, run_id[, ...]) run retriever errors. on_retriever_start (serialized, query, *, run_id) run retriever start running. on_retry (retry_state, *, run_id[, parent_run_id]) run retry event. on_text (text, *, run_id[, parent_run_id]) run arbitrary text. on_tool_end (output, *, run_id[, parent_run_id]) run tool end running. on_tool_error (error, *, run_id[, parent_run_id]) run tool errors. on_tool_start (serialized, input_str, *, run_id) run tool start running. __init__ ( ) → none [source] # return type : none on_agent_action ( action : agentaction , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run agent action. parameter : action ( agentaction ) – agent action. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_agent_finish ( finish : agentfinish , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run agent end. parameter : finish ( agentfinish ) – agent finish. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_chain_end ( output : dict [ str , ] , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run chain end running. parameter : output ( dict [ str , ] ) – output chain. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_chain_error ( error : baseexception , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run chain errors. parameter : error ( baseexception ) – error occurred. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_chain_start ( serialized : dict [ str , ] , input : dict [ str , ] , * , run_id : uuid , parent_run_id : uuid | none = none , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → # run chain start running. parameter : serialized ( dict [ str , ] ) – serialized chain. input ( dict [ str , ] ) – inputs. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. tag ( optional [ list [ str ] ] ) – tags. metadata ( optional [ dict [ str , ] ] ) – metadata. kwargs ( ) – additional keyword arguments. return type : on_chat_model_start ( serialized : dict [ str , ] , message : list [ list [ basemessage ] ] , * , run_id : uuid , parent_run_id : uuid | none = none , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → # run chat model start running. attention : method called chat models. you’re implementing handler non-chat model, use on_llm_start instead. parameter : serialized ( dict [ str , ] ) – serialized chat model. message ( list [ list [ basemessage ] ] ) – messages. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. tag ( optional [ list [ str ] ] ) – tags. metadata ( optional [ dict [ str , ] ] ) – metadata. kwargs ( ) – additional keyword arguments. return type : on_custom_event ( name : str , data : , * , run_id : uuid , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → # override define handler custom event. parameter : name ( str ) – name custom event. data ( ) – data custom event. format match
format specified user. run_id ( uuid ) – id run. tag ( list [ str ] | none ) – tag associated custom event
(includes inherited tags). metadata ( dict [ str , ] | none ) – metadata associated custom event
(includes inherited metadata). kwargs ( ) – return type : new version 0.2.15. on_llm_end ( response : llmresult , ** kwargs : ) → none [source] # collect token usage. parameter : response ( llmresult ) – kwargs ( ) – return type : none on_llm_error ( error : baseexception , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run llm errors. parameter : error ( baseexception ) – error occurred. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_llm_new_token ( token : str , ** kwargs : ) → none [source] # run new llm token. available streaming enabled. parameter : token ( str ) – kwargs ( ) – return type : none on_llm_start ( serialized : dict [ str , ] , prompt : list [ str ] , ** kwargs : ) → none [source] # run llm start running. parameter : serialized ( dict [ str , ] ) – prompt ( list [ str ] ) – kwargs ( ) – return type : none on_retriever_end ( document : sequence [ document ] , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run retriever end running. parameter : document ( sequence [ document ] ) – document retrieved. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_retriever_error ( error : baseexception , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run retriever errors. parameter : error ( baseexception ) – error occurred. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_retriever_start ( serialized : dict [ str , ] , query : str , * , run_id : uuid , parent_run_id : uuid | none = none , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → # run retriever start running. parameter : serialized ( dict [ str , ] ) – serialized retriever. query ( str ) – query. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. tag ( optional [ list [ str ] ] ) – tags. metadata ( optional [ dict [ str , ] ] ) – metadata. kwargs ( ) – additional keyword arguments. return type : on_retry ( retry_state : retrycallstate , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run retry event. parameter : retry_state ( retrycallstate ) – retry state. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_text ( text : str , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run arbitrary text. parameter : text ( str ) – text. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_tool_end ( output : , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run tool end running. parameter : output ( ) – output tool. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_tool_error ( error : baseexception , * , run_id : uuid , parent_run_id : uuid | none = none , ** kwargs : ) → # run tool errors. parameter : error ( baseexception ) – error occurred. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. kwargs ( ) – additional keyword arguments. return type : on_tool_start ( serialized : dict [ str , ] , input_str : str , * , run_id : uuid , parent_run_id : uuid | none = none , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , input : dict [ str , ] | none = none , ** kwargs : ) → # run tool start running. parameter : serialized ( dict [ str , ] ) – serialized tool. input_str ( str ) – input string. run_id ( uuid ) – run id. id current run. parent_run_id ( uuid ) – parent run id. id parent run. tag ( optional [ list [ str ] ] ) – tags. metadata ( optional [ dict [ str , ] ] ) – metadata. input ( optional [ dict [ str , ] ] ) – inputs. kwargs ( ) – additional keyword arguments. return type :
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/chains.html#langchain-google-vertexai-chains
chain # function chains.create_structured_runnable (function, ...) create runnable sequence us openai functions. chains.get_output_parser (functions) get appropriate function output parser given user functions.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/chains/langchain_google_vertexai.chains.create_structured_runnable.html#langchain_google_vertexai.chains.create_structured_runnable
create_structured_runnable # langchain_google_vertexai.chains. create_structured_runnable ( function : type [ basemodel ] | sequence [ type [ basemodel ] ] , llm : runnable , * , prompt : baseprompttemplate | none = none , use_extra_step : bool = false ) → runnable [source] # create runnable sequence us openai functions. parameter : function ( type [ basemodel ] | sequence [ type [ basemodel ] ] ) – either single pydantic.basemodel class sequence
pydantic.basemodels classes.
best results, pydantic.basemodels
description parameters. llm ( runnable ) – language model use,
assumed support google vertex function-calling api. prompt ( baseprompttemplate | none ) – baseprompttemplate pas model. use_extra_step ( bool ) – whether make extra step parse output function return : runnable sequence pas given function model run. return type : runnable example typing import optional langchain_google_vertexai import chatvertexai , create_structured_runnable langchain_core.prompts import chatprompttemplate langchain_core.pydantic_v1 import basemodel , field class recordperson ( basemodel ): """record identifying information person.""" name : str = field ( ... , description = "the person's name" ) age : int = field ( ... , description = "the person's age" ) fav_food : optional [ str ] = field ( none , description = "the person's favorite food" ) class recorddog ( basemodel ): """record identifying information dog.""" name : str = field ( ... , description = "the dog's name" ) color : str = field ( ... , description = "the dog's color" ) fav_food : optional [ str ] = field ( none , description = "the dog's favorite food" ) llm = chatvertexai ( model_name = "gemini-pro" ) prompt = chatprompttemplate . from_template ( """ world class algorithm recording entities. make call relevant function record entity following input: {input} tip: make sure answer correct format""" ) chain = create_structured_runnable ([ recordperson , recorddog ], llm , prompt = prompt ) chain . invoke ({ "input" : "harry chubby brown beagle loved chicken" }) # -> recorddog(name="harry", color="brown", fav_food="chicken")
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/chains/langchain_google_vertexai.chains.get_output_parser.html#langchain_google_vertexai.chains.get_output_parser
get_output_parser # langchain_google_vertexai.chains. get_output_parser ( function : sequence [ type [ basemodel ] ] ) → baseoutputparser | basegenerationoutputparser [source] # get appropriate function output parser given user functions. parameter : function ( sequence [ type [ basemodel ] ] ) – sequence element dictionary, pydantic.basemodel class,
python function. dictionary passed in, assumed
already valid openai function. return : pydanticfunctionsoutputparser return type : baseoutputparser | basegenerationoutputparser
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/chat_models.html#langchain-google-vertexai-chat-models
chat_models # wrapper around google vertexai chat-based models. class chat_models.chatvertexai google cloud vertex ai chat model integration.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/chat_models/langchain_google_vertexai.chat_models.chatvertexai.html#langchain_google_vertexai.chat_models.chatvertexai
chatvertexai # class langchain_google_vertexai.chat_models. chatvertexai [source] # bases: _vertexaicommon , basechatmodel google cloud vertex ai chat model integration. setup: must langchain-google-vertexai python package installed
.. code-block:: bash pip install -u langchain-google-vertexai either: credential configured environment (gcloud, workload identity, etc…) store path service account json file google_application_credentials environment variable codebase us google.auth library first look application
credential variable mentioned above, look system-level auth. information, see: https://cloud.google.com/docs/authentication/application-default-credentials#gac https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth . key init args — completion params: model: str name chatvertexai model use. e.g. “gemini-1.5-flash-001”,
“gemini-1.5-pro-001”, etc. temperature: optional[float] sampling temperature. max_tokens: optional[int] max number token generate. stop: optional[list[str]] default stop sequences. safety_settings: optional[dict[vertexai.generative_models.harmcategory, vertexai.generative_models.harmblockthreshold]] default safety setting use generations. key init args — client params: max_retries: int max number retries. credentials: optional[google.auth.credentials.credentials] default custom credential use making api calls.
provided, credential ascertained environment. project: optional[str] default gcp project use making vertex api calls. location: str = “us-central1” default location use making api calls. request_parallelism: int = 5 amount parallelism allowed request issued vertexai models.
default 5. base_url: optional[str] base url api requests. see full list supported init args description params section. instantiate: langchain_google_vertexai import chatvertexai llm = chatvertexai ( model = "gemini-1.5-flash-001" , temperature = 0 , max_tokens = none , max_retries = 6 , stop = none , # params... ) invoke: message = [ ( "system" , "you helpful translator. translate user sentence french." ), ( "human" , "i love programming." ), ] llm . invoke ( message ) aimessage ( content = "j'adore programmer. “, response_metadata={‘is_blocked’: false, ‘safety_ratings’: [{‘category’: ‘harm_category_hate_speech’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_dangerous_content’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_harassment’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_sexually_explicit’, ‘probability_label’: ‘negligible’, ‘blocked’: false}], ‘citation_metadata’: none, ‘usage_metadata’: {‘prompt_token_count’: 17, ‘candidates_token_count’: 7, ‘total_token_count’: 24}}, id=’run-925ce305-2268-44c4-875f-dde9128520ad-0’) stream: chunk llm . stream ( message ): print ( chunk ) aimessagechunk ( content = 'j' , response_metadata = { 'is_blocked' : false , 'safety_ratings' : [], 'citation_metadata' : none }, id = 'run-9df01d73-84d9-42db-9d6b-b1466a019e89' ) aimessagechunk ( content = "'adore programmer. “, response_metadata={‘is_blocked’: false, ‘safety_ratings’: [{‘category’: ‘harm_category_hate_speech’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_dangerous_content’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_harassment’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_sexually_explicit’, ‘probability_label’: ‘negligible’, ‘blocked’: false}], ‘citation_metadata’: none}, id=’run-9df01d73-84d9-42db-9d6b-b1466a019e89’) aimessagechunk(content=’’, response_metadata={‘is_blocked’: false, ‘safety_ratings’: [], ‘citation_metadata’: none, ‘usage_metadata’: {‘prompt_token_count’: 17, ‘candidates_token_count’: 7, ‘total_token_count’: 24}}, id=’run-9df01d73-84d9-42db-9d6b-b1466a019e89’) stream = llm . stream ( message ) full = next ( stream ) chunk stream : full += chunk full aimessagechunk ( content = "j'adore programmer. “, response_metadata={‘is_blocked’: false, ‘safety_ratings’: [{‘category’: ‘harm_category_hate_speech’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_dangerous_content’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_harassment’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_sexually_explicit’, ‘probability_label’: ‘negligible’, ‘blocked’: false}], ‘citation_metadata’: none, ‘usage_metadata’: {‘prompt_token_count’: 17, ‘candidates_token_count’: 7, ‘total_token_count’: 24}}, id=’run-b7f7492c-4cb5-42d0-8fc3-dce9b293b0fb’) async: await llm . ainvoke ( message ) # stream: # async chunk (await llm.astream(messages)) # batch: # await llm.abatch([messages]) aimessage ( content = "j'adore programmer. “, response_metadata={‘is_blocked’: false, ‘safety_ratings’: [{‘category’: ‘harm_category_hate_speech’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_dangerous_content’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_harassment’, ‘probability_label’: ‘negligible’, ‘blocked’: false}, {‘category’: ‘harm_category_sexually_explicit’, ‘probability_label’: ‘negligible’, ‘blocked’: false}], ‘citation_metadata’: none, ‘usage_metadata’: {‘prompt_token_count’: 17, ‘candidates_token_count’: 7, ‘total_token_count’: 24}}, id=’run-925ce305-2268-44c4-875f-dde9128520ad-0’) tool calling: langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getpopulation ( basemodel ): '''get current population given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) llm_with_tools = llm . bind_tools ([ getweather , getpopulation ]) ai_msg = llm_with_tools . invoke ( "which city hotter today bigger: la ny?" ) ai_msg . tool_calls [{ 'name' : 'getweather' , 'args' : { 'location' : 'los angeles, ca' }, 'id' : '2a2401fa-40db-470d-83ce-4e52de910d9e' }, { 'name' : 'getweather' , 'args' : { 'location' : 'new york city, ny' }, 'id' : '96761deb-ab7f-4ef9-b4b4-6d44562fc46e' }, { 'name' : 'getpopulation' , 'args' : { 'location' : 'los angeles, ca' }, 'id' : '9147d532-abee-43a2-adb5-12f164300484' }, { 'name' : 'getpopulation' , 'args' : { 'location' : 'new york city, ny' }, 'id' : 'c43374ea-bde5-49ca-8487-5b83ebeea1e6' }] see chatvertexai.bind_tools() method more. structured output: typing import optional langchain_core.pydantic_v1 import basemodel , field class joke ( basemodel ): '''joke tell user.''' setup : str = field ( description = "the setup joke" ) punchline : str = field ( description = "the punchline joke" ) rating : optional [ int ] = field ( description = "how funny joke is, 1 10" ) structured_llm = llm . with_structured_output ( joke ) structured_llm . invoke ( "tell joke cats" ) joke ( setup = 'what call cat love bowl?' , punchline = 'an alley cat!' , rating = none ) see chatvertexai.with_structured_output() more. image input: import base64 import httpx langchain_core.messages import humanmessage image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-gfp-wisconsin-madison-the-nature-boardwalk.jpg" image_data = base64 . b64encode ( httpx . get ( image_url ) . content ) . decode ( "utf-8" ) message = humanmessage ( content = [ { "type" : "text" , "text" : "describe weather image" }, { "type" : "image_url" , "image_url" : { "url" : f "data:image/jpeg;base64, { image_data } " }, }, ], ) ai_msg = llm . invoke ([ message ]) ai_msg . content 'the weather image appears sunny pleasant. sky bright blue scattered white clouds, suggesting clear mild day. lush green grass indicates recent rainfall sufficient moisture. absence strong shadow suggests sun high sky, possibly late afternoon. overall, image conveys sense tranquility warmth, characteristic beautiful summer day. ‘ also point gc file faster / efficient byte transferred back forth. llm . invoke ( [ humanmessage ( [ "what's image?" , { "type" : "media" , "file_uri" : "gs://cloud-samples-data/generative-ai/image/scones.jpg" , "mime_type" : "image/jpeg" , }, ] ) ] ) . content 'the image five blueberry scone arranged piece baking paper. list picture:
* five blueberry scones: scattered across parchment paper, dusted powdered sugar.
* two cup coffee: two white cup saucers. one appears full, partially drunk.
* bowl blueberries: brown bowl filled fresh blueberries, placed near scones.
* spoon: silver spoon word “let’s jam” rest paper.
* pink peonies: several pink peony lie beside scones, adding touch color.
* baking paper: scones, cups, bowl, spoon arranged piece white baking paper, splattered purple. paper crinkled sits dark surface. image rustic delicious feel, suggesting cozy enjoyable breakfast brunch setting.
‘ video input: note : currently supported gemini-...-vision models. llm = chatvertexai ( model = "gemini-1.0-pro-vision" ) llm . invoke ( [ humanmessage ( [ "what's video?" , { "type" : "media" , "file_uri" : "gs://cloud-samples-data/video/animals.mp4" , "mime_type" : "video/mp4" , }, ] ) ] ) . content 'the video new feature google photo called "zoomable selfies". feature allows user take selfies animal zoo. video show several example people taking selfies animals, including tiger, elephant, sea otter. video also show feature works. user simply need open google photo app select "zoomable selfies" option. then, need choose animal list available animals. app guide user process taking selfie.' audio input: langchain_core.messages import humanmessage llm = chatvertexai ( model = "gemini-1.5-flash-001" ) llm . invoke ( [ humanmessage ( [ "what's audio about?" , { "type" : "media" , "file_uri" : "gs://cloud-samples-data/generative-ai/audio/pixel.mp3" , "mime_type" : "audio/mpeg" , }, ] ) ] ) . content "this audio interview two product manager google work pixel feature drops. discus feature drop important showcasing google device constantly improving getting better. also discus highlight january feature drop new feature coming march drop pixel phone pixel watches. interview concludes discussion user feedback extremely important deciding feature include feature drops. " token usage: ai_msg = llm . invoke ( message ) ai_msg . usage_metadata { 'input_tokens' : 17 , 'output_tokens' : 7 , 'total_tokens' : 24 } response metadata ai_msg = llm . invoke ( message ) ai_msg . response_metadata { 'is_blocked' : false , 'safety_ratings' : [{ 'category' : 'harm_category_hate_speech' , 'probability_label' : 'negligible' , 'blocked' : false }, { 'category' : 'harm_category_dangerous_content' , 'probability_label' : 'negligible' , 'blocked' : false }, { 'category' : 'harm_category_harassment' , 'probability_label' : 'negligible' , 'blocked' : false }, { 'category' : 'harm_category_sexually_explicit' , 'probability_label' : 'negligible' , 'blocked' : false }], 'usage_metadata' : { 'prompt_token_count' : 17 , 'candidates_token_count' : 7 , 'total_token_count' : 24 }} safety setting langchain_google_vertexai import harmblockthreshold , harmcategory llm = chatvertexai ( model = "gemini-1.5-pro" , safety_settings = { harmcategory . harm_category_hate_speech : harmblockthreshold . block_low_and_above , harmcategory . harm_category_dangerous_content : harmblockthreshold . block_medium_and_above , harmcategory . harm_category_harassment : harmblockthreshold . block_low_and_above , harmcategory . harm_category_sexually_explicit : harmblockthreshold . block_only_high , }, ) llm . invoke ( message ) . response_metadata { 'is_blocked' : false , 'safety_ratings' : [{ 'category' : 'harm_category_hate_speech' , 'probability_label' : 'negligible' , 'blocked' : false }, { 'category' : 'harm_category_dangerous_content' , 'probability_label' : 'negligible' , 'blocked' : false }, { 'category' : 'harm_category_harassment' , 'probability_label' : 'negligible' , 'blocked' : false }, { 'category' : 'harm_category_sexually_explicit' , 'probability_label' : 'negligible' , 'blocked' : false }], 'usage_metadata' : { 'prompt_token_count' : 17 , 'candidates_token_count' : 7 , 'total_token_count' : 24 }} needed mypy typing recognize model_name valid arg. note chatvertexai implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
us default parameter vertexai.init defined. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param cached_content : str | none = none # optional. use model cache mode. supported gemini 1.5 later
models. must string containing cache name (a sequence numbers) param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param client_cert_source : callable [ [ ] , tuple [ byte , byte ] ] | none = none # callback return client certificate byte private key byte param convert_system_message_to_human : bool = false # [deprecated] since new gemini model support setting system message,
setting parameter true discouraged. param credential : = none # default custom credential (google.auth.credentials.credentials) use param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param example : list [ basemessage ] | none = none # param full_model_name : str | none = none # full name model’s endpoint. param location : str = 'us-central1' # default location use making api calls. param max_output_tokens : int | none = none (alias 'max_tokens') # token limit determines maximum amount text output one prompt. param max_retries : int = 6 # maximum number retries make generating. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'chat-bison-default' (alias 'model') # underlying model name. param n : int = 1 # many completion generate prompt. param project : str | none = none # default gcp project use making vertex api calls. param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param request_parallelism : int = 5 # amount parallelism allowed request issued vertexai models. param response_mime_type : str | none = none # optional. output response mimetype generated candidate text. supported gemini 1.5 later models. supported mimetype: “text/plain”: (default) text output. “application/json”: json response candidates. model also need prompted output appropriate response
type, otherwise behavior undefined. preview feature. param response_schema : dict [ str , ] | none = none # optional. enforce schema output. work response_mime_type set application/json .
format dictionary follow open api schema. param safety_settings : 'safetysettingstype' | none = none # default safety setting use generations. example: langchain_google_vertexai import harmblockthreshold, harmcategory safety_settings = { harmcategory.harm_category_unspecified: harmblockthreshold.block_none,
harmcategory.harm_category_dangerous_content: harmblockthreshold.block_medium_and_above,
harmcategory.harm_category_hate_speech: harmblockthreshold.block_only_high,
harmcategory.harm_category_harassment: harmblockthreshold.block_low_and_above,
harmcategory.harm_category_sexually_explicit: harmblockthreshold.block_none, } param stop : list [ str ] | none = none (alias 'stop_sequences') # optional list stop word use generating. param streaming : bool = false # whether stream result not. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # sampling temperature, control degree randomness token selection. param top_k : int | none = none # model selects token output, next token selected param top_p : float | none = none # token selected probable least sum param tuned_model_name : str | none = none # name tuned model. tuned_model_name passed
model_name used determine model family param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ tool | tool | _tooldictlike | basetool | type [ basemodel ] | functiondescription | callable | functiondeclaration | dict [ str , ] ] , tool_config : _toolconfigdict | none = none , * , tool_choice : dict | list [ str ] | str | literal [ 'auto' , 'none' , 'any' ] | literal [ true ] | bool | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model. assumes model compatible vertex tool-calling api. parameter : tool ( sequence [ tool | tool | _tooldictlike | basetool | type [ basemodel ] | functiondescription | callable | functiondeclaration | dict [ str , ] ] ) – list tool definition bind chat model.
pydantic model, callable, basetool. pydantic
models, callables, basetools automatically converted
schema dictionary representation. **kwargs ( ) – additional parameter pas runnable constructor. tool_config ( _toolconfigdict | none ) – tool_choice ( dict | list [ str ] | str | literal [ 'auto' , 'none' , 'any' ] | ~typing.literal [ true ] | bool | none ) – **kwargs – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) – stop ( list [ str ] | none ) – kwargs ( ) – return type : str configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) → runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) – configurablefield instance used select
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int [source] # get number token present text. parameter : text ( str ) – return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # get number token messages. useful checking input fit model’s context window. parameter : message ( list [ basemessage ] ) – message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) → list [ int ] # return ordered id token text. parameter : text ( str ) – string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] invoke ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → basemessage # transform single input output. override implement. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use invoking runnable.
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , * , include_raw : bool = false , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] [source] # model wrapper return output formatted match given schema. changed version 1.1.0: return type corrected version 1.1.0. previously dict schema
provided output form [{"args": {}, "name": "schema_name"}] output list
single dict “args” one dict corresponded schema.
1.1.0 fixed schema (the value
corresponding old “args” key) returned directly. parameter : schema ( dict | type [ basemodel ] ) – output schema dict pydantic class. pydantic class
model output object class. dict
model output dict. pydantic class returned
attribute validated, whereas dict be. method “function_calling” schema dict, dict
must match openai function-calling spec. include_raw ( bool ) – false parsed structured output returned.
key “raw”, “parsed”, “parsing_error”. kwargs ( ) – return : runnable take chatmodel input. include_raw true
dict key — raw: basemessage, parsed: optional[_dictorpydantic],
parsing_error: optional[baseexception]. include_raw false
_dictorpydantic returned, _dictorpydantic depends schema.
schema pydantic class _dictorpydantic pydantic class.
schema dict _dictorpydantic dict. return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example: pydantic schema, exclude raw: langchain_core.pydantic_v1 import basemodel langchain_google_vertexai import chatvertexai class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatvertexai ( model_name = "gemini-pro" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same.', justification='a pound pound.' # ) example: pydantic schema, include raw: langchain_core.pydantic_v1 import basemodel langchain_google_vertexai import chatvertexai class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatvertexai ( model_name = "gemini-pro" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema, exclude raw: langchain_core.pydantic_v1 import basemodel langchain_core.utils.function_calling import convert_to_openai_function langchain_google_vertexai import chatvertexai class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str dict_schema = convert_to_openai_function ( answerwithjustification ) llm = chatvertexai ( model_name = "gemini-pro" , temperature = 0 ) structured_llm = llm . with_structured_output ( dict_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } property async_prediction_client : predictionserviceasyncclient # return predictionserviceclient. property prediction_client : predictionserviceclient # return predictionserviceclient. task_executor : classvar [ executor | none ] = fieldinfo(exclude=true, extra={}) #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/embeddings.html#langchain-google-vertexai-embeddings
embeddings # class embeddings.googleembeddingmodeltype (value[, ...]) embeddings.googleembeddingmodelversion (value) embeddings.vertexaiembeddings google cloud vertexai embedding models.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/embeddings/langchain_google_vertexai.embeddings.googleembeddingmodeltype.html#langchain_google_vertexai.embeddings.googleembeddingmodeltype
googleembeddingmodeltype # class langchain_google_vertexai.embeddings. googleembeddingmodeltype ( value , name = none , * , module = none , qualname = none , type = none , start = 1 , boundary = none ) [source] # text = '1' # multimodal = '2' #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/embeddings/langchain_google_vertexai.embeddings.googleembeddingmodelversion.html#langchain_google_vertexai.embeddings.googleembeddingmodelversion
googleembeddingmodelversion # class langchain_google_vertexai.embeddings. googleembeddingmodelversion ( value , name = none , * , module = none , qualname = none , type = none , start = 1 , boundary = none ) [source] # embeddings_june_2023 = '1' # embeddings_nov_2023 = '2' # embeddings_dec_2023 = '3' # embeddings_may_2024 = '4' # task_type_supported # check model generation support task type. output_dimensionality_supported # check model generation support output dimensionality.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/embeddings/langchain_google_vertexai.embeddings.vertexaiembeddings.html#langchain_google_vertexai.embeddings.vertexaiembeddings
vertexaiembeddings # class langchain_google_vertexai.embeddings. vertexaiembeddings [source] # bases: _vertexaicommon , embeddings google cloud vertexai embedding models. initialize sentence_transformer. param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
us default parameter vertexai.init defined. param client_cert_source : callable [ [ ] , tuple [ byte , byte ] ] | none = none # callback return client certificate byte private key byte param credential : = none # default custom credential (google.auth.credentials.credentials) use param full_model_name : str | none = none # full name model’s endpoint. param location : str = 'us-central1' # default location use making api calls. param max_output_tokens : int | none = none (alias 'max_tokens') # token limit determines maximum amount text output one prompt. param max_retries : int = 6 # maximum number retries make generating. param model_name : str = none (alias 'model') # underlying model name. param n : int = 1 # many completion generate prompt. param project : str | none = none # default gcp project use making vertex api calls. param request_parallelism : int = 5 # amount parallelism allowed request issued vertexai models. param safety_settings : 'safetysettingstype' | none = none # default safety setting use generations. example: langchain_google_vertexai import harmblockthreshold, harmcategory safety_settings = { harmcategory.harm_category_unspecified: harmblockthreshold.block_none,
harmcategory.harm_category_sexually_explicit: harmblockthreshold.block_none, } param stop : list [ str ] | none = none (alias 'stop_sequences') # optional list stop word use generating. param streaming : bool = false # whether stream result not. param temperature : float | none = none # sampling temperature, control degree randomness token selection. param top_k : int | none = none # model selects token output, next token selected param top_p : float | none = none # token selected probable least sum param tuned_model_name : str | none = none # name tuned model. tuned_model_name passed
model_name used determine model family async aembed_documents ( text : list [ str ] ) → list [ list [ float ] ] # asynchronous embed search docs. parameter : text ( list [ str ] ) – list text embed. return : list embeddings. return type : list [ list [float]] async aembed_query ( text : str ) → list [ float ] # asynchronous embed query text. parameter : text ( str ) – text embed. return : embedding. return type : list [float] embed ( text : list [ str ] , batch_size : int = 0 , embeddings_task_type : literal [ 'retrieval_query' , 'retrieval_document' , 'semantic_similarity' , 'classification' , 'clustering' , 'question_answering' , 'fact_verification' ] | none = none , dimension : int | none = none ) → list [ list [ float ] ] [source] # embed list strings. parameter : text ( list [ str ] ) – list[str] list string embed. batch_size ( int ) – [int] batch size embeddings send model.
zero, largest batch size detected dynamically
first request, starting 250, 5. embeddings_task_type ( literal [ 'retrieval_query' , 'retrieval_document' , 'semantic_similarity' , 'classification' , 'clustering' , 'question_answering' , 'fact_verification' ] | none ) – [str] optional embeddings task type,
one following retrieval_query - text query search/retrieval setting. retrieval_document - text document search/retrieval setting. semantic_similarity - embeddings used semantic textual similarity (sts). classification - embeddings used classification.
clustering - embeddings used clustering.
following supported preview models:
question_answering
fact_verification dimension ( int | none ) – [int] optional. output embeddings dimensions.
supported preview models. return : list embeddings, one text. return type : list [ list [float]] embed_documents ( text : list [ str ] , batch_size : int = 0 ) → list [ list [ float ] ] [source] # embed list documents. parameter : text ( list [ str ] ) – list[str] list text embed. batch_size ( int ) – [int] batch size embeddings send model.
first request, starting 250, 5. return : list embeddings, one text. return type : list [ list [float]] embed_image ( image_path : str , contextual_text : str | none = none , dimension : int | none = none ) → list [ float ] [source] # embed image. parameter : image_path ( str ) – path image (local, google cloud storage web) generate for. ( embeddings ) – contextual_text ( str | none ) – text generate embeddings for. dimension ( int | none ) – return : embedding image. return type : list [float] embed_query ( text : str ) → list [ float ] [source] # embed text. parameter : text ( str ) – text embed. return : embedding text. return type : list [float] get_num_tokens ( text : str ) → int # get number token present text. useful checking input fit model’s context window. parameter : text ( str ) – string input tokenize. return : integer number token text. return type : int property async_prediction_client : predictionserviceasyncclient # return predictionserviceclient. property model_type : str # property model_version : googleembeddingmodelversion # property prediction_client : predictionserviceclient # return predictionserviceclient. task_executor : classvar [ executor | none ] = fieldinfo(exclude=true, extra={}) #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/evaluators.html#langchain-google-vertexai-evaluators
evaluator # class evaluators.evaluation.vertexpairwisestringevaluator (...) evaluate perplexity predicted string. evaluators.evaluation.vertexstringevaluator (...) evaluate perplexity predicted string.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/evaluators/langchain_google_vertexai.evaluators.evaluation.vertexpairwisestringevaluator.html#langchain_google_vertexai.evaluators.evaluation.vertexpairwisestringevaluator
vertexpairwisestringevaluator # class langchain_google_vertexai.evaluators.evaluation. vertexpairwisestringevaluator ( metric : str , ** kwargs ) [source] # evaluate perplexity predicted string. attribute requires_input whether evaluator requires input string. requires_reference whether evaluator requires reference label. method __init__ (metric, **kwargs) aevaluate_string_pairs (*, prediction, ...[, ...]) asynchronously evaluate output string pairs. evaluate_string_pairs (*, prediction, ...[, ...]) evaluate output string pairs. parameter : metric ( str ) – __init__ ( metric : str , ** kwargs ) [source] # parameter : metric ( str ) – async aevaluate_string_pairs ( * , prediction : str , prediction_b : str , reference : str | none = none , input : str | none = none , ** kwargs : ) → dict # asynchronously evaluate output string pairs. parameter : prediction ( str ) – output string first model. prediction_b ( str ) – output string second model. reference ( optional [ str ] , optional ) – expected output / reference string. input ( optional [ str ] , optional ) – input string. **kwargs – additional keyword arguments, callback optional reference strings. return : dictionary containing preference, scores, and/or information. return type : dict evaluate_string_pairs ( * , prediction : str , prediction_b : str , reference : str | none = none , input : str | none = none , ** kwargs : ) → dict # evaluate output string pairs. parameter : prediction ( str ) – output string first model. prediction_b ( str ) – output string second model. reference ( optional [ str ] , optional ) – expected output / reference string. input ( optional [ str ] , optional ) – input string. **kwargs – additional keyword arguments, callback optional reference strings. return : dictionary containing preference, scores, and/or information. return type : dict
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/evaluators/langchain_google_vertexai.evaluators.evaluation.vertexstringevaluator.html#langchain_google_vertexai.evaluators.evaluation.vertexstringevaluator
vertexstringevaluator # class langchain_google_vertexai.evaluators.evaluation. vertexstringevaluator ( metric : str , ** kwargs ) [source] # evaluate perplexity predicted string. attribute evaluation_name name evaluation. requires_input whether evaluator requires input string. requires_reference whether evaluator requires reference label. method __init__ (metric, **kwargs) aevaluate_strings (*, prediction[, ...]) asynchronously evaluate chain llm output, based optional input label. evaluate (examples, predictions, *[, ...]) evaluate_strings (*, prediction[, reference, ...]) evaluate chain llm output, based optional input label. parameter : metric ( str ) – __init__ ( metric : str , ** kwargs ) [source] # parameter : metric ( str ) – async aevaluate_strings ( * , prediction : str , reference : str | none = none , input : str | none = none , ** kwargs : ) → dict # asynchronously evaluate chain llm output, based optional input label. parameter : prediction ( str ) – llm chain prediction evaluate. reference ( optional [ str ] , optional ) – reference label evaluate against. input ( optional [ str ] , optional ) – input consider evaluation. **kwargs – additional keyword arguments, including callbacks, tags, etc. return : evaluation result containing score value. return type : dict evaluate ( example : sequence [ dict [ str , str ] ] , prediction : sequence [ dict [ str , str ] ] , * , question_key : str = 'context' , answer_key : str = 'reference' , prediction_key : str = 'prediction' , instruction_key : str = 'instruction' , ** kwargs : ) → list [ dict ] [source] # parameter : example ( sequence [ dict [ str , str ] ] ) – prediction ( sequence [ dict [ str , str ] ] ) – question_key ( str ) – answer_key ( str ) – prediction_key ( str ) – instruction_key ( str ) – kwargs ( ) – return type : list [dict] evaluate_strings ( * , prediction : str , reference : str | none = none , input : str | none = none , ** kwargs : ) → dict # evaluate chain llm output, based optional input label. parameter : prediction ( str ) – llm chain prediction evaluate. reference ( optional [ str ] , optional ) – reference label evaluate against. input ( optional [ str ] , optional ) – input consider evaluation. **kwargs – additional keyword arguments, including callbacks, tags, etc. return : evaluation result containing score value. return type : dict
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/functions_utils.html#langchain-google-vertexai-functions-utils
functions_utils # class functions_utils.pydanticfunctionsoutputparser parse output pydantic object.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/functions_utils/langchain_google_vertexai.functions_utils.pydanticfunctionsoutputparser.html#langchain_google_vertexai.functions_utils.pydanticfunctionsoutputparser
pydanticfunctionsoutputparser # class langchain_google_vertexai.functions_utils. pydanticfunctionsoutputparser [source] # bases: baseoutputparser parse output pydantic object. parser used parse output chatmodel us
google vertex function format invoke functions. parser extract function call invocation match
pydantic schema provided. exception raised function call match
provided schema. example … code-block:: python message = aimessage( content=”this test message”,
additional_kwargs={ “function_call”: { “name”: “cookie”,
“arguments”: json.dumps({“name”: “value”, “age”: 10}), } }, )
chat_generation = chatgeneration(message=message) class cookie(basemodel): name: str
age: int class dog(basemodel): species: str # full output
parser = pydanticoutputfunctionsparser( pydantic_schema={“cookie”: cookie, “dog”: dog} )
result = parser.parse_result([chat_generation]) note pydanticfunctionsoutputparser implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param pydantic_schema : type [ basemodel ] | dict [ str , type [ basemodel ] ] [required] # async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
runnable implement native async version invoke. subclass override method run asynchronously. parameter : input ( str | basemessage ) – config ( runnableconfig | none ) – kwargs ( | none ) – return type : async aparse ( text : str ) → # async parse single string model output structure. parameter : text ( str ) – string output language model. return : structured output. return type : async aparse_result ( result : list [ generation ] , * , partial : bool = false ) → # async parse list candidate model generation specific format. return value parsed first generation result, assumed highest-likelihood generation. parameter : result ( list [ generation ] ) – list generation parsed. generation assumed
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) → runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) – dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) get_format_instructions ( ) → str # instruction llm output formatted. return type : str invoke ( input : str | basemessage , config : runnableconfig | none = none ) → # transform single input output. override implement. parameter : input ( str | basemessage ) – input runnable. config ( runnableconfig | none ) – config use invoking runnable.
details. return : output runnable. return type : parse ( text : str ) → basemodel [source] # parse single string model output structure. parameter : text ( str ) – string output language model. return : structured output. return type : basemodel parse_result ( result : list [ generation ] , * , partial : bool = false ) → basemodel [source] # parse list candidate model generation specific format. return value parsed first generation result, assumed highest-likelihood generation. parameter : result ( list [ generation ] ) – list generation parsed. generation assumed
parser parse partial results. default false. return : structured output. return type : basemodel parse_with_prompt ( completion : str , prompt : promptvalue ) → # parse output llm call input prompt context. prompt largely provided event outputparser want
retry fix output way, need information
prompt so. parameter : completion ( str ) – string output language model. prompt ( promptvalue ) – input promptvalue. return : structured output. return type : stream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) → iterator [ output ] # default implementation stream, call invoke.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/gemma.html#langchain-google-vertexai-gemma
gemma # class gemma.gemmachatlocalhf field : gemma.gemmachatlocalkaggle needed mypy typing recognize model_name valid arg. gemma.gemmachatvertexaimodelgarden needed mypy typing recognize model_name valid arg. gemma.gemmalocalhf local gemma model loaded huggingface. gemma.gemmalocalkaggle local gemma chat model loaded kaggle. gemma.gemmavertexaimodelgarden create new model parsing validating input data keyword arguments. function gemma.gemma_messages_to_prompt (history) convert list message chat prompt gemma.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/gemma/langchain_google_vertexai.gemma.gemmachatlocalhf.html#langchain_google_vertexai.gemma.gemmachatlocalhf
gemmachatlocalhf # class langchain_google_vertexai.gemma. gemmachatlocalhf [source] # bases: _gemmalocalhfbase , basechatmodel note gemmachatlocalhf implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param cache_dir : str | none = none # param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param hf_access_token : str [required] # param max_tokens : int | none = none # maximum number token generate. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'google/gemma-2b' (alias 'model') # gemma model name. param parse_response : bool = false # whether post-process chat response clean repeations param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # temperature use sampling. param top_k : int | none = none # top-k value use sampling. param top_p : float | none = none # top-p value use sampling. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , ** kwargs : ) → runnable [ languagemodelinput , basemessage ] # parameter : tool ( sequence [ union [ dict [ str , ] , type , callable , basetool ] ] ) – kwargs ( ) – return type : runnable [languagemodelinput, basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) – stop ( list [ str ] | none ) – kwargs ( ) – return type : str configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) → runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) – configurablefield instance used select
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type , * , include_raw : bool = false , ** kwargs : ) → runnable [ languagemodelinput , dict | basemodel ] # model wrapper return output formatted match given schema. parameter : schema ( union [ dict , type ] ) – output schema. passed as: openai function/tool schema, json schema, typeddict class (support added 0.2.26), pydantic class. schema pydantic class model output
schema field specifying pydantic typeddict class. changed version 0.2.26: added support typeddict class. include_raw ( bool ) – false parsed structured output returned.
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [languagemodelinput, union[dict, basemodel ]] example: pydantic schema (include_raw=false): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_core.pydantic_v1 import basemodel langchain_core.utils.function_calling import convert_to_openai_tool class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str dict_schema = convert_to_openai_tool ( answerwithjustification ) llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( dict_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # }
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/gemma/langchain_google_vertexai.gemma.gemmachatlocalkaggle.html#langchain_google_vertexai.gemma.gemmachatlocalkaggle
gemmachatlocalkaggle # class langchain_google_vertexai.gemma. gemmachatlocalkaggle [source] # bases: _gemmalocalkagglebase , basechatmodel needed mypy typing recognize model_name valid arg. note gemmachatlocalkaggle implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param keras_backend : str = 'jax' # param max_tokens : int | none = none # maximum number token generate. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'gemma_2b_en' (alias 'model') # gemma model name. param parse_response : bool = false # whether post-process chat response clean repeations param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # temperature use sampling. param top_k : int | none = none # top-k value use sampling. param top_p : float | none = none # top-p value use sampling. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/gemma/langchain_google_vertexai.gemma.gemmachatvertexaimodelgarden.html#langchain_google_vertexai.gemma.gemmachatvertexaimodelgarden
gemmachatvertexaimodelgarden # class langchain_google_vertexai.gemma. gemmachatvertexaimodelgarden [source] # bases: _gemmabase , _basevertexaimodelgarden , basechatmodel needed mypy typing recognize model_name valid arg. note gemmachatvertexaimodelgarden implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param allowed_model_args : list [ str ] | none = ['temperature', 'top_p', 'top_k', 'max_tokens', 'max_length'] # allowed optional args passed model. param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
us default parameter vertexai.init defined. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param client_cert_source : callable [ [ ] , tuple [ byte , byte ] ] | none = none # callback return client certificate byte private key byte param credential : = none # default custom credential (google.auth.credentials.credentials) use param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param endpoint_id : str [required] # name endpoint model deployed. param full_model_name : str | none = none # full name model’s endpoint. param location : str = 'us-central1' # default location use making api calls. param max_retries : int = 6 # maximum number retries make generating. param max_tokens : int | none = none # maximum number token generate. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str | none = none (alias 'model') # underlying model name. param parse_response : bool = false # whether post-process chat response clean repeations param project : str | none = none # default gcp project use making vertex api calls. param prompt_arg : str = 'prompt' # param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param request_parallelism : int = 5 # amount parallelism allowed request issued vertexai models. param result_arg : str | none = 'generated_text' # set result_arg none output model expected string. param single_example_per_request : bool = true # llm endpoint currently serf first example request param stop : list [ str ] | none = none (alias 'stop_sequences') # optional list stop word use generating. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # temperature use sampling. param top_k : int | none = none # top-k value use sampling. param top_p : float | none = none # top-p value use sampling. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [languagemodelinput, union[dict, basemodel ]] example: pydantic schema (include_raw=false): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_core.pydantic_v1 import basemodel langchain_core.utils.function_calling import convert_to_openai_tool class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str dict_schema = convert_to_openai_tool ( answerwithjustification ) llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( dict_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } property async_prediction_client : predictionserviceasyncclient # return predictionserviceclient. property endpoint_path : str # property prediction_client : predictionserviceclient # return predictionserviceclient. task_executor : classvar [ executor | none ] = fieldinfo(exclude=true, extra={}) #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/gemma/langchain_google_vertexai.gemma.gemmalocalhf.html#langchain_google_vertexai.gemma.gemmalocalhf
gemmalocalhf # class langchain_google_vertexai.gemma. gemmalocalhf [source] # bases: _gemmalocalhfbase , basellm local gemma model loaded huggingface. note gemmalocalhf implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param cache_dir : str | none = none # param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param hf_access_token : str [required] # param max_tokens : int | none = none # maximum number token generate. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'google/gemma-2b' (alias 'model') # gemma model name. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # temperature use sampling. param top_k : int | none = none # top-k value use sampling. param top_p : float | none = none # top-p value use sampling. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) – prompt generate from. stop ( list [ str ] | none ) – stop word use generating. model output cut
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) → runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) – dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) generate ( prompt : list [ str ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , * , tag : list [ str ] | list [ list [ str ] ] | none = none , metadata : dict [ str , ] | list [ dict [ str , ] ] | none = none , run_name : str | list [ str ] | none = none , run_id : uuid | list [ uuid | none ] | none = none , ** kwargs : ) → llmresult # pas sequence prompt model return generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int # get number token present text. useful checking input fit model’s context window. parameter : text ( str ) – string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # get number token messages. useful checking input fit model’s context window. parameter : message ( list [ basemessage ] ) – message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) → list [ int ] # return ordered id token text. parameter : text ( str ) – string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] invoke ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → str # transform single input output. override implement. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use invoking runnable.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/gemma/langchain_google_vertexai.gemma.gemmalocalkaggle.html#langchain_google_vertexai.gemma.gemmalocalkaggle
gemmalocalkaggle # class langchain_google_vertexai.gemma. gemmalocalkaggle [source] # bases: _gemmalocalkagglebase , basellm local gemma chat model loaded kaggle. needed typing. note gemmalocalkaggle implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param keras_backend : str = 'jax' # param max_tokens : int | none = none # maximum number token generate. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'gemma_2b_en' (alias 'model') # gemma model name. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # temperature use sampling. param top_k : int | none = none # top-k value use sampling. param top_p : float | none = none # top-p value use sampling. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) – prompt generate from. stop ( list [ str ] | none ) – stop word use generating. model output cut
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/gemma/langchain_google_vertexai.gemma.gemmavertexaimodelgarden.html#langchain_google_vertexai.gemma.gemmavertexaimodelgarden
gemmavertexaimodelgarden # class langchain_google_vertexai.gemma. gemmavertexaimodelgarden [source] # bases: vertexaimodelgarden create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. note gemmavertexaimodelgarden implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param allowed_model_args : list [ str ] | none = ['temperature', 'top_p', 'top_k', 'max_tokens'] # allowed optional args passed model. param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
us default parameter vertexai.init defined. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param client_cert_source : callable [ [ ] , tuple [ byte , byte ] ] | none = none # callback return client certificate byte private key byte param credential : = none # default custom credential (google.auth.credentials.credentials) use param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param endpoint_id : str [required] # name endpoint model deployed. param full_model_name : str | none = none # full name model’s endpoint. param location : str = 'us-central1' # default location use making api calls. param max_retries : int = 6 # maximum number retries make generating. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str | none = none (alias 'model') # underlying model name. param project : str | none = none # default gcp project use making vertex api calls. param prompt_arg : str = 'prompt' # param request_parallelism : int = 5 # amount parallelism allowed request issued vertexai models. param result_arg : str | none = 'generated_text' # set result_arg none output model expected string. param single_example_per_request : bool = true # llm endpoint currently serf first example request param stop : list [ str ] | none = none (alias 'stop_sequences') # optional list stop word use generating. param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) – prompt generate from. stop ( list [ str ] | none ) – stop word use generating. model output cut
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : iterator [str] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] property async_prediction_client : predictionserviceasyncclient # return predictionserviceclient. property endpoint_path : str # property prediction_client : predictionserviceclient # return predictionserviceclient. task_executor : classvar [ executor | none ] = fieldinfo(exclude=true, extra={}) #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/gemma/langchain_google_vertexai.gemma.gemma_messages_to_prompt.html#langchain_google_vertexai.gemma.gemma_messages_to_prompt
gemma_messages_to_prompt # langchain_google_vertexai.gemma. gemma_messages_to_prompt ( history : list [ basemessage ] ) → str [source] # convert list message chat prompt gemma. parameter : history ( list [ basemessage ] ) – return type : str
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/llms.html#langchain-google-vertexai-llms
llm # class llms.vertexai google vertex ai large language models.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/llms/langchain_google_vertexai.llms.vertexai.html#langchain_google_vertexai.llms.vertexai
vertexai # class langchain_google_vertexai.llms. vertexai [source] # bases: _vertexaicommon , basellm google vertex ai large language models. needed mypy typing recognize model_name valid arg. note vertexai implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
us default parameter vertexai.init defined. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param client_cert_source : callable [ [ ] , tuple [ byte , byte ] ] | none = none # callback return client certificate byte private key byte param credential : = none # default custom credential (google.auth.credentials.credentials) use param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param full_model_name : str | none = none # full name model’s endpoint. param location : str = 'us-central1' # default location use making api calls. param max_output_tokens : int | none = none (alias 'max_tokens') # token limit determines maximum amount text output one prompt. param max_retries : int = 6 # maximum number retries make generating. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'text-bison' (alias 'model') # name vertex ai large language model. param n : int = 1 # many completion generate prompt. param project : str | none = none # default gcp project use making vertex api calls. param request_parallelism : int = 5 # amount parallelism allowed request issued vertexai models. param safety_settings : 'safetysettingstype' | none = none # default safety setting use generations. example: langchain_google_vertexai import harmblockthreshold, harmcategory safety_settings = { harmcategory.harm_category_unspecified: harmblockthreshold.block_none,
model_name used determine model family param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) – prompt generate from. stop ( list [ str ] | none ) – stop word use generating. model output cut
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : iterator [str] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] property async_prediction_client : predictionserviceasyncclient # return predictionserviceclient. property prediction_client : predictionserviceclient # return predictionserviceclient. task_executor : classvar [ executor | none ] = fieldinfo(exclude=true, extra={}) #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/model_garden.html#langchain-google-vertexai-model-garden
model_garden # class model_garden.chatanthropicvertex create new model parsing validating input data keyword arguments. model_garden.vertexaimodelgarden large language model served vertex ai model garden.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/model_garden/langchain_google_vertexai.model_garden.chatanthropicvertex.html#langchain_google_vertexai.model_garden.chatanthropicvertex
chatanthropicvertex # class langchain_google_vertexai.model_garden. chatanthropicvertex [source] # bases: _vertexaicommon , basechatmodel create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. note chatanthropicvertex implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param access_token : str | none = none # param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
us default parameter vertexai.init defined. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param client_cert_source : callable [ [ ] , tuple [ byte , byte ] ] | none = none # callback return client certificate byte private key byte param credential : credential | none = none # default custom credential (google.auth.credentials.credentials) use param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param full_model_name : str | none = none # full name model’s endpoint. param location : str = 'us-central1' # default location use making api calls. param max_output_tokens : int = 1024 (alias 'max_tokens') # token limit determines maximum amount text output one prompt. param max_retries : int = 6 # maximum number retries make generating. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str | none = none (alias 'model') # underlying model name. param n : int = 1 # many completion generate prompt. param project : str | none = none # default gcp project use making vertex api calls. param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param request_parallelism : int = 5 # amount parallelism allowed request issued vertexai models. param safety_settings : 'safetysettingstype' | none = none # default safety setting use generations. example: langchain_google_vertexai import harmblockthreshold, harmcategory safety_settings = { harmcategory.harm_category_unspecified: harmblockthreshold.block_none,
harmcategory.harm_category_sexually_explicit: harmblockthreshold.block_none, } param stop : list [ str ] | none = none (alias 'stop_sequences') # optional list stop word use generating. param stream_usage : bool = true # param streaming : bool = false # whether stream result not. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # sampling temperature, control degree randomness token selection. param top_k : int | none = none # model selects token output, next token selected param top_p : float | none = none # token selected probable least sum param tuned_model_name : str | none = none # name tuned model. tuned_model_name passed
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] , * , tool_choice : dict [ str , str ] | literal [ 'any' , 'auto' ] | str | none = none , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model parameter : tool ( sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] ) – tool_choice ( dict [ str , str ] | literal [ 'any' , 'auto' ] | str | none ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) – stop ( list [ str ] | none ) – kwargs ( ) – return type : str configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) → runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) – configurablefield instance used select
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , * , include_raw : bool = false , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] [source] # model wrapper return output formatted match given schema. parameter : schema ( dict | type [ basemodel ] ) – include_raw ( bool ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] property async_prediction_client : predictionserviceasyncclient # return predictionserviceclient. property prediction_client : predictionserviceclient # return predictionserviceclient. task_executor : classvar [ executor | none ] = fieldinfo(exclude=true, extra={}) #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/model_garden/langchain_google_vertexai.model_garden.vertexaimodelgarden.html#langchain_google_vertexai.model_garden.vertexaimodelgarden
vertexaimodelgarden # class langchain_google_vertexai.model_garden. vertexaimodelgarden [source] # bases: _basevertexaimodelgarden , basellm large language model served vertex ai model garden. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. note vertexaimodelgarden implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param allowed_model_args : list [ str ] | none = none # allowed optional args passed model. param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/model_garden_maas.html#langchain-google-vertexai-model-garden-maas
model_garden_maas # class model_garden_maas.llama.vertexmodelgardenllama integration llama 3.1 google cloud vertex ai model-as-a-service. model_garden_maas.mistral.vertexmodelgardenmistral create new model parsing validating input data keyword arguments.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/model_garden_maas/langchain_google_vertexai.model_garden_maas.llama.vertexmodelgardenllama.html#langchain_google_vertexai.model_garden_maas.llama.vertexmodelgardenllama
vertexmodelgardenllama # class langchain_google_vertexai.model_garden_maas.llama. vertexmodelgardenllama [source] # bases: _basevertexmaasmodelgarden , basechatmodel integration llama 3.1 google cloud vertex ai model-as-a-service. information, see: https://cloud.google.com/blog/products/ai-machine-learning/llama-3-1-on-vertex-ai setup: need enable corresponding maas model (google cloud ui console ->
vertex ai -> model garden -> search model need click enable) must langchain-google-vertexai python package installed
credential variable mentioned above, look system-level auth. information, see: https://cloud.google.com/docs/authentication/application-default-credentials#gac https://googleapis.dev/python/google-auth/latest/reference/google.auth.html#module-google.auth . key init args — completion params: model: str name vertexmaas model use (“meta/llama3-405b-instruct-maas”) append_tools_to_system_message: bool whether append tool system message key init args — client params: credentials: optional[google.auth.credentials.credentials] default custom credential use making api calls.
provided, credential ascertained environment. project: optional[str] default gcp project use making vertex api calls. location: str = “us-central1” default location use making api calls. see full list supported init args description params section. instantiate: langchain_google_vertexai import vertexmaas llm = vertexmodelgardenllama ( model = "meta/llama3-405b-instruct-maas" , # params... ) invoke: message = [ ( "system" , "you helpful translator. translate user sentence french." ), ( "human" , "i love programming." ), ] llm . invoke ( message ) aimessage ( content = "j'adore programmer. “, id=’run-925ce305-2268-44c4-875f-dde9128520ad-0’) stream: chunk llm . stream ( message ): print ( chunk ) aimessagechunk ( content = 'j' , id = 'run-9df01d73-84d9-42db-9d6b-b1466a019e89' ) aimessagechunk ( content = "'adore programmer. “, id=’run-9df01d73-84d9-42db-9d6b-b1466a019e89’) aimessagechunk(content=’’, id=’run-9df01d73-84d9-42db-9d6b-b1466a019e89’) stream = llm . stream ( message ) full = next ( stream ) chunk stream : full += chunk full aimessagechunk ( content = "j'adore programmer. “, id=’run-b7f7492c-4cb5-42d0-8fc3-dce9b293b0fb’) create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. note vertexmodelgardenllama implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
us default parameter vertexai.init defined. param append_tools_to_system_message : bool = false # whether append tool system message not. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param client_cert_source : callable [ [ ] , tuple [ byte , byte ] ] | none = none # callback return client certificate byte private key byte param credential : = none # default custom credential (google.auth.credentials.credentials) use param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param full_model_name : str | none = none # full name model’s endpoint. param location : str = 'us-central1' # default location use making api calls. param max_retries : int = 6 # maximum number retries make generating. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_family : vertexmaasmodelfamily | none = none # param model_name : str | none = none (alias 'model') # underlying model name. param project : str | none = none # default gcp project use making vertex api calls. param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param request_parallelism : int = 5 # amount parallelism allowed request issued vertexai models. param stop : list [ str ] | none = none (alias 'stop_sequences') # optional list stop word use generating. param tag : list [ str ] | none = none # tag add run trace. param timeout : int = 120 # param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model. parameter : tool ( sequence [ dict [ str , ] | type | callable | basetool ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) – stop ( list [ str ] | none ) – kwargs ( ) – return type : str configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) → runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) – configurablefield instance used select
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) → int # get number token present text. useful checking input fit model’s context window. parameter : text ( str ) – string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) → int # get number token messages. useful checking input fit model’s context window. parameter : message ( list [ basemessage ] ) – message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) → list [ int ] # return ordered id token text. parameter : text ( str ) – string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] get_url ( ) → str # return type : str invoke ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) → basemessage # transform single input output. override implement. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use invoking runnable.
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [languagemodelinput, union[dict, basemodel ]] example: pydantic schema (include_raw=false): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_core.pydantic_v1 import basemodel langchain_core.utils.function_calling import convert_to_openai_tool class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str dict_schema = convert_to_openai_tool ( answerwithjustification ) llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( dict_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } property async_prediction_client : predictionserviceasyncclient # return predictionserviceclient. property prediction_client : predictionserviceclient # return predictionserviceclient. task_executor : classvar [ executor | none ] = fieldinfo(exclude=true, extra={}) #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/model_garden_maas/langchain_google_vertexai.model_garden_maas.mistral.vertexmodelgardenmistral.html#langchain_google_vertexai.model_garden_maas.mistral.vertexmodelgardenmistral
vertexmodelgardenmistral # class langchain_google_vertexai.model_garden_maas.mistral. vertexmodelgardenmistral [source] # bases: _basevertexmaasmodelgarden , chatmistralai create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. note vertexmodelgardenmistral implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param additional_headers : dict [ str , str ] | none = none # key-value dictionary representing additional header model call param api_endpoint : str | none = none (alias 'base_url') # desired api endpoint, e.g., us-central1-aiplatform.googleapis.com param api_transport : str | none = none # desired api transport method, either ‘grpc’ ‘rest’.
tool keyword argument. false (default), always use streaming case available. param endpoint : str | none = none (alias 'base_url') # param full_model_name : str | none = none # full name model’s endpoint. param location : str = 'us-central1' # default location use making api calls. param max_concurrent_requests : int = 64 # param max_retries : int = 6 # maximum number retries make generating. param max_tokens : int | none = none # param metadata : dict [ str , ] | none = none # metadata add run trace. param mistral_api_key : secretstr | none [optional] (alias 'api_key') # constraint : type = string writeonly = true format = password param model : str = 'mistral-small' (alias 'model_name') # param model_family : vertexmaasmodelfamily | none = none # param model_name : str | none = none (alias 'model') # underlying model name. param project : str | none = none # default gcp project use making vertex api calls. param random_seed : int | none = none # param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param request_parallelism : int = 5 # amount parallelism allowed request issued vertexai models. param safe_mode : bool = false # param stop : list [ str ] | none = none (alias 'stop_sequences') # optional list stop word use generating. param streaming : bool = false # param tag : list [ str ] | none = none # tag add run trace. param temperature : float = 0.7 # param timeout : int = 120 # param top_p : float = 1 # decode using nucleus sampling: consider smallest set token whose
probability sum least top_p. must closed interval [0.0, 1.0]. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) – config ( runnableconfig | sequence [ runnableconfig ] | none ) – return_exceptions ( bool ) – kwargs ( | none ) – return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] # bind tool-like object chat model. assumes model compatible openai tool-calling api. parameter : tool ( sequence [ dict [ str , ] | type | callable | basetool ] ) – list tool definition bind chat model.
support tool definition handled langchain_core.utils.function_calling.convert_to_openai_tool() . tool_choice – tool require model call.
(if any), dict form:
{“type”: “function”, “function”: {“name”: >}}. kwargs ( ) – additional parameter passed directly self.bind(**kwargs) . return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) – stop ( list [ str ] | none ) – kwargs ( ) – return type : str completion_with_retry ( run_manager : callbackmanagerforllmrun | none = none , ** kwargs : ) → [source] # use tenacity retry completion call. parameter : run_manager ( callbackmanagerforllmrun | none ) – kwargs ( ) – return type : configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) → runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) – configurablefield instance used select
subclass override method support streaming output. parameter : input ( languagemodelinput ) – input runnable. config ( optional [ runnableconfig ] ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) – yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type | none = none , * , method : literal [ 'function_calling' , 'json_mode' ] = 'function_calling' , include_raw : bool = false , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # model wrapper return output formatted match given schema. args: schema: output schema. passed as: openai function/tool schema, json schema, typeddict class (support added 0.1.12), pydantic class. schema pydantic class model output
schema field specifying pydantic typeddict class. changed version 0.1.12: added support typeddict class. method: method steering model generation, either “function_calling”
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] example: schema=pydantic class, method=”function_calling”, include_raw=false: typing import optional langchain_mistralai import chatmistralai langchain_core.pydantic_v1 import basemodel , field class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str # provide default value and/or description fields, passed # model. important part improving model's ability # correctly return structured outputs. justification : optional [ str ] = field ( default = none , description = "a justification answer." ) llm = chatmistralai ( model = "mistral-large-latest" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: schema=pydantic class, method=”function_calling”, include_raw=true: langchain_mistralai import chatmistralai langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmistralai ( model = "mistral-large-latest" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: schema=typeddict class, method=”function_calling”, include_raw=false: # important: using python { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example: schema=openai function schema, method=”function_calling”, include_raw=false: langchain_mistralai import chatmistralai oai_schema = { 'name' : 'answerwithjustification' , 'description' : 'an answer user question along justification answer.' , 'parameters' : { 'type' : 'object' , 'properties' : { 'answer' : { 'type' : 'string' }, 'justification' : { 'description' : 'a justification answer.' , 'type' : 'string' } }, 'required' : [ 'answer' ] } } llm = chatmistralai ( model = "mistral-large-latest" , temperature = 0 ) structured_llm = llm . with_structured_output ( oai_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example: schema=pydantic class, method=”json_mode”, include_raw=true: langchain_mistralai import chatmistralai langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): answer : str justification : str llm = chatmistralai ( model = "mistral-large-latest" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , method = "json_mode" , include_raw = true ) structured_llm . invoke ( "answer following question. " "make sure return json blob key 'answer' 'justification'. “ “what’s heavier pound brick pound feathers?” )
# } parameter : schema ( dict | type | none ) – method ( literal [ 'function_calling' , 'json_mode' ] ) – include_raw ( bool ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] property async_prediction_client : predictionserviceasyncclient # return predictionserviceclient. property prediction_client : predictionserviceclient # return predictionserviceclient. task_executor : classvar [ executor | none ] = fieldinfo(exclude=true, extra={}) #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/utils.html#langchain-google-vertexai-utils
utils # function utils.create_context_cache (model, messages) creates cache content model.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/utils/langchain_google_vertexai.utils.create_context_cache.html#langchain_google_vertexai.utils.create_context_cache
create_context_cache # langchain_google_vertexai.utils. create_context_cache ( model : chatvertexai , message : list [ basemessage ] , expire_time : datetime | none = none , time_to_live : timedelta | none = none , tool : sequence [ tool | tool | _tooldictlike | basetool | type [ basemodel ] | functiondescription | callable | functiondeclaration | dict [ str , ] ] | none = none , tool_config : _toolconfigdict | none = none ) → str [source] # creates cache content model. parameter : model ( chatvertexai ) – chatvertexai model. must least gemini-1.5 pro flash. message ( list [ basemessage ] ) – list message cache. expire_time ( datetime | none ) – timestamp resource considered expired. set ( one expire_time ttl set. neither ) – api side used (currently 1 hour). ttl ( default ) – api side used (currently 1 hour). time_to_live ( timedelta | none ) – ttl resource. provided, expiration time computed – created_time + ttl. set – api side used (currently 1 hour). ttl – api side used (currently 1 hour). tool ( sequence [ tool | tool | _tooldictlike | basetool | type [ basemodel ] | functiondescription | callable | functiondeclaration | dict [ str , ] ] | none ) – list tool definition bind chat model.
schema dictionary representation. tool_config ( _toolconfigdict | none ) – optional. immutable. tool config. config shared
tools. raise : valueerror – model doesn’t support context catching. return : string identificator created cache. return type : str
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vectorstores.html#langchain-google-vertexai-vectorstores
vectorstores # class vectorstores.document_storage.datastoredocumentstorage (...) store document google cloud datastore. vectorstores.document_storage.documentstorage () abstract interface key, text storage retrieving documents. vectorstores.document_storage.gcsdocumentstorage (bucket) store document google cloud storage. vectorstores.vectorstores.vectorsearchvectorstore (...) vertexai vectorstore handle search indexing using vector search store document google cloud storage. vectorstores.vectorstores.vectorsearchvectorstoredatastore (...) vectorsearch datastore document storage. vectorstores.vectorstores.vectorsearchvectorstoregcs (...) alias vectorsearchvectorstore consistency rest vector store different document storage backends.
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vectorstores/langchain_google_vertexai.vectorstores.document_storage.datastoredocumentstorage.html#langchain_google_vertexai.vectorstores.document_storage.datastoredocumentstorage
datastoredocumentstorage # class langchain_google_vertexai.vectorstores.document_storage. datastoredocumentstorage ( datastore_client : datastore.client , kind : str = 'document_id' , text_property_name : str = 'text' , metadata_property_name : str = 'metadata' ) [source] # store document google cloud datastore. constructor.
:param bucket: bucket document stored.
:param prefix: prefix prepended document names. method __init__ (datastore_client[, kind, ...]) constructor. amdelete (keys) async delete given key associated values. amget (keys) async get value associated given keys. amset (key_value_pairs) async set value given keys. ayield_keys (*[, prefix]) async get iterator key match given prefix. mdelete (keys) deletes sequence document key. mget (keys) get batch document id. mset (key_value_pairs) store series document using key yield_keys (*[, prefix]) yield key document storage. parameter : datastore_client ( datastore.client ) – kind ( str ) – text_property_name ( str ) – metadata_property_name ( str ) – __init__ ( datastore_client : datastore.client , kind : str = 'document_id' , text_property_name : str = 'text' , metadata_property_name : str = 'metadata' ) → none [source] # constructor.
:param prefix: prefix prepended document names. parameter : datastore_client ( datastore.client ) – kind ( str ) – text_property_name ( str ) – metadata_property_name ( str ) – return type : none async amdelete ( key : sequence [ k ] ) → none # async delete given key associated values. parameter : key ( sequence [ k ] ) – sequence key delete. return type : none async amget ( key : sequence [ k ] ) → list [ v | none ] # async get value associated given keys. parameter : key ( sequence [ k ] ) – sequence keys. return : sequence optional value associated keys.
key found, corresponding value none. return type : list [ v | none] async amset ( key_value_pairs : sequence [ tuple [ k , v ] ] ) → none # async set value given keys. parameter : key_value_pairs ( sequence [ tuple [ k , v ] ] ) – sequence key-value pairs. return type : none async ayield_keys ( * , prefix : str | none = none ) → asynciterator [ k ] | asynciterator [ str ] # async get iterator key match given prefix. parameter : prefix ( str ) – prefix match. yield : iterator[k | str] – iterator key match given prefix.
method allowed return iterator either k str
depending make sense given store. return type : asynciterator [ k ] | asynciterator [str] mdelete ( key : sequence [ str ] ) → none [source] # deletes sequence document key. parameter : key ( sequence [ str ] ) – sequence key delete. return type : none mget ( key : sequence [ str ] ) → list [ document | none ] [source] # get batch document id.
:param ids: list id text. return : list texts. key id found id record return none instead. parameter : key ( sequence [ str ] ) – return type : list [ document | none] mset ( key_value_pairs : sequence [ tuple [ str , document ] ] ) → none [source] # store series document using key parameter : key_value_pairs ( sequence [ tuple [ k , v ] ] ) – sequence key-value pairs. return type : none yield_keys ( * , prefix : str | none = none ) → iterator [ str ] [source] # yield key document storage. parameter : prefix ( str | none ) – ignored return type : iterator [str]
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vectorstores/langchain_google_vertexai.vectorstores.document_storage.documentstorage.html#langchain_google_vertexai.vectorstores.document_storage.documentstorage
documentstorage # class langchain_google_vertexai.vectorstores.document_storage. documentstorage [source] # abstract interface key, text storage retrieving documents. method __init__ () amdelete (keys) async delete given key associated values. amget (keys) async get value associated given keys. amset (key_value_pairs) async set value given keys. ayield_keys (*[, prefix]) async get iterator key match given prefix. mdelete (keys) delete given key associated values. mget (keys) get value associated given keys. mset (key_value_pairs) set value given keys. yield_keys (*[, prefix]) get iterator key match given prefix. __init__ ( ) # async amdelete ( key : sequence [ k ] ) → none # async delete given key associated values. parameter : key ( sequence [ k ] ) – sequence key delete. return type : none async amget ( key : sequence [ k ] ) → list [ v | none ] # async get value associated given keys. parameter : key ( sequence [ k ] ) – sequence keys. return : sequence optional value associated keys.
depending make sense given store. return type : asynciterator [ k ] | asynciterator [str] abstract mdelete ( key : sequence [ k ] ) → none # delete given key associated values. parameter : key ( sequence [ k ] ) – sequence key delete. return type : none abstract mget ( key : sequence [ k ] ) → list [ v | none ] # get value associated given keys. parameter : key ( sequence [ k ] ) – sequence keys. return : sequence optional value associated keys.
key found, corresponding value none. return type : list [ v | none] abstract mset ( key_value_pairs : sequence [ tuple [ k , v ] ] ) → none # set value given keys. parameter : key_value_pairs ( sequence [ tuple [ k , v ] ] ) – sequence key-value pairs. return type : none abstract yield_keys ( * , prefix : str | none = none ) → iterator [ k ] | iterator [ str ] # get iterator key match given prefix. parameter : prefix ( str ) – prefix match. yield : iterator[k | str] – iterator key match given prefix.
depending make sense given store. return type : iterator [ k ] | iterator [str]
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vectorstores/langchain_google_vertexai.vectorstores.document_storage.gcsdocumentstorage.html#langchain_google_vertexai.vectorstores.document_storage.gcsdocumentstorage
gcsdocumentstorage # class langchain_google_vertexai.vectorstores.document_storage. gcsdocumentstorage ( bucket : bucket , prefix : str | none = 'documents' , threaded = true , n_threads = 8 ) [source] # store document google cloud storage.
pair id, document_text name blob {prefix}/{id} stored
plain text format. constructor.
:param prefix: prefix prepended document names. method __init__ (bucket[, prefix, threaded, n_threads]) constructor. amdelete (keys) async delete given key associated values. amget (keys) async get value associated given keys. amset (key_value_pairs) async set value given keys. ayield_keys (*[, prefix]) async get iterator key match given prefix. mdelete (keys) deletes batch document id. mget (keys) get batch document id. mset (key_value_pairs) store series document using key yield_keys (*[, prefix]) yield key present storage. parameter : bucket ( storage.bucket ) – prefix ( optional [ str ] ) – __init__ ( bucket : bucket , prefix : str | none = 'documents' , threaded = true , n_threads = 8 ) → none [source] # constructor.
:param prefix: prefix prepended document names. parameter : bucket ( bucket ) – prefix ( str | none ) – return type : none async amdelete ( key : sequence [ k ] ) → none # async delete given key associated values. parameter : key ( sequence [ k ] ) – sequence key delete. return type : none async amget ( key : sequence [ k ] ) → list [ v | none ] # async get value associated given keys. parameter : key ( sequence [ k ] ) – sequence keys. return : sequence optional value associated keys.
depending make sense given store. return type : asynciterator [ k ] | asynciterator [str] mdelete ( key : sequence [ str ] ) → none [source] # deletes batch document id. parameter : key ( sequence [ str ] ) – list id text. return type : none mget ( key : sequence [ str ] ) → list [ document | none ] [source] # get batch document id.
default implementation loop get_by_id .
subclass faster way retrieve data batch implement
method.
:param ids: list id text. return : list documents. key id found id record return none instead. parameter : key ( sequence [ str ] ) – return type : list [ document | none] mset ( key_value_pairs : sequence [ tuple [ str , document ] ] ) → none [source] # store series document using key parameter : key_value_pairs ( sequence [ tuple [ k , v ] ] ) – sequence key-value pairs. return type : none yield_keys ( * , prefix : str | none = none ) → iterator [ str ] [source] # yield key present storage. parameter : prefix ( str | none ) – ignored. us prefix provided constructor. return type : iterator [str]
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vectorstores/langchain_google_vertexai.vectorstores.vectorstores.vectorsearchvectorstore.html#langchain_google_vertexai.vectorstores.vectorstores.vectorsearchvectorstore
vectorsearchvectorstore # class langchain_google_vertexai.vectorstores.vectorstores. vectorsearchvectorstore ( searcher : searcher , document_storage : documentstorage , embbedings : embeddings | none = none ) [source] # vertexai vectorstore handle search indexing using vector search
store document google cloud storage. constructor. parameter : searcher ( searcher ) – object charge searching storing index. document_storage ( documentstorage ) – object charge storing retrieving documents. embbedings ( embeddings | none ) – object charge transforming text embbeddings. attribute embbedings return embeddings object. embeddings access query embedding object available. method __init__ (searcher, document_storage[, ...]) constructor. aadd_documents (documents, **kwargs) async run document embeddings add vectorstore. aadd_texts (texts[, metadatas]) async run text embeddings add vectorstore. add_documents (documents, **kwargs) add update document vectorstore. add_texts (texts[, metadatas, ids, ...]) run text embeddings add vectorstore. adelete ([ids]) async delete vector id criteria. afrom_documents (documents, embedding, **kwargs) async return vectorstore initialized document embeddings. afrom_texts (texts, embedding[, metadatas]) async return vectorstore initialized text embeddings. aget_by_ids (ids, /) async get document ids. amax_marginal_relevance_search (query[, k, ...]) async return doc selected using maximal marginal relevance. amax_marginal_relevance_search_by_vector (...) async return doc selected using maximal marginal relevance. as_retriever (**kwargs) return vectorstoreretriever initialized vectorstore. asearch (query, search_type, **kwargs) async return doc similar query using specified search type. asimilarity_search (query[, k]) async return doc similar query. asimilarity_search_by_vector (embedding[, k]) async return doc similar embedding vector. asimilarity_search_with_relevance_scores (query) async return doc relevance score range [0, 1]. asimilarity_search_with_score (*args, **kwargs) async run similarity search distance. delete ([ids]) delete vector id criteria. from_components (project_id, region, ...[, ...]) take object creation constructor. from_documents (documents, embedding, **kwargs) return vectorstore initialized document embeddings. from_texts (texts, embedding[, metadatas]) use component instead. get_by_ids (ids, /) get document ids. max_marginal_relevance_search (query[, k, ...]) return doc selected using maximal marginal relevance. max_marginal_relevance_search_by_vector (...) return doc selected using maximal marginal relevance. search (query, search_type, **kwargs) return doc similar query using specified search type. similarity_search (query[, k, filter, ...]) return doc similar query. similarity_search_by_vector (embedding[, k]) return doc similar embedding vector. similarity_search_by_vector_with_score (embedding) return doc similar embedding cosine distance. similarity_search_with_relevance_scores (query) return doc relevance score range [0, 1]. similarity_search_with_score (query[, k, ...]) return doc similar query cosine distance query. __init__ ( searcher : searcher , document_storage : documentstorage , embbedings : embeddings | none = none ) → none # constructor. parameter : searcher ( searcher ) – object charge searching storing index. document_storage ( documentstorage ) – object charge storing retrieving documents. embbedings ( embeddings | none ) – object charge transforming text embbeddings. return type : none async aadd_documents ( document : list [ document ] , ** kwargs : ) → list [ str ] # async run document embeddings add
vectorstore. parameter : document ( list [ document ] ) – document add vectorstore. kwargs ( ) – additional keyword arguments. return : list id added texts. raise : valueerror – number id match number documents. return type : list[str] async aadd_texts ( text : iterable [ str ] , metadata : list [ dict ] | none = none , ** kwargs : ) → list [ str ] # async run text embeddings add vectorstore. parameter : text ( iterable [ str ] ) – iterable string add vectorstore. metadata ( list [ dict ] | none ) – optional list metadata associated texts.
default none. **kwargs ( ) – vectorstore specific parameters. return : list id adding text vectorstore. raise : valueerror – number metadata match number texts. valueerror – number id match number texts. return type : list [str] add_documents ( document : list [ document ] , ** kwargs : ) → list [ str ] # add update document vectorstore. parameter : document ( list [ document ] ) – document add vectorstore. kwargs ( ) – additional keyword arguments.
kwargs contains id document contain ids,
id kwargs receive precedence. return : list id added texts. raise : valueerror – number id match number documents. return type : list[str] add_texts ( text : iterable [ str ] , metadata : list [ dict ] | none = none , * , id : list [ str ] | none = none , is_complete_overwrite : bool = false , ** kwargs : ) → list [ str ] # run text embeddings add vectorstore. parameter : text ( iterable [ str ] ) – iterable string add vectorstore. metadata ( list [ dict ] | none ) – optional list metadata associated texts. id ( list [ str ] | none ) – optional list id assigned text index.
none, unique id generated. is_complete_overwrite ( bool ) – optional, determines whether append
overwrite operation. relevant batch update indexes. kwargs ( ) – vectorstore specific parameters. return : list id adding text vectorstore. return type : list [str] async adelete ( id : list [ str ] | none = none , ** kwargs : ) → bool | none # async delete vector id criteria. parameter : id ( list [ str ] | none ) – list id delete. none, delete all. default none. **kwargs ( ) – keyword argument subclass might use. return : true deletion successful,
false otherwise, none implemented. return type : optional[bool] async classmethod afrom_documents ( document : list [ document ] , embedding : embeddings , ** kwargs : ) → vst # async return vectorstore initialized document embeddings. parameter : document ( list [ document ] ) – list document add vectorstore. embedding ( embeddings ) – embedding function use. kwargs ( ) – additional keyword arguments. return : vectorstore initialized document embeddings. return type : vectorstore async classmethod afrom_texts ( text : list [ str ] , embedding : embeddings , metadata : list [ dict ] | none = none , ** kwargs : ) → vst # async return vectorstore initialized text embeddings. parameter : text ( list [ str ] ) – text add vectorstore. embedding ( embeddings ) – embedding function use. metadata ( list [ dict ] | none ) – optional list metadata associated texts.
default none. kwargs ( ) – additional keyword arguments. return : vectorstore initialized text embeddings. return type : vectorstore async aget_by_ids ( id : sequence [ str ] , / ) → list [ document ] # async get document ids. returned document expected id field set id
document vector store. fewer document may returned requested id found
duplicated ids. user assume order returned document match
order input ids. instead, user rely id field
returned documents. method raise exception document found
ids. parameter : id ( sequence [ str ] ) – list id retrieve. return : list documents. return type : list[ document ] new version 0.2.11. async amax_marginal_relevance_search ( query : str , k : int = 4 , fetch_k : int = 20 , lambda_mult : float = 0.5 , ** kwargs : ) → list [ document ] # async return doc selected using maximal marginal relevance. maximal marginal relevance optimizes similarity query diversity
among selected documents. parameter : query ( str ) – text look document similar to. k ( int ) – number document return. default 4. fetch_k ( int ) – number document fetch pas mmr algorithm.
default 20. lambda_mult ( float ) – number 0 1 determines degree
diversity among result 0 corresponding
maximum diversity 1 minimum diversity.
default 0.5. kwargs ( ) – return : list document selected maximal marginal relevance. return type : list[ document ] async amax_marginal_relevance_search_by_vector ( embedding : list [ float ] , k : int = 4 , fetch_k : int = 20 , lambda_mult : float = 0.5 , ** kwargs : ) → list [ document ] # async return doc selected using maximal marginal relevance. maximal marginal relevance optimizes similarity query diversity
among selected documents. parameter : embedding ( list [ float ] ) – embedding look document similar to. k ( int ) – number document return. default 4. fetch_k ( int ) – number document fetch pas mmr algorithm.
default 0.5. **kwargs ( ) – argument pas search method. return : list document selected maximal marginal relevance. return type : list[ document ] as_retriever ( ** kwargs : ) → vectorstoreretriever # return vectorstoreretriever initialized vectorstore. parameter : **kwargs ( ) – keyword argument pas search function.
include:
search_type (optional[str]): defines type search retriever perform.
“similarity” (default), “mmr”,
“similarity_score_threshold”. search_kwargs (optional[dict]): keyword argument pas search function. include thing like: k: amount document return (default: 4)
score_threshold: minimum relevance threshold similarity_score_threshold fetch_k: amount document pas mmr algorithm (default: 20) lambda_mult: diversity result returned mmr; 1 minimum diversity 0 maximum. (default: 0.5) filter: filter document metadata return : retriever class vectorstore. return type : vectorstoreretriever examples: # retrieve document higher diversity # useful dataset many similar document docsearch . as_retriever ( search_type = "mmr" , search_kwargs = { 'k' : 6 , 'lambda_mult' : 0.25 } ) # fetch document mmr algorithm consider # return top 5 docsearch . as_retriever ( search_type = "mmr" , search_kwargs = { 'k' : 5 , 'fetch_k' : 50 } ) # retrieve document relevance score # certain threshold docsearch . as_retriever ( search_type = "similarity_score_threshold" , search_kwargs = { 'score_threshold' : 0.8 } ) # get single similar document dataset docsearch . as_retriever ( search_kwargs = { 'k' : 1 }) # use filter retrieve document specific paper docsearch . as_retriever ( search_kwargs = { 'filter' : { 'paper_title' : 'gpt-4 technical report' }} ) async asearch ( query : str , search_type : str , ** kwargs : ) → list [ document ] # async return doc similar query using specified search type. parameter : query ( str ) – input text. search_type ( str ) – type search perform. “similarity”,
“mmr”, “similarity_score_threshold”. **kwargs ( ) – argument pas search method. return : list document similar query. raise : valueerror – search_type one “similarity”,
“mmr”, “similarity_score_threshold”. return type : list[ document ] async asimilarity_search ( query : str , k : int = 4 , ** kwargs : ) → list [ document ] # async return doc similar query. parameter : query ( str ) – input text. k ( int ) – number document return. default 4. **kwargs ( ) – argument pas search method. return : list document similar query. return type : list[ document ] async asimilarity_search_by_vector ( embedding : list [ float ] , k : int = 4 , ** kwargs : ) → list [ document ] # async return doc similar embedding vector. parameter : embedding ( list [ float ] ) – embedding look document similar to. k ( int ) – number document return. default 4. **kwargs ( ) – argument pas search method. return : list document similar query vector. return type : list[ document ] async asimilarity_search_with_relevance_scores ( query : str , k : int = 4 , ** kwargs : ) → list [ tuple [ document , float ] ] # async return doc relevance score range [0, 1]. 0 dissimilar, 1 similar. parameter : query ( str ) – input text. k ( int ) – number document return. default 4. **kwargs ( ) – kwargs passed similarity search. include:
score_threshold: optional, floating point value 0 1 filter resulting set retrieved doc return : list tuples (doc, similarity_score) return type : list[tuple[ document , float]] async asimilarity_search_with_score ( * args : , ** kwargs : ) → list [ tuple [ document , float ] ] # async run similarity search distance. parameter : *args ( ) – argument pas search method. **kwargs ( ) – argument pas search method. return : list tuples (doc, similarity_score). return type : list[tuple[ document , float]] delete ( id : list [ str ] | none = none , ** kwargs : ) → bool | none # delete vector id criteria. parameter : id ( list [ str ] | none ) – list id delete. none, delete all. default none. **kwargs ( ) – keyword argument subclass might use. return : true deletion successful,
false otherwise, none implemented. return type : optional[bool] classmethod from_components ( project_id : str , region : str , gcs_bucket_name : str , index_id : str , endpoint_id : str , private_service_connect_ip_address : str | none = none , credentials_path : str | none = none , embedding : embeddings | none = none , stream_update : bool = false , ** kwargs : ) → vectorsearchvectorstore [source] # take object creation constructor. parameter : project_id ( str ) – gcp project id. region ( str ) – default location making api calls. must regional. ( location gc bucket must ) – gcs_bucket_name ( str ) – location vector stored created. ( order index ) – index_id ( str ) – id created index. endpoint_id ( str ) – id created endpoint. private_service_connect_ip_address ( str | none ) – ip address private instance. ( service connect ) – credentials_path ( str | none ) – (optional) path google credential system. ( local file ) – embedding ( embeddings | none ) – embeddings used texts. ( embedding ) – stream_update ( bool ) – whether update streaming batching. vectorsearch
index must compatible stream/batch updates. kwargs ( ) – additional keyword argument pas
vertexaivectorsearch.__init__(). return : configured vertexaivectorsearch. return type : vectorsearchvectorstore classmethod from_documents ( document : list [ document ] , embedding : embeddings , ** kwargs : ) → vst # return vectorstore initialized document embeddings. parameter : document ( list [ document ] ) – list document add vectorstore. embedding ( embeddings ) – embedding function use. kwargs ( ) – additional keyword arguments. return : vectorstore initialized document embeddings. return type : vectorstore classmethod from_texts ( text : list [ str ] , embedding : embeddings , metadata : list [ dict ] | none = none , ** kwargs : ) → _basevertexaivectorstore # use component instead. parameter : text ( list [ str ] ) – embedding ( embeddings ) – metadata ( list [ dict ] | none ) – kwargs ( ) – return type : _basevertexaivectorstore get_by_ids ( id : sequence [ str ] , / ) → list [ document ] # get document ids. returned document expected id field set id
ids. parameter : id ( sequence [ str ] ) – list id retrieve. return : list documents. return type : list[ document ] new version 0.2.11. max_marginal_relevance_search ( query : str , k : int = 4 , fetch_k : int = 20 , lambda_mult : float = 0.5 , ** kwargs : ) → list [ document ] # return doc selected using maximal marginal relevance. maximal marginal relevance optimizes similarity query diversity
default 0.5. **kwargs ( ) – argument pas search method. return : list document selected maximal marginal relevance. return type : list[ document ] max_marginal_relevance_search_by_vector ( embedding : list [ float ] , k : int = 4 , fetch_k : int = 20 , lambda_mult : float = 0.5 , ** kwargs : ) → list [ document ] # return doc selected using maximal marginal relevance. maximal marginal relevance optimizes similarity query diversity
default 0.5. **kwargs ( ) – argument pas search method. return : list document selected maximal marginal relevance. return type : list[ document ] search ( query : str , search_type : str , ** kwargs : ) → list [ document ] # return doc similar query using specified search type. parameter : query ( str ) – input text search_type ( str ) – type search perform. “similarity”,
“mmr”, “similarity_score_threshold”. return type : list[ document ] similarity_search ( query : str , k : int = 4 , filter : list [ namespace ] | none = none , numeric_filter : list [ numericnamespace ] | none = none , ** kwargs : ) → list [ document ] # return doc similar query. parameter : query ( str ) – string used search similar documents. k ( int ) – amount neighbor retrieved. filter ( list [ namespace ] | none ) – optional. list namespaces filtering matching results.
example:
[namespace(“color”, [“red”], []), namespace(“shape”, [], [“squared”])]
match datapoints satisfy “red color” include
datapoints “squared shape”. please refer https://cloud.google.com/vertex-ai/docs/matching-engine/filtering#json detail. numeric_filter ( list [ numericnamespace ] | none ) – optional. list numericnamespaces filterning
matching results. please refer https://cloud.google.com/vertex-ai/docs/matching-engine/filtering#json detail. kwargs ( ) – return : list k matching documents. return type : list [ document ] similarity_search_by_vector ( embedding : list [ float ] , k : int = 4 , ** kwargs : ) → list [ document ] # return doc similar embedding vector. parameter : embedding ( list [ float ] ) – embedding look document similar to. k ( int ) – number document return. default 4. **kwargs ( ) – argument pas search method. return : list document similar query vector. return type : list[ document ] similarity_search_by_vector_with_score ( embedding : list [ float ] , k : int = 4 , filter : list [ namespace ] | none = none , numeric_filter : list [ numericnamespace ] | none = none ) → list [ tuple [ document , float ] ] # return doc similar embedding cosine distance. parameter : embedding ( list [ float ] ) – embedding look document similar to. k ( int ) – number document return. default 4. filter ( list [ namespace ] | none ) – optional. list namespaces filtering
matching results.
matching results. please refer https://cloud.google.com/vertex-ai/docs/matching-engine/filtering#json detail. return : list document similar
query text cosine distance float each.
lower score represents similarity. return type : list[tuple[ document , float]] similarity_search_with_relevance_scores ( query : str , k : int = 4 , ** kwargs : ) → list [ tuple [ document , float ] ] # return doc relevance score range [0, 1]. 0 dissimilar, 1 similar. parameter : query ( str ) – input text. k ( int ) – number document return. default 4. **kwargs ( ) – kwargs passed similarity search. include:
score_threshold: optional, floating point value 0 1 filter resulting set retrieved docs. return : list tuples (doc, similarity_score). return type : list[tuple[ document , float]] similarity_search_with_score ( query : str , k : int = 4 , filter : list [ namespace ] | none = none , numeric_filter : list [ numericnamespace ] | none = none ) → list [ tuple [ document , float ] ] # return doc similar query cosine distance query. parameter : query ( str ) – string query look document similar to. k ( int ) – number document return. default 4. filter ( list [ namespace ] | none ) – optional. list namespaces filtering
lower score represents similarity. return type : list[tuple[ document , float]]
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vectorstores/langchain_google_vertexai.vectorstores.vectorstores.vectorsearchvectorstoredatastore.html#langchain_google_vertexai.vectorstores.vectorstores.vectorsearchvectorstoredatastore
vectorsearchvectorstoredatastore # class langchain_google_vertexai.vectorstores.vectorstores. vectorsearchvectorstoredatastore ( searcher : searcher , document_storage : documentstorage , embbedings : embeddings | none = none ) [source] # vectorsearch datastore document storage. constructor. parameter : searcher ( searcher ) – object charge searching storing index. document_storage ( documentstorage ) – object charge storing retrieving documents. embbedings ( embeddings | none ) – object charge transforming text embbeddings. attribute embbedings return embeddings object. embeddings access query embedding object available. method __init__ (searcher, document_storage[, ...]) constructor. aadd_documents (documents, **kwargs) async run document embeddings add vectorstore. aadd_texts (texts[, metadatas]) async run text embeddings add vectorstore. add_documents (documents, **kwargs) add update document vectorstore. add_texts (texts[, metadatas, ids, ...]) run text embeddings add vectorstore. adelete ([ids]) async delete vector id criteria. afrom_documents (documents, embedding, **kwargs) async return vectorstore initialized document embeddings. afrom_texts (texts, embedding[, metadatas]) async return vectorstore initialized text embeddings. aget_by_ids (ids, /) async get document ids. amax_marginal_relevance_search (query[, k, ...]) async return doc selected using maximal marginal relevance. amax_marginal_relevance_search_by_vector (...) async return doc selected using maximal marginal relevance. as_retriever (**kwargs) return vectorstoreretriever initialized vectorstore. asearch (query, search_type, **kwargs) async return doc similar query using specified search type. asimilarity_search (query[, k]) async return doc similar query. asimilarity_search_by_vector (embedding[, k]) async return doc similar embedding vector. asimilarity_search_with_relevance_scores (query) async return doc relevance score range [0, 1]. asimilarity_search_with_score (*args, **kwargs) async run similarity search distance. delete ([ids]) delete vector id criteria. from_components (project_id, region, ...[, ...]) take object creation constructor. from_documents (documents, embedding, **kwargs) return vectorstore initialized document embeddings. from_texts (texts, embedding[, metadatas]) use component instead. get_by_ids (ids, /) get document ids. max_marginal_relevance_search (query[, k, ...]) return doc selected using maximal marginal relevance. max_marginal_relevance_search_by_vector (...) return doc selected using maximal marginal relevance. search (query, search_type, **kwargs) return doc similar query using specified search type. similarity_search (query[, k, filter, ...]) return doc similar query. similarity_search_by_vector (embedding[, k]) return doc similar embedding vector. similarity_search_by_vector_with_score (embedding) return doc similar embedding cosine distance. similarity_search_with_relevance_scores (query) return doc relevance score range [0, 1]. similarity_search_with_score (query[, k, ...]) return doc similar query cosine distance query. __init__ ( searcher : searcher , document_storage : documentstorage , embbedings : embeddings | none = none ) → none # constructor. parameter : searcher ( searcher ) – object charge searching storing index. document_storage ( documentstorage ) – object charge storing retrieving documents. embbedings ( embeddings | none ) – object charge transforming text embbeddings. return type : none async aadd_documents ( document : list [ document ] , ** kwargs : ) → list [ str ] # async run document embeddings add
false otherwise, none implemented. return type : optional[bool] classmethod from_components ( project_id : str , region : str , index_id : str , endpoint_id : str , index_staging_bucket_name : str | none = none , credentials_path : str | none = none , embedding : embeddings | none = none , stream_update : bool = false , datastore_client_kwargs : dict [ str , ] | none = none , datastore_kind : str = 'document_id' , datastore_text_property_name : str = 'text' , datastore_metadata_property_name : str = 'metadata' , ** kwargs : dict [ str , ] ) → vectorsearchvectorstoredatastore [source] # take object creation constructor. parameter : project_id ( str ) – gcp project id. region ( str ) – default location making api calls. must
location gc bucket must regional. index_id ( str ) – id created index. endpoint_id ( str ) – id created endpoint. index_staging_bucket_name ( str | none ) – (optional) index updated batch,
bucket data staged updating index.
required updating index. credentials_path ( str | none ) – (optional) path google credential system. ( local file ) – embedding ( embeddings | none ) – embeddings used texts. ( embedding ) – stream_update ( bool ) – whether update streaming batching. vectorsearch
index must compatible stream/batch updates. kwargs ( dict [ str , ] ) – additional keyword argument pas
vertexaivectorsearch.__init__(). datastore_client_kwargs ( dict [ str , ] | none ) – datastore_kind ( str ) – datastore_text_property_name ( str ) – datastore_metadata_property_name ( str ) – return : configured vectorsearchvectorstoredatastore. return type : vectorsearchvectorstoredatastore classmethod from_documents ( document : list [ document ] , embedding : embeddings , ** kwargs : ) → vst # return vectorstore initialized document embeddings. parameter : document ( list [ document ] ) – list document add vectorstore. embedding ( embeddings ) – embedding function use. kwargs ( ) – additional keyword arguments. return : vectorstore initialized document embeddings. return type : vectorstore classmethod from_texts ( text : list [ str ] , embedding : embeddings , metadata : list [ dict ] | none = none , ** kwargs : ) → _basevertexaivectorstore # use component instead. parameter : text ( list [ str ] ) – embedding ( embeddings ) – metadata ( list [ dict ] | none ) – kwargs ( ) – return type : _basevertexaivectorstore get_by_ids ( id : sequence [ str ] , / ) → list [ document ] # get document ids. returned document expected id field set id
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vectorstores/langchain_google_vertexai.vectorstores.vectorstores.vectorsearchvectorstoregcs.html#langchain_google_vertexai.vectorstores.vectorstores.vectorsearchvectorstoregcs
vectorsearchvectorstoregcs # class langchain_google_vertexai.vectorstores.vectorstores. vectorsearchvectorstoregcs ( searcher : searcher , document_storage : documentstorage , embbedings : embeddings | none = none ) [source] # alias vectorsearchvectorstore consistency rest vector
store different document storage backends. constructor. parameter : searcher ( searcher ) – object charge searching storing index. document_storage ( documentstorage ) – object charge storing retrieving documents. embbedings ( embeddings | none ) – object charge transforming text embbeddings. attribute embbedings return embeddings object. embeddings access query embedding object available. method __init__ (searcher, document_storage[, ...]) constructor. aadd_documents (documents, **kwargs) async run document embeddings add vectorstore. aadd_texts (texts[, metadatas]) async run text embeddings add vectorstore. add_documents (documents, **kwargs) add update document vectorstore. add_texts (texts[, metadatas, ids, ...]) run text embeddings add vectorstore. adelete ([ids]) async delete vector id criteria. afrom_documents (documents, embedding, **kwargs) async return vectorstore initialized document embeddings. afrom_texts (texts, embedding[, metadatas]) async return vectorstore initialized text embeddings. aget_by_ids (ids, /) async get document ids. amax_marginal_relevance_search (query[, k, ...]) async return doc selected using maximal marginal relevance. amax_marginal_relevance_search_by_vector (...) async return doc selected using maximal marginal relevance. as_retriever (**kwargs) return vectorstoreretriever initialized vectorstore. asearch (query, search_type, **kwargs) async return doc similar query using specified search type. asimilarity_search (query[, k]) async return doc similar query. asimilarity_search_by_vector (embedding[, k]) async return doc similar embedding vector. asimilarity_search_with_relevance_scores (query) async return doc relevance score range [0, 1]. asimilarity_search_with_score (*args, **kwargs) async run similarity search distance. delete ([ids]) delete vector id criteria. from_components (project_id, region, ...[, ...]) take object creation constructor. from_documents (documents, embedding, **kwargs) return vectorstore initialized document embeddings. from_texts (texts, embedding[, metadatas]) use component instead. get_by_ids (ids, /) get document ids. max_marginal_relevance_search (query[, k, ...]) return doc selected using maximal marginal relevance. max_marginal_relevance_search_by_vector (...) return doc selected using maximal marginal relevance. search (query, search_type, **kwargs) return doc similar query using specified search type. similarity_search (query[, k, filter, ...]) return doc similar query. similarity_search_by_vector (embedding[, k]) return doc similar embedding vector. similarity_search_by_vector_with_score (embedding) return doc similar embedding cosine distance. similarity_search_with_relevance_scores (query) return doc relevance score range [0, 1]. similarity_search_with_score (query[, k, ...]) return doc similar query cosine distance query. __init__ ( searcher : searcher , document_storage : documentstorage , embbedings : embeddings | none = none ) → none # constructor. parameter : searcher ( searcher ) – object charge searching storing index. document_storage ( documentstorage ) – object charge storing retrieving documents. embbedings ( embeddings | none ) – object charge transforming text embbeddings. return type : none async aadd_documents ( document : list [ document ] , ** kwargs : ) → list [ str ] # async run document embeddings add
false otherwise, none implemented. return type : optional[bool] classmethod from_components ( project_id : str , region : str , gcs_bucket_name : str , index_id : str , endpoint_id : str , private_service_connect_ip_address : str | none = none , credentials_path : str | none = none , embedding : embeddings | none = none , stream_update : bool = false , ** kwargs : ) → vectorsearchvectorstore # take object creation constructor. parameter : project_id ( str ) – gcp project id. region ( str ) – default location making api calls. must regional. ( location gc bucket must ) – gcs_bucket_name ( str ) – location vector stored created. ( order index ) – index_id ( str ) – id created index. endpoint_id ( str ) – id created endpoint. private_service_connect_ip_address ( str | none ) – ip address private instance. ( service connect ) – credentials_path ( str | none ) – (optional) path google credential system. ( local file ) – embedding ( embeddings | none ) – embeddings used texts. ( embedding ) – stream_update ( bool ) – whether update streaming batching. vectorsearch
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vision_models.html#langchain-google-vertexai-vision-models
vision_models # class vision_models.vertexaiimagecaptioning implementation image captioning model llm. vision_models.vertexaiimagecaptioningchat implementation image captioning model chat. vision_models.vertexaiimageeditorchat given image prompt, edits image. vision_models.vertexaiimagegeneratorchat generates image prompt. vision_models.vertexaivisualqnachat chat implementation visual qna model
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vision_models/langchain_google_vertexai.vision_models.vertexaiimagecaptioning.html#langchain_google_vertexai.vision_models.vertexaiimagecaptioning
vertexaiimagecaptioning # class langchain_google_vertexai.vision_models. vertexaiimagecaptioning [source] # bases: _basevertexaiimagecaptioning , basellm implementation image captioning model llm. note vertexaiimagecaptioning implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param cached_client : = none # param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param language : str = 'en' # language query param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'imagetext@001' # name model use param number_of_results : int = 1 # number result return one query param project : str | none = none # google cloud project param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) → str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) – prompt generate from. stop ( list [ str ] | none ) – stop word use generating. model output cut
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) – input runnable. config ( runnableconfig | none ) – config use runnable. default none. kwargs ( ) – additional keyword argument pas runnable. stop ( list [ str ] | none ) – yield : output runnable. return type : iterator [str] to_json ( ) → serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) → runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) – kwargs ( ) – return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] property client : imagetextmodel #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vision_models/langchain_google_vertexai.vision_models.vertexaiimagecaptioningchat.html#langchain_google_vertexai.vision_models.vertexaiimagecaptioningchat
vertexaiimagecaptioningchat # class langchain_google_vertexai.vision_models. vertexaiimagecaptioningchat [source] # bases: _basevertexaiimagecaptioning , basechatmodel implementation image captioning model chat. note vertexaiimagecaptioningchat implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param cached_client : = none # param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param language : str = 'en' # language query param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'imagetext@001' # name model use param number_of_results : int = 1 # number result return one query param project : str | none = none # google cloud project param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [languagemodelinput, union[dict, basemodel ]] example: pydantic schema (include_raw=false): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_core.pydantic_v1 import basemodel langchain_core.utils.function_calling import convert_to_openai_tool class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str dict_schema = convert_to_openai_tool ( answerwithjustification ) llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( dict_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } property client : imagetextmodel #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vision_models/langchain_google_vertexai.vision_models.vertexaiimageeditorchat.html#langchain_google_vertexai.vision_models.vertexaiimageeditorchat
vertexaiimageeditorchat # class langchain_google_vertexai.vision_models. vertexaiimageeditorchat [source] # bases: _basevertexaiimagegenerator , basechatmodel given image prompt, edits image.
currently support mask free editing. note vertexaiimageeditorchat implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param cached_client : = none # param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param guidance_scale : float | none = none # control strength prompt param language : str | none = none # language text prompt image supported value “en” english,
“hi” hindi, “ja” japanese, “ko” korean, “auto” automatic
language detection param metadata : dict [ str , ] | none = none # metadata add run trace. param model_name : str = 'imagegeneration@002' # name base model param negative_prompt : str | none = none # description want omit
generated image param number_of_results : int = 1 # number image generate param project : str | none = none # google cloud project id param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param seed : int | none = none # random seed image generation param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) → basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) – stop ( list [ str ] | none ) – callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) – kwargs ( ) – return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) → list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [languagemodelinput, union[dict, basemodel ]] example: pydantic schema (include_raw=false): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_core.pydantic_v1 import basemodel langchain_core.utils.function_calling import convert_to_openai_tool class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str dict_schema = convert_to_openai_tool ( answerwithjustification ) llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( dict_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } property client : imagegenerationmodel #
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vision_models/langchain_google_vertexai.vision_models.vertexaiimagegeneratorchat.html#langchain_google_vertexai.vision_models.vertexaiimagegeneratorchat
vertexaiimagegeneratorchat # class langchain_google_vertexai.vision_models. vertexaiimagegeneratorchat [source] # bases: _basevertexaiimagegenerator , basechatmodel generates image prompt. note vertexaiimagegeneratorchat implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param cached_client : = none # param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called
page: https://python.langchain.com/v0.2/api_reference/google_vertexai/vision_models/langchain_google_vertexai.vision_models.vertexaivisualqnachat.html#langchain_google_vertexai.vision_models.vertexaivisualqnachat
vertexaivisualqnachat # class langchain_google_vertexai.vision_models. vertexaivisualqnachat [source] # bases: _baseimagetextmodel , basechatmodel chat implementation visual qna model note vertexaivisualqnachat implement standard runnable interface . 🏃 runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache it’s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param cached_client : = none # param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. “tool_calling”, bypass streaming case model called