page: https://python.langchain.com/v0.2/api_reference/aws/index.html
langchain-aws: 0.1.18 # agent # class agents.base.bedrockagentaction agentaction session id information. agents.base.bedrockagentfinish agentfinish session id information. agents.base.bedrockagentsrunnable invoke bedrock agent agents.base.guardrailconfiguration function agents.base.get_boto_session ([...]) construct boto3 session agents.base.parse_agent_response (response) parses raw response bedrock agent chat_models # class chat_models.bedrock.chatbedrock chat model us bedrock api. chat_models.bedrock.chatpromptadapter () adapter class prepare input langchain prompt format chat model expects. chat_models.bedrock_converse.chatbedrockconverse bedrock chat model integration built bedrock converse api. function chat_models.bedrock.convert_messages_to_prompt_anthropic (...) format list message full prompt anthropic model chat_models.bedrock.convert_messages_to_prompt_llama (...) convert list message prompt llama. chat_models.bedrock.convert_messages_to_prompt_llama3 (...) convert list message prompt llama. chat_models.bedrock.convert_messages_to_prompt_mistral (...) convert list message prompt mistral. deprecated class chat_models.bedrock.bedrockchat deprecated since version 0.1.0: use chatbedrock instead. embeddings # class embeddings.bedrock.bedrockembeddings bedrock embedding models. function_calling # class function_calling.anthropictool function_calling.functiondescription representation callable function send llm. function_calling.tooldescription representation callable function openai api. function_calling.toolsoutputparser field : function function_calling.convert_to_anthropic_tool (tool) function_calling.get_system_message (tools) graph # class graphs.neptune_graph.baseneptunegraph () graphs.neptune_graph.neptuneanalyticsgraph (...) neptune analytics wrapper graph operations. graphs.neptune_graph.neptunegraph (host[, ...]) neptune wrapper graph operations. graphs.neptune_graph.neptunequeryexception (...) exception neptune queries. graphs.neptune_rdf_graph.neptunerdfgraph (host) neptune wrapper rdf graph operations. llm # class llms.bedrock.anthropictool llms.bedrock.bedrockbase base class bedrock models. llms.bedrock.bedrockllm bedrock models. llms.bedrock.llminputoutputadapter () adapter class prepare input langchain format llm model expects. llms.sagemaker_endpoint.contenthandlerbase () handler class transform input llm format sagemaker endpoint expects. llms.sagemaker_endpoint.llmcontenthandler () content handler llm class. llms.sagemaker_endpoint.lineiterator (stream) helper class parsing byte stream input. llms.sagemaker_endpoint.sagemakerendpoint sagemaker inference endpoint models. function llms.bedrock.extract_tool_calls (content) llms.sagemaker_endpoint.enforce_stop_tokens (...) cut text soon stop word occur. deprecated class llms.bedrock.bedrock deprecated since version 0.1.0: use bedrockllm instead. retriever # class retrievers.bedrock.amazonknowledgebasesretriever amazon bedrock knowledge base retrieval. retrievers.bedrock.retrievalconfig configuration retrieval. retrievers.bedrock.searchfilter filter configuration retrieval. retrievers.bedrock.vectorsearchconfig configuration vector search. retrievers.kendra.additionalresultattribute additional result attribute. retrievers.kendra.additionalresultattributevalue value additional result attribute. retrievers.kendra.amazonkendraretriever amazon kendra index retriever. retrievers.kendra.documentattribute document attribute. retrievers.kendra.documentattributevalue value document attribute. retrievers.kendra.highlight information highlight keywords excerpt. retrievers.kendra.queryresult amazon kendra query api search result. retrievers.kendra.queryresultitem query api result item. retrievers.kendra.resultitem base class result item. retrievers.kendra.retrieveresult amazon kendra retrieve api search result. retrievers.kendra.retrieveresultitem retrieve api result item. retrievers.kendra.textwithhighlights text highlights. function retrievers.kendra.clean_excerpt (excerpt) clean excerpt kendra. retrievers.kendra.combined_text (item) combine resultitem title excerpt single string. utility # class utilities.redis.tokenescaper ([escape_chars_re]) escape punctuation within input string. utilities.utils.distancestrategy (value[, ...]) enumerator distance strategy calculating distance vectors. function utilities.math.cosine_similarity (x, y) row-wise cosine similarity two equal-width matrices. utilities.math.cosine_similarity_top_k (x, y) row-wise cosine similarity optional top-k score threshold filtering. utilities.redis.get_client (redis_url, **kwargs) get redis client connection url given. utilities.utils.filter_complex_metadata (...) filter metadata type supported vector store. utilities.utils.maximal_marginal_relevance (...) calculate maximal marginal relevance. utils # function utils.enforce_stop_tokens (text, stop) cut text soon stop word occur. utils.get_num_tokens_anthropic (text) get number token string text. utils.get_token_ids_anthropic (text) get token id string text. vectorstores # class vectorstores.inmemorydb.base.inmemoryvectorstore (...) inmemoryvectorstore vector database. vectorstores.inmemorydb.base.inmemoryvectorstoreretriever retriever inmemoryvectorstore. vectorstores.inmemorydb.filters.inmemorydbfilter () collection inmemorydbfilterfields. vectorstores.inmemorydb.filters.inmemorydbfilterexpression ([...]) logical expression inmemorydbfilterfields. vectorstores.inmemorydb.filters.inmemorydbfilterfield (field) base class inmemorydbfilterfields. vectorstores.inmemorydb.filters.inmemorydbfilteroperator (value) inmemorydbfilteroperator enumerator used create inmemorydbfilterexpressions vectorstores.inmemorydb.filters.inmemorydbnum (field) inmemorydbfilterfield representing numeric field inmemorydb index. vectorstores.inmemorydb.filters.inmemorydbtag (field) inmemorydbfilterfield representing tag inmemorydb index. vectorstores.inmemorydb.filters.inmemorydbtext (field) inmemorydbfilterfield representing text field inmemorydb index. vectorstores.inmemorydb.schema.flatvectorfield schema flat vector field redis. vectorstores.inmemorydb.schema.hnswvectorfield schema hnsw vector field redis. vectorstores.inmemorydb.schema.inmemorydbdistancemetric (value) distance metric redis vector fields. vectorstores.inmemorydb.schema.inmemorydbfield base class redis fields. vectorstores.inmemorydb.schema.inmemorydbmodel schema memorydb index. vectorstores.inmemorydb.schema.inmemorydbvectorfield base class redis vector fields. vectorstores.inmemorydb.schema.numericfieldschema schema numeric field redis. vectorstores.inmemorydb.schema.tagfieldschema schema tag field redis. vectorstores.inmemorydb.schema.textfieldschema schema text field redis. function vectorstores.inmemorydb.base.check_index_exists (...) check memorydb index exists. vectorstores.inmemorydb.filters.check_operator_misuse (func) decorator check misuse equality operators. vectorstores.inmemorydb.schema.read_schema (...) read index schema dict yaml file.
page: https://python.langchain.com/v0.2/api_reference/aws/agents.html#langchain-aws-agents
agent # class agents.base.bedrockagentaction agentaction session id information. agents.base.bedrockagentfinish agentfinish session id information. agents.base.bedrockagentsrunnable invoke bedrock agent agents.base.guardrailconfiguration function agents.base.get_boto_session ([...]) construct boto3 session agents.base.parse_agent_response (response) parses raw response bedrock agent
page: https://python.langchain.com/v0.2/api_reference/aws/agents/langchain_aws.agents.base.bedrockagentaction.html#langchain_aws.agents.base.bedrockagentaction
bedrockagentaction # class langchain_aws.agents.base. bedrockagentaction [source] # bases: agentaction agentaction session id information. parameter : session_id â€“ session id param log : str [required] # additional information log action.
log used ways. first, used audit
exactly llm predicted lead (tool, tool_input).
second, used future iteration show llm prior
thoughts. useful (tool, tool_input) contain
full information llm prediction (for example, thought tool/tool_input). param session_id : str [required] # param tool : str [required] # name tool execute. param tool_input : str | dict [required] # input pas tool. param type : literal [ 'agentaction' ] = 'agentaction' # property message : sequence [ basemessage ] # return message correspond action.
page: https://python.langchain.com/v0.2/api_reference/aws/agents/langchain_aws.agents.base.bedrockagentfinish.html#langchain_aws.agents.base.bedrockagentfinish
bedrockagentfinish # class langchain_aws.agents.base. bedrockagentfinish [source] # bases: agentfinish agentfinish session id information. parameter : session_id â€“ session id override init support instantiation position backward compat. param log : str [required] # additional information log return value.
used pas along full llm prediction, parsed
return value. example, full llm prediction final answer: 2 may want return 2 return value, pas
along full string log (for debugging observability purposes). param return_values : dict [required] # dictionary return values. param session_id : str [required] # param type : literal [ 'agentfinish' ] = 'agentfinish' # property message : sequence [ basemessage ] # message correspond observation.
page: https://python.langchain.com/v0.2/api_reference/aws/agents/langchain_aws.agents.base.bedrockagentsrunnable.html#langchain_aws.agents.base.bedrockagentsrunnable
bedrockagentsrunnable # class langchain_aws.agents.base. bedrockagentsrunnable [source] # bases: runnableserializable [ dict , union [ list [ bedrockagentaction ], bedrockagentfinish ]] invoke bedrock agent note bedrockagentsrunnable implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param agent_alias_id : str | none = 'tstaliasid' # bedrock agent alias id param agent_id : str | none = none # bedrock agent id param client : = none # boto3 client param credentials_profile_name : str | none = none # credential use invoke agent param endpoint_url : str | none = none # endpoint url param region_name : str | none = none # region async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
default false. kwargs ( | none ) â€“ additional keyword argument pas runnable. yield : tuple index input output runnable. return type : asynciterator [ tuple [int, output | exception]] async ainvoke ( input : input , config : runnableconfig | none = none , ** kwargs : ) â†’ output # default implementation ainvoke, call invoke thread. default implementation allows usage async code even
runnable implement native async version invoke. subclass override method run asynchronously. parameter : input ( input ) â€“ config ( runnableconfig | none ) â€“ kwargs ( ) â€“ return type : output async astream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) â†’ asynciterator [ output ] # default implementation astream, call ainvoke.
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) â†’ runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) â€“ dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) classmethod create_agent ( agent_name : str , agent_resource_role_arn : str , foundation_model : str , instruction : str , tool : list [ basetool ] = [] , * , client_token : str | none = none , customer_encryption_key_arn : str | none = none , description : str | none = none , guardrail_configuration : guardrailconfiguration | none = none , idle_session_ttl_in_seconds : int | none = none , credentials_profile_name : str | none = none , region_name : str | none = none , bedrock_endpoint_url : str | none = none , runtime_endpoint_url : str | none = none , ** kwargs : ) â†’ bedrockagentsrunnable [source] # creates bedrock agent runnable used agentexecutor
langgraph. also set bedrock agent, action action group infrastructure
donâ€™t exist, ensures agent prepared state
ready called. parameter : agent_name ( str ) â€“ name agent agent_resource_role_arn ( str ) â€“ amazon resource name (arn) iam role
permission invoke api operation agent. foundation_model ( str ) â€“ foundation model used orchestration
agent create instruction ( str ) â€“ instruction tell agent
interact user tool ( list [ basetool ] ) â€“ list tools. accepts langchainâ€™s basetool format client_token ( str | none ) â€“ unique, case-sensitive identifier ensure api
request completes one time. token match
previous request, amazon bedrock ignores request,
return error customer_encryption_key_arn ( str | none ) â€“ amazon resource name (arn) km key
encrypt agent description ( str | none ) â€“ description agent guardrail_configuration ( guardrailconfiguration | none ) â€“ unique guardrail configuration assigned
agent created. idle_session_ttl_in_seconds ( int | none ) â€“ number second amazon bedrock
keep information userâ€™s conversation agent. user
interaction remains active amount time specified.
conversation occurs time, session expires amazon
bedrock deletes data provided timeout credentials_profile_name ( str | none ) â€“ profile name use different default region_name ( str | none ) â€“ region bedrock agent bedrock_endpoint_url ( str | none ) â€“ endpoint url bedrock agent runtime_endpoint_url ( str | none ) â€“ endpoint url bedrock agent runtime **kwargs ( ) â€“ additional argument return : bedrockagentsrunnable configured invoke bedrock agent return type : bedrockagentsrunnable invoke ( input : dict , config : runnableconfig | none = none ) â†’ list [ bedrockagentaction ] | bedrockagentfinish [source] # invoke bedrock agent. parameter : input ( dict ) â€“ langchain runnable input dictionary include:
input: input text agent
memory_id: memory id use agent memory enabled
session_id: session id use. provided, new session started end_session: boolean indicating whether end session
intermediate_steps: intermediate step used provide roc invocation detail enable_trace: boolean flag enable trace invoke bedrock agent config ( runnableconfig | none ) â€“ return : union[list[bedrockagentaction], bedrockagentfinish] return type : list [ bedrockagentaction ] | bedrockagentfinish stream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) â†’ iterator [ output ] # default implementation stream, call invoke.
page: https://python.langchain.com/v0.2/api_reference/aws/agents/langchain_aws.agents.base.guardrailconfiguration.html#langchain_aws.agents.base.guardrailconfiguration
guardrailconfiguration # class langchain_aws.agents.base. guardrailconfiguration [source] # guardrail_identifier : str # guardrail_version : str #
page: https://python.langchain.com/v0.2/api_reference/aws/agents/langchain_aws.agents.base.get_boto_session.html#langchain_aws.agents.base.get_boto_session
get_boto_session # langchain_aws.agents.base. get_boto_session ( credentials_profile_name : str | none = none , region_name : str | none = none , endpoint_url : str | none = none ) â†’ [source] # construct boto3 session parameter : credentials_profile_name ( str | none ) â€“ region_name ( str | none ) â€“ endpoint_url ( str | none ) â€“ return type :
page: https://python.langchain.com/v0.2/api_reference/aws/agents/langchain_aws.agents.base.parse_agent_response.html#langchain_aws.agents.base.parse_agent_response
parse_agent_response # langchain_aws.agents.base. parse_agent_response ( response : ) â†’ list [ bedrockagentaction ] | bedrockagentfinish [source] # parses raw response bedrock agent parameter : response ( ) â€“ raw response bedrock agent return type : list [ bedrockagentaction ] | bedrockagentfinish return either bedrockagentaction bedrockagentfinish
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models.html#langchain-aws-chat-models
chat_models # class chat_models.bedrock.chatbedrock chat model us bedrock api. chat_models.bedrock.chatpromptadapter () adapter class prepare input langchain prompt format chat model expects. chat_models.bedrock_converse.chatbedrockconverse bedrock chat model integration built bedrock converse api. function chat_models.bedrock.convert_messages_to_prompt_anthropic (...) format list message full prompt anthropic model chat_models.bedrock.convert_messages_to_prompt_llama (...) convert list message prompt llama. chat_models.bedrock.convert_messages_to_prompt_llama3 (...) convert list message prompt llama. chat_models.bedrock.convert_messages_to_prompt_mistral (...) convert list message prompt mistral. deprecated class chat_models.bedrock.bedrockchat deprecated since version 0.1.0: use chatbedrock instead.
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.chatbedrock.html#langchain_aws.chat_models.bedrock.chatbedrock
chatbedrock # class langchain_aws.chat_models.bedrock. chatbedrock [source] # bases: basechatmodel , bedrockbase chat model us bedrock api. note chatbedrock implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param beta_use_converse_api : bool = false # use new bedrock converse api provides standardized interface
bedrock models. support still beta. see chatbedrockconverse doc more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param config : = none # optional botocore.config.config instance pas client. param credentials_profile_name : str | none = none # name profile ~/.aws/credentials ~/.aws/config files,
either access key role information specified.
specified, default credential profile or, ec2 instance,
credential imds used.
see: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. â€œtool_callingâ€, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param endpoint_url : str | none = none # needed donâ€™t want default us-east-1 endpoint param guardrail : mapping [ str , ] | none = {'guardrailidentifier': none, 'guardrailversion': none, 'trace': none} # optional dictionary configure guardrail bedrock. field â€˜guardrailsâ€™ consists two keys: â€˜idâ€™ â€˜versionâ€™,
strings, initialized none. itâ€™s used
determine specific guardrail enabled properly set. type: optional[mapping[str, str]]: mapping â€˜idâ€™ â€˜versionâ€™ keys. example:
llm = bedrock(model_id=â€â€, client=, model_kwargs={},
guardrails={ â€œidâ€: â€œâ€,
â€œversionâ€: â€œâ€}) enable tracing guardrails, set â€˜traceâ€™ key true pas callback handler
â€˜run_managerâ€™ parameter â€˜generateâ€™, â€˜_callâ€™ methods. example:
â€œversionâ€: â€œâ€,
â€œtraceâ€: true}, callbacks=[bedrockasynccallbackhandler()]) [ https://python.langchain.com/docs/modules/callbacks/ ] information callback handlers. class bedrockasynccallbackhandler(asynccallbackhandler): async def on_llm_error( self,
error: baseexception, ** kwargs: any, ) -> any: reason = kwargs.get(â€œreasonâ€)
reason == â€œguardrail_intervenedâ€: â€¦logic handle guardrail interventionâ€¦ param metadata : dict [ str , ] | none = none # metadata add run trace. param model_id : str [required] # id model call, e.g., amazon.titan-text-express-v1,
equivalent modelid property list-foundation-models api. custom
provisioned models, arn value expected. param model_kwargs : dict [ str , ] | none = none # keyword argument pas model. param provider : str | none = none # model provider, e.g., amazon, cohere, ai21, etc. supplied, provider
extracted first part model_id e.g. â€˜amazonâ€™
â€˜amazon.titan-text-express-v1â€™. value provided model id
provider them, e.g., custom provisioned model arn
associated them. param provider_stop_reason_key_map : mapping [ str , str ] = {'ai21': 'finishreason', 'amazon': 'completionreason', 'anthropic': 'stop_reason', 'cohere': 'finish_reason', 'mistral': 'stop_reason'} # param provider_stop_sequence_key_name_map : mapping [ str , str ] = {'ai21': 'stop_sequences', 'amazon': 'stopsequences', 'anthropic': 'stop_sequences', 'cohere': 'stop_sequences', 'mistral': 'stop_sequences'} # param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param region_name : str | none = none # aws region e.g., us-west-2 . fallsback aws_default_region env variable
region specified ~/.aws/config case provided here. param streaming : bool = false # whether stream results. param system_prompt_with_tools : str = '' # param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) â†’ basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) â€“ stop ( list [ str ] | none ) â€“ callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) â€“ kwargs ( ) â€“ return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) â€“ config ( runnableconfig | sequence [ runnableconfig ] | none ) â€“ return_exceptions ( bool ) â€“ kwargs ( | none ) â€“ return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] , * , tool_choice : dict | str | literal [ 'auto' , 'none' ] | bool | none = none , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model. assumes model tool calling api. parameter : tool ( sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] ) â€“ list tool definition bind chat model.
dictionary, pydantic model, callable, basetool. pydantic
schema dictionary representation. tool_choice ( dict | str | literal [ 'auto' , 'none' ] | bool | none ) â€“ tool require model call.
{â€œtypeâ€: â€œfunctionâ€, â€œfunctionâ€: {â€œnameâ€: >}}. **kwargs ( ) â€“ additional parameter pas runnable constructor. return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) â€“ stop ( list [ str ] | none ) â€“ kwargs ( ) â€“ return type : str configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) â†’ runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) â€“ configurablefield instance used select
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) â†’ int [source] # get number token present text. useful checking input fit modelâ€™s context window. parameter : text ( str ) â€“ string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) â†’ int # get number token messages. useful checking input fit modelâ€™s context window. parameter : message ( list [ basemessage ] ) â€“ message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) â†’ list [ int ] [source] # return ordered id token text. parameter : text ( str ) â€“ string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] invoke ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) â†’ basemessage # transform single input output. override implement. parameter : input ( languagemodelinput ) â€“ input runnable. config ( optional [ runnableconfig ] ) â€“ config use invoking runnable.
details. stop ( optional [ list [ str ] ] ) â€“ kwargs ( ) â€“ return : output runnable. return type : basemessage predict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : text ( str ) â€“ stop ( sequence [ str ] | none ) â€“ kwargs ( ) â€“ return type : str predict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) â†’ basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) â€“ stop ( sequence [ str ] | none ) â€“ kwargs ( ) â€“ return type : basemessage set_system_prompt_with_tools ( xml_tools_system_prompt : str ) â†’ none [source] # workaround bind. set system prompt tool parameter : xml_tools_system_prompt ( str ) â€“ return type : none stream ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) â†’ iterator [ basemessagechunk ] # default implementation stream, call invoke.
subclass override method support streaming output. parameter : input ( languagemodelinput ) â€“ input runnable. config ( optional [ runnableconfig ] ) â€“ config use runnable. default none. kwargs ( ) â€“ additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) â€“ yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , * , include_raw : bool = false , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] [source] # model wrapper return output formatted match given schema. parameter : schema ( dict | type [ basemodel ] ) â€“ output schema dict pydantic class. pydantic class
attribute validated, whereas dict be. include_raw ( bool ) â€“ false parsed structured output returned.
key â€œrawâ€, â€œparsedâ€, â€œparsing_errorâ€. kwargs ( ) â€“ return : runnable take chatmodel input. output type depends
include_raw schema. include_raw true output dict keys: raw: basemessage,
parsed: optional[_dictorpydantic],
parsing_error: optional[baseexception], include_raw false schema dict runnable output dict.
include_raw false schema type[basemodel] runnable
output basemodel. return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example: pydantic schema (include_raw=false): langchain_aws.chat_models.bedrock import chatbedrock langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatbedrock ( model_id = "anthropic.claude-3-sonnet-20240229-v1:0" , model_kwargs = { "temperature" : 0.001 }, ) # type: ignore[call-arg] structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_aws.chat_models.bedrock import chatbedrock langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatbedrock ( model_id = "anthropic.claude-3-sonnet-20240229-v1:0" , model_kwargs = { "temperature" : 0.001 }, ) # type: ignore[call-arg] structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_aws.chat_models.bedrock import chatbedrock schema = { "name" : "answerwithjustification" , "description" : "an answer user question along justification answer." , "input_schema" : { "type" : "object" , "properties" : { "answer" : { "type" : "string" }, "justification" : { "type" : "string" }, }, "required" : [ "answer" , "justification" ] } } llm = chatbedrock ( model_id = "anthropic.claude-3-sonnet-20240229-v1:0" , model_kwargs = { "temperature" : 0.001 }, ) # type: ignore[call-arg] structured_llm = llm . with_structured_output ( schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example using chatbedrock aws amazon neptune sparql chatbedrock track token usage chatmodels response metadata
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.chatpromptadapter.html#langchain_aws.chat_models.bedrock.chatpromptadapter
chatpromptadapter # class langchain_aws.chat_models.bedrock. chatpromptadapter [source] # adapter class prepare input langchain prompt format
chat model expects. method __init__ () convert_messages_to_prompt (provider, ...) format_messages (provider, messages) __init__ ( ) # classmethod convert_messages_to_prompt ( provider : str , message : list [ basemessage ] , model : str ) â†’ str [source] # parameter : provider ( str ) â€“ message ( list [ basemessage ] ) â€“ model ( str ) â€“ return type : str classmethod format_messages ( provider : str , message : list [ basemessage ] ) â†’ tuple [ str | none , list [ dict ] ] [source] # parameter : provider ( str ) â€“ message ( list [ basemessage ] ) â€“ return type : tuple [str | none, list [ dict ]]
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.chatbedrockconverse.html#langchain_aws.chat_models.bedrock_converse.chatbedrockconverse
chatbedrockconverse # class langchain_aws.chat_models.bedrock_converse. chatbedrockconverse [source] # bases: basechatmodel bedrock chat model integration built bedrock converse api. implementation eventually replace existing chatbedrock implementation
bedrock converse api feature parity older bedrock api.
specifically converse api yet support custom bedrock models. setup: use amazon bedrock make sure youâ€™ve gone step described
here: https://docs.aws.amazon.com/bedrock/latest/userguide/setting-up.html thatâ€™s completed, install langchain integration: pip install -u langchain-aws key init args â€” completion params: model: str name bedrockconverse model use. temperature: float sampling temperature. max_tokens: optional[int] max number token generate. key init args â€” client params: region_name: optional[str] aws region use, e.g. â€˜us-west-2â€™. base_url: optional[str] bedrock endpoint use. needed donâ€™t want default us-east-
1 endpoint. credentials_profile_name: optional[str] name profile ~/.aws/credentials ~/.aws/config files. see full list supported init args description params section. instantiate: langchain_aws import chatbedrockconverse llm = chatbedrockconverse ( model = "anthropic.claude-3-sonnet-20240229-v1:0" , temperature = 0 , max_tokens = none , # params... ) invoke: message = [ ( "system" , "you helpful translator. translate user sentence french." ), ( "human" , "i love programming." ), ] llm . invoke ( message ) aimessage ( content = [{ 'type' : 'text' , 'text' : "j'aime la programmation." }], response_metadata = { 'responsemetadata' : { 'requestid' : '9ef1e313-a4c1-4f79-b631-171f658d3c0e' , 'httpstatuscode' : 200 , 'httpheaders' : { 'date' : 'sat, 15 jun 2024 01:19:24 gmt' , 'content-type' : 'application/json' , 'content-length' : '205' , 'connection' : 'keep-alive' , 'x-amzn-requestid' : '9ef1e313-a4c1-4f79-b631-171f658d3c0e' }, 'retryattempts' : 0 }, 'stopreason' : 'end_turn' , 'metrics' : { 'latencyms' : 609 }}, id = 'run-754e152b-2b41-4784-9538-d40d71a5c3bc-0' , usage_metadata = { 'input_tokens' : 25 , 'output_tokens' : 11 , 'total_tokens' : 36 }) stream: chunk llm . stream ( message ): print ( chunk ) aimessagechunk ( content = [], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'type' : 'text' , 'text' : 'j' , 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'text' : "'" , 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'text' : 'a' , 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'text' : 'ime' , 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'text' : ' la' , 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'text' : ' programm' , 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'text' : 'ation' , 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'text' : '.' , 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [{ 'index' : 0 }], id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [], response_metadata = { 'stopreason' : 'end_turn' }, id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' ) aimessagechunk ( content = [], response_metadata = { 'metrics' : { 'latencyms' : 581 }}, id = 'run-da3c2606-4792-440a-ac66-72e0d1f6d117' , usage_metadata = { 'input_tokens' : 25 , 'output_tokens' : 11 , 'total_tokens' : 36 }) stream = llm . stream ( message ) full = next ( stream ) chunk stream : full += chunk full aimessagechunk ( content = [{ 'type' : 'text' , 'text' : "j'aime la programmation." , 'index' : 0 }], response_metadata = { 'stopreason' : 'end_turn' , 'metrics' : { 'latencyms' : 554 }}, id = 'run-56a5a5e0-de86-412b-9835-624652dc3539' , usage_metadata = { 'input_tokens' : 25 , 'output_tokens' : 11 , 'total_tokens' : 36 }) tool calling: langchain_core.pydantic_v1 import basemodel , field class getweather ( basemodel ): '''get current weather given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) class getpopulation ( basemodel ): '''get current population given location''' location : str = field ( ... , description = "the city state, e.g. san francisco, ca" ) llm_with_tools = llm . bind_tools ([ getweather , getpopulation ]) ai_msg = llm_with_tools . invoke ( "which city hotter today bigger: la ny?" ) ai_msg . tool_calls [{ 'name' : 'getweather' , 'args' : { 'location' : 'los angeles, ca' }, 'id' : 'tooluse_mspi2igutqygp-xbx6xgvw' }, { 'name' : 'getweather' , 'args' : { 'location' : 'new york, ny' }, 'id' : 'tooluse_tophidhvr2m0xf5_5tyqwg' }, { 'name' : 'getpopulation' , 'args' : { 'location' : 'los angeles, ca' }, 'id' : 'tooluse__gcy_klbsc-gqb-bf_pxng' }, { 'name' : 'getpopulation' , 'args' : { 'location' : 'new york, ny' }, 'id' : 'tooluse_-1hsogx0tqcsaig7cdfy8q' }] see chatbedrockconverse.bind_tools() method more. structured output: typing import optional langchain_core.pydantic_v1 import basemodel , field class joke ( basemodel ): '''joke tell user.''' setup : str = field ( description = "the setup joke" ) punchline : str = field ( description = "the punchline joke" ) rating : optional [ int ] = field ( description = "how funny joke is, 1 10" ) structured_llm = llm . with_structured_output ( joke ) structured_llm . invoke ( "tell joke cats" ) joke ( setup = 'what call cat get dressed up?' , punchline = 'a purrfessional!' , rating = 7 ) see chatbedrockconverse.with_structured_output() more. image input: import base64 import httpx langchain_core.messages import humanmessage image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-gfp-wisconsin-madison-the-nature-boardwalk.jpg" image_data = base64 . b64encode ( httpx . get ( image_url ) . content ) . decode ( "utf-8" ) message = humanmessage ( content = [ { "type" : "text" , "text" : "describe weather image" }, { "type" : "image" , "source" : { "type" : "base64" , "media_type" : "image/jpeg" , "data" : image_data }, }, ], ) ai_msg = llm . invoke ([ message ]) ai_msg . content [{ 'type' : 'text' , 'text' : 'the image depicts sunny day partly cloudy sky. sky brilliant blue color scattered white cloud drifting across. lighting cloud pattern suggest pleasant, mild weather conditions. scene show open grassy field meadow, indicating warm temperature conducive vegetation growth. overall, weather portrayed scenic outdoor image appears sunny clouds, likely representing nice, comfortable day.' }] token usage: ai_msg = llm . invoke ( message ) ai_msg . usage_metadata { 'input_tokens' : 25 , 'output_tokens' : 11 , 'total_tokens' : 36 } response metadata ai_msg = llm . invoke ( message ) ai_msg . response_metadata { 'responsemetadata' : { 'requestid' : '776a2a26-5946-45ae-859e-82dc5f12017c' , 'httpstatuscode' : 200 , 'httpheaders' : { 'date' : 'mon, 17 jun 2024 01:37:05 gmt' , 'content-type' : 'application/json' , 'content-length' : '206' , 'connection' : 'keep-alive' , 'x-amzn-requestid' : '776a2a26-5946-45ae-859e-82dc5f12017c' }, 'retryattempts' : 0 }, 'stopreason' : 'end_turn' , 'metrics' : { 'latencyms' : 1290 }} note chatbedrockconverse implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param additional_model_request_fields : dict [ str , ] | none = none # additional inference parameter model supports. parameter beyond base set inference parameter converse support
inferenceconfig field. param additional_model_response_field_paths : list [ str ] | none = none # additional model parameter field path return response. converse return requested field json pointer object
additionalmodelresponsefields field. following example json
additionalmodelresponsefieldpaths. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param config : = none # optional botocore.config.config instance pas client. param credentials_profile_name : str | none = none # name profile ~/.aws/credentials ~/.aws/config files. profile either access key role information specified.
credential imds used. see: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. â€œtool_callingâ€, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param endpoint_url : str | none = none (alias 'base_url') # needed donâ€™t want default us-east-1 endpoint param guardrail_config : dict [ str , ] | none = none (alias 'guardrails') # configuration information guardrail want use request. param max_tokens : int | none = none # max token generate. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_id : str [required] (alias 'model') # id model call. e.g., "anthropic.claude-3-sonnet-20240229-v1:0" . equivalent
modelid property list-foundation-models api. custom provisioned
models, arn value expected. see https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns list supported built-in models. param provider : str = '' # model provider, e.g., amazon, cohere, ai21, etc. supplied, provider extracted first part model_id, e.g.
â€˜amazonâ€™ â€˜amazon.titan-text-express-v1â€™. value provided model
id provider them, like custom provisioned model
arn associated them. param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param region_name : str | none = none # aws region, e.g., us-west-2 . fall back aws_default_region env variable region specified ~/.aws/config
case provided here. param stop_sequences : list [ str ] | none = none (alias 'stop') # stop generation substring occurs. param supports_tool_choice_values : sequence [ literal [ 'auto' , 'any' , 'tool' ] ] | none = none # type tool_choice value model supports. inferred specified. inferred (â€˜autoâ€™, â€˜anyâ€™, â€˜toolâ€™) â€˜claude-3â€™
model used, (â€˜autoâ€™, â€˜anyâ€™) â€˜mistral-largeâ€™ model used, empty otherwise. param tag : list [ str ] | none = none # tag add run trace. param temperature : float | none = none # sampling temperature. must 0 1. param top_p : float | none = none # percentage most-likely candidate considered next token. must 0 1. example, choose value 0.8 topp, model selects
top 80% probability distribution token could next
sequence. param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) â†’ basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) â€“ stop ( list [ str ] | none ) â€“ callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) â€“ kwargs ( ) â€“ return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) â€“ config ( runnableconfig | sequence [ runnableconfig ] | none ) â€“ return_exceptions ( bool ) â€“ kwargs ( | none ) â€“ return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] , * , tool_choice : dict | str | literal [ 'auto' , 'any' ] | none = none , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # parameter : tool ( sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] ) â€“ tool_choice ( dict | str | literal [ 'auto' , 'any' ] | none ) â€“ kwargs ( ) â€“ return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], basemessage ] call_as_llm ( message : str , stop : list [ str ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( str ) â€“ stop ( list [ str ] | none ) â€“ kwargs ( ) â€“ return type : str configurable_alternatives ( : configurablefield , * , default_key : str = 'default' , prefix_keys : bool = false , ** kwargs : runnable [ input , output ] | callable [ [ ] , runnable [ input , output ] ] ) â†’ runnableserializable [ input , output ] # configure alternative runnables set runtime. parameter : ( configurablefield ) â€“ configurablefield instance used select
subclass override method support streaming output. parameter : input ( languagemodelinput ) â€“ input runnable. config ( optional [ runnableconfig ] ) â€“ config use runnable. default none. kwargs ( ) â€“ additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) â€“ yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict [ str , ] | type [ _bm ] | type , * , include_raw : bool = false , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] [source] # model wrapper return output formatted match given schema. parameter : schema ( dict [ str , ] | type [ _bm ] | type ) â€“ output schema. passed as: openai function/tool schema, json schema, typeddict class (support added 0.2.26), pydantic class. schema pydantic class model output
instance schema (i.e., pydantic object). otherwise, include_raw false runnable output dict. include_raw true, runnable output dict keys: "raw" : basemessage "parsed" : none parsing error, otherwise type depends schema described above. "parsing_error" : optional[baseexception] return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example: pydantic schema (include_raw=false): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_core.pydantic_v1 import basemodel langchain_core.utils.function_calling import convert_to_openai_tool class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str dict_schema = convert_to_openai_tool ( answerwithjustification ) llm = chatmodel ( model = "model-name" , temperature = 0 ) structured_llm = llm . with_structured_output ( dict_schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # } example using chatbedrockconverse chatbedrock
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.convert_messages_to_prompt_anthropic.html#langchain_aws.chat_models.bedrock.convert_messages_to_prompt_anthropic
convert_messages_to_prompt_anthropic # langchain_aws.chat_models.bedrock. convert_messages_to_prompt_anthropic ( message : list [ basemessage ] , * , human_prompt : str = '\n\nhuman:' , ai_prompt : str = '\n\nassistant:' ) â†’ str [source] # format list message full prompt anthropic model args: message (list[basemessage]): list basemessage combine.
human_prompt (str, optional): human prompt tag. default â€œ human:â€. ai_prompt (str, optional): ai prompt tag. default â€œ assistant:â€. returns: str: combined string necessary human_prompt ai_prompt tags. parameter : message ( list [ basemessage ] ) â€“ human_prompt ( str ) â€“ ai_prompt ( str ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.convert_messages_to_prompt_llama.html#langchain_aws.chat_models.bedrock.convert_messages_to_prompt_llama
convert_messages_to_prompt_llama # langchain_aws.chat_models.bedrock. convert_messages_to_prompt_llama ( message : list [ basemessage ] ) â†’ str [source] # convert list message prompt llama. parameter : message ( list [ basemessage ] ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.convert_messages_to_prompt_llama3.html#langchain_aws.chat_models.bedrock.convert_messages_to_prompt_llama3
convert_messages_to_prompt_llama3 # langchain_aws.chat_models.bedrock. convert_messages_to_prompt_llama3 ( message : list [ basemessage ] ) â†’ str [source] # convert list message prompt llama. parameter : message ( list [ basemessage ] ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.convert_messages_to_prompt_mistral.html#langchain_aws.chat_models.bedrock.convert_messages_to_prompt_mistral
convert_messages_to_prompt_mistral # langchain_aws.chat_models.bedrock. convert_messages_to_prompt_mistral ( message : list [ basemessage ] ) â†’ str [source] # convert list message prompt mistral. parameter : message ( list [ basemessage ] ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.bedrockchat.html#langchain_aws.chat_models.bedrock.bedrockchat
bedrockchat # class langchain_aws.chat_models.bedrock. bedrockchat [source] # bases: chatbedrock deprecated since version 0.1.0: use chatbedrock instead. note bedrockchat implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param beta_use_converse_api : bool = false # use new bedrock converse api provides standardized interface
yielding result complete. parameter : input ( sequence [ input ] ) â€“ config ( runnableconfig | sequence [ runnableconfig ] | none ) â€“ return_exceptions ( bool ) â€“ kwargs ( | none ) â€“ return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] , * , tool_choice : dict | str | literal [ 'auto' , 'none' ] | bool | none = none , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] # bind tool-like object chat model. assumes model tool calling api. parameter : tool ( sequence [ dict [ str , ] | type [ basemodel ] | callable | basetool ] ) â€“ list tool definition bind chat model.
details. stop ( optional [ list [ str ] ] ) â€“ kwargs ( ) â€“ return : output runnable. return type : basemessage predict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : text ( str ) â€“ stop ( sequence [ str ] | none ) â€“ kwargs ( ) â€“ return type : str predict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) â†’ basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) â€“ stop ( sequence [ str ] | none ) â€“ kwargs ( ) â€“ return type : basemessage set_system_prompt_with_tools ( xml_tools_system_prompt : str ) â†’ none # workaround bind. set system prompt tool parameter : xml_tools_system_prompt ( str ) â€“ return type : none stream ( input : languagemodelinput , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) â†’ iterator [ basemessagechunk ] # default implementation stream, call invoke.
subclass override method support streaming output. parameter : input ( languagemodelinput ) â€“ input runnable. config ( optional [ runnableconfig ] ) â€“ config use runnable. default none. kwargs ( ) â€“ additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) â€“ yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , * , include_raw : bool = false , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # model wrapper return output formatted match given schema. parameter : schema ( dict | type [ basemodel ] ) â€“ output schema dict pydantic class. pydantic class
output basemodel. return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example: pydantic schema (include_raw=false): langchain_aws.chat_models.bedrock import chatbedrock langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatbedrock ( model_id = "anthropic.claude-3-sonnet-20240229-v1:0" , model_kwargs = { "temperature" : 0.001 }, ) # type: ignore[call-arg] structured_llm = llm . with_structured_output ( answerwithjustification ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> answerwithjustification( # answer='they weigh same', # justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.' # ) example: pydantic schema (include_raw=true): langchain_aws.chat_models.bedrock import chatbedrock langchain_core.pydantic_v1 import basemodel class answerwithjustification ( basemodel ): '''an answer user question along justification answer.''' answer : str justification : str llm = chatbedrock ( model_id = "anthropic.claude-3-sonnet-20240229-v1:0" , model_kwargs = { "temperature" : 0.001 }, ) # type: ignore[call-arg] structured_llm = llm . with_structured_output ( answerwithjustification , include_raw = true ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'raw': aimessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ao02pnfyxd6gn1yzc0uxpsvf', 'function': {'arguments': '{"answer":"they weigh same.","justification":"both pound brick pound feather weigh one pound. weight same, volume density object may differ."}', 'name': 'answerwithjustification'}, 'type': 'function'}]}), # 'parsed': answerwithjustification(answer='they weigh same.', justification='both pound brick pound feather weigh one pound. weight same, volume density object may differ.'), # 'parsing_error': none # } example: dict schema (include_raw=false): langchain_aws.chat_models.bedrock import chatbedrock schema = { "name" : "answerwithjustification" , "description" : "an answer user question along justification answer." , "input_schema" : { "type" : "object" , "properties" : { "answer" : { "type" : "string" }, "justification" : { "type" : "string" }, }, "required" : [ "answer" , "justification" ] } } llm = chatbedrock ( model_id = "anthropic.claude-3-sonnet-20240229-v1:0" , model_kwargs = { "temperature" : 0.001 }, ) # type: ignore[call-arg] structured_llm = llm . with_structured_output ( schema ) structured_llm . invoke ( "what weighs pound brick pound feathers" ) # -> { # 'answer': 'they weigh same', # 'justification': 'both pound brick pound feather weigh one pound. weight same, volume density two substance differ.' # }
page: https://python.langchain.com/v0.2/api_reference/aws/embeddings.html#langchain-aws-embeddings
embeddings # class embeddings.bedrock.bedrockembeddings bedrock embedding models.
page: https://python.langchain.com/v0.2/api_reference/aws/embeddings/langchain_aws.embeddings.bedrock.bedrockembeddings.html#langchain_aws.embeddings.bedrock.bedrockembeddings
bedrockembeddings # class langchain_aws.embeddings.bedrock. bedrockembeddings [source] # bases: basemodel , embeddings bedrock embedding models. authenticate, aws client us following method
automatically load credentials: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html specific credential profile used, must pas
name profile ~/.aws/credentials file used. make sure credential / role used required policy
access bedrock service. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param client : = none # bedrock client. param config : = none # optional botocore.config.config instance pas client. param credentials_profile_name : str | none = none # name profile ~/.aws/credentials ~/.aws/config files,
see: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html param endpoint_url : str | none = none # needed donâ€™t want default us-east-1 endpoint param model_id : str = 'amazon.titan-embed-text-v1' # id model call, e.g., amazon.titan-embed-text-v1,
equivalent modelid property list-foundation-models api param model_kwargs : dict | none = none # keyword argument pas model. param normalize : bool = false # whether embeddings normalized unit vector param region_name : str | none = none # aws region e.g., us-west-2 . fallsback aws_default_region env variable
region specified ~/.aws/config case provided here. async aembed_documents ( text : list [ str ] ) â†’ list [ list [ float ] ] [source] # asynchronous compute doc embeddings using bedrock model. parameter : text ( list [ str ] ) â€“ list text embed return : list embeddings, one text. return type : list [ list [float]] async aembed_query ( text : str ) â†’ list [ float ] [source] # asynchronous compute query embeddings using bedrock model. parameter : text ( str ) â€“ text embed. return : embeddings text. return type : list [float] embed_documents ( text : list [ str ] ) â†’ list [ list [ float ] ] [source] # compute doc embeddings using bedrock model. parameter : text ( list [ str ] ) â€“ list text embed return : list embeddings, one text. return type : list [ list [float]] embed_query ( text : str ) â†’ list [ float ] [source] # compute query embeddings using bedrock model. parameter : text ( str ) â€“ text embed. return : embeddings text. return type : list [float] example using bedrockembeddings aws amazon memorydb bedrock
page: https://python.langchain.com/v0.2/api_reference/aws/function_calling.html#langchain-aws-function-calling
function_calling # method creating function spec style bedrock function
supported model provider class function_calling.anthropictool function_calling.functiondescription representation callable function send llm. function_calling.tooldescription representation callable function openai api. function_calling.toolsoutputparser field : function function_calling.convert_to_anthropic_tool (tool) function_calling.get_system_message (tools)
page: https://python.langchain.com/v0.2/api_reference/aws/function_calling/langchain_aws.function_calling.anthropictool.html#langchain_aws.function_calling.anthropictool
anthropictool # class langchain_aws.function_calling. anthropictool [source] # name : str # description : str # input_schema : dict [ str , ] #
page: https://python.langchain.com/v0.2/api_reference/aws/function_calling/langchain_aws.function_calling.functiondescription.html#langchain_aws.function_calling.functiondescription
functiondescription # class langchain_aws.function_calling. functiondescription [source] # representation callable function send llm. name : str # name function. description : str # description function. parameter : dict # parameter function.
page: https://python.langchain.com/v0.2/api_reference/aws/function_calling/langchain_aws.function_calling.tooldescription.html#langchain_aws.function_calling.tooldescription
tooldescription # class langchain_aws.function_calling. tooldescription [source] # representation callable function openai api. type : literal [ 'function' ] # function : functiondescription #
page: https://python.langchain.com/v0.2/api_reference/aws/function_calling/langchain_aws.function_calling.toolsoutputparser.html#langchain_aws.function_calling.toolsoutputparser
toolsoutputparser # class langchain_aws.function_calling. toolsoutputparser [source] # bases: basegenerationoutputparser note toolsoutputparser implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param args_only : bool = false # param first_tool_only : bool = false # param pydantic_schemas : list [ type [ basemodel ] ] | none = none # async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
page: https://python.langchain.com/v0.2/api_reference/aws/function_calling/langchain_aws.function_calling.convert_to_anthropic_tool.html#langchain_aws.function_calling.convert_to_anthropic_tool
convert_to_anthropic_tool # langchain_aws.function_calling. convert_to_anthropic_tool ( tool : dict [ str , ] | type [ basemodel ] | callable | basetool ) â†’ anthropictool [source] # parameter : tool ( dict [ str , ] | type [ basemodel ] | callable | basetool ) â€“ return type : anthropictool
page: https://python.langchain.com/v0.2/api_reference/aws/function_calling/langchain_aws.function_calling.get_system_message.html#langchain_aws.function_calling.get_system_message
get_system_message # langchain_aws.function_calling. get_system_message ( tool : list [ anthropictool ] ) â†’ str [source] # parameter : tool ( list [ anthropictool ] ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/graphs.html#langchain-aws-graphs
graph # class graphs.neptune_graph.baseneptunegraph () graphs.neptune_graph.neptuneanalyticsgraph (...) neptune analytics wrapper graph operations. graphs.neptune_graph.neptunegraph (host[, ...]) neptune wrapper graph operations. graphs.neptune_graph.neptunequeryexception (...) exception neptune queries. graphs.neptune_rdf_graph.neptunerdfgraph (host) neptune wrapper rdf graph operations.
page: https://python.langchain.com/v0.2/api_reference/aws/graphs/langchain_aws.graphs.neptune_graph.baseneptunegraph.html#langchain_aws.graphs.neptune_graph.baseneptunegraph
baseneptunegraph # class langchain_aws.graphs.neptune_graph. baseneptunegraph [source] # attribute get_schema return schema neptune database method __init__ () query (query[, params]) __init__ ( ) # abstract query ( query : str , params : dict = {} ) â†’ dict [source] # parameter : query ( str ) â€“ params ( dict ) â€“ return type : dict
page: https://python.langchain.com/v0.2/api_reference/aws/graphs/langchain_aws.graphs.neptune_graph.neptuneanalyticsgraph.html#langchain_aws.graphs.neptune_graph.neptuneanalyticsgraph
neptuneanalyticsgraph # class langchain_aws.graphs.neptune_graph. neptuneanalyticsgraph ( graph_identifier : str , client : = none , credentials_profile_name : str | none = none , region_name : str | none = none ) [source] # neptune analytics wrapper graph operations. parameter : client ( ) â€“ optional boto3 neptune client credentials_profile_name ( str | none ) â€“ optional aws profile name region_name ( str | none ) â€“ optional aws region, e.g., us-west-2 graph_identifier ( str ) â€“ graph identifier neptune analytics graph example graph = neptuneanalyticsgraph( graph_identifier=â€™â€™ ) security note : make sure database connection us credential narrowly-scoped include necessary permissions.
failure may result data corruption loss, since calling
code may attempt command would result deletion, mutation
data appropriately prompted reading sensitive data
data present database.
best way guard negative outcome (as appropriate)
limit permission granted credential used tool. see https://python.langchain.com/docs/security information. create new neptune analytics graph wrapper instance. attribute get_schema return schema neptune database method __init__ (graph_identifier[, client, ...]) create new neptune analytics graph wrapper instance. query (query[, params]) query neptune database. __init__ ( graph_identifier : str , client : = none , credentials_profile_name : str | none = none , region_name : str | none = none ) â†’ none [source] # create new neptune analytics graph wrapper instance. parameter : graph_identifier ( str ) â€“ client ( ) â€“ credentials_profile_name ( str | none ) â€“ region_name ( str | none ) â€“ return type : none query ( query : str , params : dict = {} ) â†’ dict [ str , ] [source] # query neptune database. parameter : query ( str ) â€“ params ( dict ) â€“ return type : dict [str, ] example using neptuneanalyticsgraph aws amazon neptune cypher
page: https://python.langchain.com/v0.2/api_reference/aws/graphs/langchain_aws.graphs.neptune_graph.neptunegraph.html#langchain_aws.graphs.neptune_graph.neptunegraph
neptunegraph # class langchain_aws.graphs.neptune_graph. neptunegraph ( host : str , port : int = 8182 , use_https : bool = true , client : = none , credentials_profile_name : str | none = none , region_name : str | none = none , sign : bool = true ) [source] # neptune wrapper graph operations. parameter : host ( str ) â€“ endpoint database instance port ( int ) â€“ port number database instance, default 8182 use_https ( bool ) â€“ whether use secure connection, default true client ( ) â€“ optional boto3 neptune client credentials_profile_name ( str | none ) â€“ optional aws profile name region_name ( str | none ) â€“ optional aws region, e.g., us-west-2 service â€“ optional service name, default neptunedata sign ( bool ) â€“ optional, whether sign request payload, default true example graph = neptunegraph( host=â€™â€™,
port=8182 ) security note : make sure database connection us credential narrowly-scoped include necessary permissions.
limit permission granted credential used tool. see https://python.langchain.com/docs/security information. create new neptune graph wrapper instance. attribute get_schema return schema neptune database method __init__ (host[, port, use_https, client, ...]) create new neptune graph wrapper instance. query (query[, params]) query neptune database. __init__ ( host : str , port : int = 8182 , use_https : bool = true , client : = none , credentials_profile_name : str | none = none , region_name : str | none = none , sign : bool = true ) â†’ none [source] # create new neptune graph wrapper instance. parameter : host ( str ) â€“ port ( int ) â€“ use_https ( bool ) â€“ client ( ) â€“ credentials_profile_name ( str | none ) â€“ region_name ( str | none ) â€“ sign ( bool ) â€“ return type : none query ( query : str , params : dict = {} ) â†’ dict [ str , ] [source] # query neptune database. parameter : query ( str ) â€“ params ( dict ) â€“ return type : dict [str, ] example using neptunegraph aws amazon neptune cypher
page: https://python.langchain.com/v0.2/api_reference/aws/graphs/langchain_aws.graphs.neptune_graph.neptunequeryexception.html#langchain_aws.graphs.neptune_graph.neptunequeryexception
neptunequeryexception # class langchain_aws.graphs.neptune_graph. neptunequeryexception ( exception : str | dict ) [source] # exception neptune queries. parameter : exception ( str | dict ) â€“
page: https://python.langchain.com/v0.2/api_reference/aws/graphs/langchain_aws.graphs.neptune_rdf_graph.neptunerdfgraph.html#langchain_aws.graphs.neptune_rdf_graph.neptunerdfgraph
neptunerdfgraph # class langchain_aws.graphs.neptune_rdf_graph. neptunerdfgraph ( host : str , port : int = 8182 , use_https : bool = true , use_iam_auth : bool = false , client : = none , credentials_profile_name : str | none = none , region_name : str | none = none , service : str = 'neptunedata' , sign : bool = true ) [source] # neptune wrapper rdf graph operations. parameter : host ( str ) â€“ endpoint database instance port ( int ) â€“ port number database instance, default 8182 use_iam_auth ( bool ) â€“ boolean indicating iam auth enabled neptune cluster use_https ( bool ) â€“ whether use secure connection, default true client ( ) â€“ optional boto3 neptune client credentials_profile_name ( str | none ) â€“ optional aws profile name region_name ( str | none ) â€“ optional aws region, e.g., us-west-2 service ( str ) â€“ optional service name, default neptunedata sign ( bool ) â€“ optional, whether sign request payload, default true example graph = neptunerdfgraph( host=â€™,
port= )
schema = graph.get_schema()
graph = neptunerdfgraph( host=â€™,
schema_elem = graph.get_schema_elements()
#â€¦ change schema_elements â€¦
graph.load_schema(schema_elem) security note : make sure database connection us credential narrowly-scoped include necessary permissions.
limit permission granted credential used tool. see https://python.langchain.com/docs/security information. attribute get_schema return schema graph database. get_schema_elements method __init__ (host[, port, use_https, ...]) get_summary () obtain neptune statistical summary class predicate graph. load_schema (schema_elements) generates set schema schema_elements. query (query) run neptune query. __init__ ( host : str , port : int = 8182 , use_https : bool = true , use_iam_auth : bool = false , client : = none , credentials_profile_name : str | none = none , region_name : str | none = none , service : str = 'neptunedata' , sign : bool = true ) â†’ none [source] # parameter : host ( str ) â€“ port ( int ) â€“ use_https ( bool ) â€“ use_iam_auth ( bool ) â€“ client ( ) â€“ credentials_profile_name ( str | none ) â€“ region_name ( str | none ) â€“ service ( str ) â€“ sign ( bool ) â€“ return type : none get_summary ( ) â†’ dict [ str , ] [source] # obtain neptune statistical summary class predicate graph. return type : dict [str, ] load_schema ( schema_elements : dict [ str , ] ) â†’ none [source] # generates set schema schema_elements. helpful
case introspected schema need pruning. parameter : schema_elements ( dict [ str , ] ) â€“ return type : none query ( query : str ) â†’ dict [ str , ] [source] # run neptune query. parameter : query ( str ) â€“ return type : dict [str, ] example using neptunerdfgraph aws amazon neptune sparql
page: https://python.langchain.com/v0.2/api_reference/aws/llms.html#langchain-aws-llms
llm # class llms.bedrock.anthropictool llms.bedrock.bedrockbase base class bedrock models. llms.bedrock.bedrockllm bedrock models. llms.bedrock.llminputoutputadapter () adapter class prepare input langchain format llm model expects. llms.sagemaker_endpoint.contenthandlerbase () handler class transform input llm format sagemaker endpoint expects. llms.sagemaker_endpoint.llmcontenthandler () content handler llm class. llms.sagemaker_endpoint.lineiterator (stream) helper class parsing byte stream input. llms.sagemaker_endpoint.sagemakerendpoint sagemaker inference endpoint models. function llms.bedrock.extract_tool_calls (content) llms.sagemaker_endpoint.enforce_stop_tokens (...) cut text soon stop word occur. deprecated class llms.bedrock.bedrock deprecated since version 0.1.0: use bedrockllm instead.
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.bedrock.anthropictool.html#langchain_aws.llms.bedrock.anthropictool
anthropictool # class langchain_aws.llms.bedrock. anthropictool [source] # name : str # description : str # input_schema : dict [ str , ] #
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.bedrock.bedrockbase.html#langchain_aws.llms.bedrock.bedrockbase
bedrockbase # class langchain_aws.llms.bedrock. bedrockbase [source] # bases: baselanguagemodel , abc base class bedrock models. note bedrockbase implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback : callback = none # callback add run trace. param config : = none # optional botocore.config.config instance pas client. param credentials_profile_name : str | none = none # name profile ~/.aws/credentials ~/.aws/config files,
see: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param endpoint_url : str | none = none # needed donâ€™t want default us-east-1 endpoint param guardrail : mapping [ str , ] | none = {'guardrailidentifier': none, 'guardrailversion': none, 'trace': none} # optional dictionary configure guardrail bedrock. field â€˜guardrailsâ€™ consists two keys: â€˜idâ€™ â€˜versionâ€™,
associated them. param provider_stop_reason_key_map : mapping [ str , str ] = {'ai21': 'finishreason', 'amazon': 'completionreason', 'anthropic': 'stop_reason', 'cohere': 'finish_reason', 'mistral': 'stop_reason'} # param provider_stop_sequence_key_name_map : mapping [ str , str ] = {'ai21': 'stop_sequences', 'amazon': 'stopsequences', 'anthropic': 'stop_sequences', 'cohere': 'stop_sequences', 'mistral': 'stop_sequences'} # param region_name : str | none = none # aws region e.g., us-west-2 . fallsback aws_default_region env variable
region specified ~/.aws/config case provided here. param streaming : bool = false # whether stream results. param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
default false. kwargs ( | none ) â€“ additional keyword argument pas runnable. yield : tuple index input output runnable. return type : asynciterator [ tuple [int, output | exception]] abstract async agenerate_prompt ( prompt : list [ promptvalue ] , stop : list [ str ] | none = none , callback : callback = none , ** kwargs : ) â†’ llmresult # asynchronously pas sequence prompt return model generations. method make use batched call model expose batched
text generation model basemessages chat models). stop ( optional [ list [ str ] ] ) â€“ stop word use generating. model output cut
first occurrence substrings. callback ( callback ) â€“ callback pas through. used executing additional
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult async ainvoke ( input : input , config : runnableconfig | none = none , ** kwargs : ) â†’ output # default implementation ainvoke, call invoke thread. default implementation allows usage async code even
runnable implement native async version invoke. subclass override method run asynchronously. parameter : input ( input ) â€“ config ( runnableconfig | none ) â€“ kwargs ( ) â€“ return type : output abstract async apredict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use ainvoke instead. asynchronously pas string model return string. use method calling pure text generation model top candidate generation needed. parameter : text ( str ) â€“ string input pas model. stop ( sequence [ str ] | none ) â€“ stop word use generating. model output cut
first occurrence substrings. **kwargs ( ) â€“ arbitrary additional keyword arguments. usually passed
model provider api call. return : top model prediction string. return type : str abstract async apredict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) â†’ basemessage # deprecated since version langchain-core==0.1.7: use ainvoke instead. asynchronously pas message model return message. use method calling chat model top candidate generation needed. parameter : message ( list [ basemessage ] ) â€“ sequence chat message corresponding single model input. stop ( sequence [ str ] | none ) â€“ stop word use generating. model output cut
model provider api call. return : top model prediction message. return type : basemessage async astream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) â†’ asynciterator [ output ] # default implementation astream, call ainvoke.
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) â†’ runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) â€“ dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) abstract generate_prompt ( prompt : list [ promptvalue ] , stop : list [ str ] | none = none , callback : callback = none , ** kwargs : ) â†’ llmresult # pas sequence prompt model return model generations. method make use batched call model expose batched
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) â†’ int # get number token present text. useful checking input fit modelâ€™s context window. parameter : text ( str ) â€“ string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) â†’ int # get number token messages. useful checking input fit modelâ€™s context window. parameter : message ( list [ basemessage ] ) â€“ message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) â†’ list [ int ] # return ordered id token text. parameter : text ( str ) â€“ string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] abstract invoke ( input : input , config : runnableconfig | none = none ) â†’ output # transform single input output. override implement. parameter : input ( input ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use invoking runnable.
details. return : output runnable. return type : output abstract predict ( text : str , * , stop : sequence [ str ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. pas single string input model return string. use method passing raw text. want pas specific type chat messages, use predict_messages. parameter : text ( str ) â€“ string input pas model. stop ( sequence [ str ] | none ) â€“ stop word use generating. model output cut
model provider api call. return : top model prediction string. return type : str abstract predict_messages ( message : list [ basemessage ] , * , stop : sequence [ str ] | none = none , ** kwargs : ) â†’ basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. pas message sequence model return message. use method passing chat messages. want pas raw text, use predict. parameter : message ( list [ basemessage ] ) â€“ sequence chat message corresponding single model input. stop ( sequence [ str ] | none ) â€“ stop word use generating. model output cut
model provider api call. return : top model prediction message. return type : basemessage stream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) â†’ iterator [ output ] # default implementation stream, call invoke.
subclass override method support streaming output. parameter : input ( input ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use runnable. default none. kwargs ( | none ) â€“ additional keyword argument pas runnable. yield : output runnable. return type : iterator [ output ] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) â€“ kwargs ( ) â€“ return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ]
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.bedrock.bedrockllm.html#langchain_aws.llms.bedrock.bedrockllm
bedrockllm # class langchain_aws.llms.bedrock. bedrockllm [source] # bases: llm , bedrockbase bedrock models. authenticate, aws client us following method
access bedrock service. note bedrockllm implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param config : = none # optional botocore.config.config instance pas client. param credentials_profile_name : str | none = none # name profile ~/.aws/credentials ~/.aws/config files,
region specified ~/.aws/config case provided here. param streaming : bool = false # whether stream results. param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) â€“ prompt generate from. stop ( list [ str ] | none ) â€“ stop word use generating. model output cut
model provider api call. return : llmresult, contains list candidate generation input prompt additional model provider-specific output. return type : llmresult get_num_tokens ( text : str ) â†’ int [source] # get number token present text. useful checking input fit modelâ€™s context window. parameter : text ( str ) â€“ string input tokenize. return : integer number token text. return type : int get_num_tokens_from_messages ( message : list [ basemessage ] ) â†’ int # get number token messages. useful checking input fit modelâ€™s context window. parameter : message ( list [ basemessage ] ) â€“ message input tokenize. return : sum number token across messages. return type : int get_token_ids ( text : str ) â†’ list [ int ] [source] # return ordered id token text. parameter : text ( str ) â€“ string input tokenize. return : list id corresponding token text, order occur text. return type : list [int] invoke ( input : promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , config : runnableconfig | none = none , * , stop : list [ str ] | none = none , ** kwargs : ) â†’ str # transform single input output. override implement. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use invoking runnable.
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use runnable. default none. kwargs ( ) â€“ additional keyword argument pas runnable. stop ( list [ str ] | none ) â€“ yield : output runnable. return type : iterator [str] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) â€“ kwargs ( ) â€“ return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example using bedrockllm aws bedrock
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.bedrock.llminputoutputadapter.html#langchain_aws.llms.bedrock.llminputoutputadapter
llminputoutputadapter # class langchain_aws.llms.bedrock. llminputoutputadapter [source] # adapter class prepare input langchain format
llm model expects. also provides helper function extract
generated text model response. attribute provider_to_output_key_map method __init__ () aprepare_output_stream (provider, response[, ...]) prepare_input (provider, model_kwargs[, ...]) prepare_output (provider, response) prepare_output_stream (provider, response[, ...]) __init__ ( ) # classmethod aprepare_output_stream ( provider : str , response : , stop : list [ str ] | none = none , messages_api : bool = false , coerce_content_to_string : bool = false ) â†’ asynciterator [ generationchunk | aimessagechunk ] [source] # parameter : provider ( str ) â€“ response ( ) â€“ stop ( list [ str ] | none ) â€“ messages_api ( bool ) â€“ coerce_content_to_string ( bool ) â€“ return type : asynciterator [ generationchunk | aimessagechunk ] classmethod prepare_input ( provider : str , model_kwargs : dict [ str , ] , prompt : str | none = none , system : str | none = none , message : list [ dict ] | none = none , tool : list [ anthropictool ] | none = none ) â†’ dict [ str , ] [source] # parameter : provider ( str ) â€“ model_kwargs ( dict [ str , ] ) â€“ prompt ( str | none ) â€“ system ( str | none ) â€“ message ( list [ dict ] | none ) â€“ tool ( list [ anthropictool ] | none ) â€“ return type : dict [str, ] classmethod prepare_output ( provider : str , response : ) â†’ dict [source] # parameter : provider ( str ) â€“ response ( ) â€“ return type : dict classmethod prepare_output_stream ( provider : str , response : , stop : list [ str ] | none = none , messages_api : bool = false , coerce_content_to_string : bool = false ) â†’ iterator [ generationchunk | aimessagechunk ] [source] # parameter : provider ( str ) â€“ response ( ) â€“ stop ( list [ str ] | none ) â€“ messages_api ( bool ) â€“ coerce_content_to_string ( bool ) â€“ return type : iterator [ generationchunk | aimessagechunk ]
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.sagemaker_endpoint.contenthandlerbase.html#langchain_aws.llms.sagemaker_endpoint.contenthandlerbase
contenthandlerbase # class langchain_aws.llms.sagemaker_endpoint. contenthandlerbase [source] # handler class transform input llm
format sagemaker endpoint expects. similarly, class handle transforming output
sagemaker endpoint format llm class expects. attribute accepts mime type response data returned endpoint content_type mime type input data passed endpoint method __init__ () transform_input (prompt, model_kwargs) transforms input format model accept request body. transform_output (output) transforms output model string llm class expects. __init__ ( ) # abstract transform_input ( prompt : input_type , model_kwargs : dict ) â†’ byte [source] # transforms input format model accept
request body. return byte seekable file
like object format specified content_type
request header. parameter : prompt ( input_type ) â€“ model_kwargs ( dict ) â€“ return type : byte abstract transform_output ( output : byte ) â†’ output_type [source] # transforms output model string
llm class expects. parameter : output ( byte ) â€“ return type : output_type example using contenthandlerbase aws
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.sagemaker_endpoint.llmcontenthandler.html#langchain_aws.llms.sagemaker_endpoint.llmcontenthandler
llmcontenthandler # class langchain_aws.llms.sagemaker_endpoint. llmcontenthandler [source] # content handler llm class. attribute accepts mime type response data returned endpoint content_type mime type input data passed endpoint method __init__ () transform_input (prompt, model_kwargs) transforms input format model accept request body. transform_output (output) transforms output model string llm class expects. __init__ ( ) # abstract transform_input ( prompt : input_type , model_kwargs : dict ) â†’ byte # transforms input format model accept
request header. parameter : prompt ( input_type ) â€“ model_kwargs ( dict ) â€“ return type : byte abstract transform_output ( output : byte ) â†’ output_type # transforms output model string
llm class expects. parameter : output ( byte ) â€“ return type : output_type example using llmcontenthandler sagemakerendpoint
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.sagemaker_endpoint.lineiterator.html#langchain_aws.llms.sagemaker_endpoint.lineiterator
lineiterator # class langchain_aws.llms.sagemaker_endpoint. lineiterator ( stream : ) [source] # helper class parsing byte stream input. output model following format: bâ€™{â€œoutputsâ€: [â€ aâ€]} â€˜ bâ€™{â€œoutputsâ€: [â€ challengingâ€]} â€˜ bâ€™{â€œoutputsâ€: [â€ problemâ€]} â€˜ â€¦ usually payloadpart event event stream
contain byte array full json, guaranteed
json object may split acrosspayloadpart events. example: {â€˜payloadpartâ€™: {â€˜bytesâ€™: bâ€™{â€œoutputsâ€: â€˜}}
{â€˜payloadpartâ€™: {â€˜bytesâ€™: bâ€™[â€ problemâ€]} â€˜}} class account concatenating byte written via â€˜writeâ€™ function
exposing method return line (ending â€˜ â€˜ character) within buffer via â€˜scan_linesâ€™ function.
maintains position last read position ensure
previous byte exposed again. detail see: https://aws.amazon.com/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/ method __init__ (stream) parameter : stream ( ) â€“ __init__ ( stream : ) â†’ none [source] # parameter : stream ( ) â€“ return type : none
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.sagemaker_endpoint.sagemakerendpoint.html#langchain_aws.llms.sagemaker_endpoint.sagemakerendpoint
sagemakerendpoint # class langchain_aws.llms.sagemaker_endpoint. sagemakerendpoint [source] # bases: llm sagemaker inference endpoint models. use, must supply endpoint name deployed
sagemaker model & region deployed. authenticate, aws client us following method
access sagemaker endpoint.
see: https://docs.aws.amazon.com/iam/latest/userguide/access_policies.html note sagemakerendpoint implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param client : = none # boto3 client sagemaker runtime param content_handler : llmcontenthandler [required] # content handler class provides input
output transform function handle format llm
endpoint. param credentials_profile_name : str | none = none # name profile ~/.aws/credentials ~/.aws/config files,
see: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param endpoint_kwargs : dict | none = none # optional attribute passed invoke_endpoint
function. see `boto3`_ . doc info.
.. _boto3: param endpoint_name : str = '' # name endpoint deployed sagemaker model.
must unique within aws region. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_kwargs : dict | none = none # keyword argument pas model. param region_name : str = '' # aws region sagemaker model deployed, eg. us-west-2 . param streaming : bool = false # whether stream results. param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) â€“ prompt generate from. stop ( list [ str ] | none ) â€“ stop word use generating. model output cut
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use runnable. default none. kwargs ( ) â€“ additional keyword argument pas runnable. stop ( list [ str ] | none ) â€“ yield : output runnable. return type : iterator [str] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) â€“ kwargs ( ) â€“ return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example using sagemakerendpoint aws sagemakerendpoint
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.bedrock.extract_tool_calls.html#langchain_aws.llms.bedrock.extract_tool_calls
extract_tool_calls # langchain_aws.llms.bedrock. extract_tool_calls ( content : list [ dict ] ) â†’ list [ toolcall ] [source] # parameter : content ( list [ dict ] ) â€“ return type : list [ toolcall ]
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.sagemaker_endpoint.enforce_stop_tokens.html#langchain_aws.llms.sagemaker_endpoint.enforce_stop_tokens
enforce_stop_tokens # langchain_aws.llms.sagemaker_endpoint. enforce_stop_tokens ( text : str , stop : list [ str ] ) â†’ str [source] # cut text soon stop word occur. parameter : text ( str ) â€“ stop ( list [ str ] ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/llms/langchain_aws.llms.bedrock.bedrock.html#langchain_aws.llms.bedrock.bedrock
bedrock # class langchain_aws.llms.bedrock. bedrock [source] # bases: bedrockllm deprecated since version 0.1.0: use bedrockllm instead. note bedrock implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param config : = none # optional botocore.config.config instance pas client. param credentials_profile_name : str | none = none # name profile ~/.aws/credentials ~/.aws/config files,
subclass override method support streaming output. parameter : input ( promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use runnable. default none. kwargs ( ) â€“ additional keyword argument pas runnable. stop ( list [ str ] | none ) â€“ yield : output runnable. return type : iterator [str] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type [ basemodel ] , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] # implemented class. parameter : schema ( dict | type [ basemodel ] ) â€“ kwargs ( ) â€“ return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example using bedrock bedrock (knowledge bases) retriever
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers.html#langchain-aws-retrievers
retriever # class retrievers.bedrock.amazonknowledgebasesretriever amazon bedrock knowledge base retrieval. retrievers.bedrock.retrievalconfig configuration retrieval. retrievers.bedrock.searchfilter filter configuration retrieval. retrievers.bedrock.vectorsearchconfig configuration vector search. retrievers.kendra.additionalresultattribute additional result attribute. retrievers.kendra.additionalresultattributevalue value additional result attribute. retrievers.kendra.amazonkendraretriever amazon kendra index retriever. retrievers.kendra.documentattribute document attribute. retrievers.kendra.documentattributevalue value document attribute. retrievers.kendra.highlight information highlight keywords excerpt. retrievers.kendra.queryresult amazon kendra query api search result. retrievers.kendra.queryresultitem query api result item. retrievers.kendra.resultitem base class result item. retrievers.kendra.retrieveresult amazon kendra retrieve api search result. retrievers.kendra.retrieveresultitem retrieve api result item. retrievers.kendra.textwithhighlights text highlights. function retrievers.kendra.clean_excerpt (excerpt) clean excerpt kendra. retrievers.kendra.combined_text (item) combine resultitem title excerpt single string.
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.amazonknowledgebasesretriever.html#langchain_aws.retrievers.bedrock.amazonknowledgebasesretriever
amazonknowledgebasesretriever # class langchain_aws.retrievers.bedrock. amazonknowledgebasesretriever [source] # bases: baseretriever amazon bedrock knowledge base retrieval. see https://aws.amazon.com/bedrock/knowledge-bases info. args: knowledge_base_id: knowledge base id.
region_name: aws region e.g., us-west-2 . fallback aws_default_region env variable region specified
~/.aws/config. credentials_profile_name: name profile ~/.aws/credentials ~/.aws/config files, either access key role information
specified. specified, default credential profile or,
ec2 instance, credential imds used. client: boto3 client bedrock agent runtime.
retrieval_config: configuration retrieval. example: langchain_community.retrievers import amazonknowledgebasesretriever retriever = amazonknowledgebasesretriever( knowledge_base_id=â€â€,
retrieval_config={ â€œvectorsearchconfigurationâ€: { â€œnumberofresultsâ€: 4 } }, ) note amazonknowledgebasesretriever implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param client : = none # param credentials_profile_name : str | none = none # param endpoint_url : str | none = none # param knowledge_base_id : str [required] # param metadata : dict [ str , ] | none = none # optional metadata associated retriever. default none.
metadata associated call retriever,
passed argument handler defined callback .
use eg identify specific instance retriever
use case. param min_score_confidence : float | none = none # constraint : minimum = 0.0 maximum = 1.0 param region_name : str | none = none # param retrieval_config : retrievalconfig [required] # param tag : list [ str ] | none = none # optional list tag associated retriever. default none.
tag associated call retriever,
use case. async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
default false. kwargs ( | none ) â€“ additional keyword argument pas runnable. yield : tuple index input output runnable. return type : asynciterator [ tuple [int, output | exception]] async aget_relevant_documents ( query : str , * , callback : callback = none , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , run_name : str | none = none , ** kwargs : ) â†’ list [ document ] # deprecated since version langchain-core==0.1.46: use ainvoke instead. asynchronously get document relevant query. user favor using .ainvoke .abatch rather aget_relevant_documents directly . parameter : query ( str ) â€“ string find relevant document for. callback ( callback ) â€“ callback manager list callbacks. tag ( optional [ list [ str ] ] ) â€“ optional list tag associated retriever.
default none. metadata ( optional [ dict [ str , ] ] ) â€“ optional metadata associated retriever.
default none. run_name ( optional [ str ] ) â€“ optional name run. default none. kwargs ( ) â€“ additional argument pas retriever. return : list relevant documents. return type : list[ document ] async ainvoke ( input : str , config : runnableconfig | none = none , ** kwargs : ) â†’ list [ document ] # asynchronously invoke retriever get relevant documents. main entry point asynchronous retriever invocations. parameter : input ( str ) â€“ query string. config ( runnableconfig | none ) â€“ configuration retriever. default none. kwargs ( ) â€“ additional argument pas retriever. return : list relevant documents. return type : list [ document ] examples: await retriever . ainvoke ( "query" ) async astream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) â†’ asynciterator [ output ] # default implementation astream, call ainvoke.
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) â†’ runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) â€“ dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) get_relevant_documents ( query : str , * , callback : callback = none , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , run_name : str | none = none , ** kwargs : ) â†’ list [ document ] # deprecated since version langchain-core==0.1.46: use invoke instead. retrieve document relevant query. user favor using .invoke .batch rather get_relevant_documents directly . parameter : query ( str ) â€“ string find relevant document for. callback ( callback ) â€“ callback manager list callbacks. default none. tag ( optional [ list [ str ] ] ) â€“ optional list tag associated retriever.
default none. run_name ( optional [ str ] ) â€“ optional name run. default none. kwargs ( ) â€“ additional argument pas retriever. return : list relevant documents. return type : list[ document ] invoke ( input : str , config : runnableconfig | none = none , ** kwargs : ) â†’ list [ document ] # invoke retriever get relevant documents. main entry point synchronous retriever invocations. parameter : input ( str ) â€“ query string. config ( runnableconfig | none ) â€“ configuration retriever. default none. kwargs ( ) â€“ additional argument pas retriever. return : list relevant documents. return type : list [ document ] examples: retriever . invoke ( "query" ) stream ( input : input , config : runnableconfig | none = none , ** kwargs : | none ) â†’ iterator [ output ] # default implementation stream, call invoke.
subclass override method support streaming output. parameter : input ( input ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use runnable. default none. kwargs ( | none ) â€“ additional keyword argument pas runnable. yield : output runnable. return type : iterator [ output ] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented example using amazonknowledgebasesretriever aws bedrock (knowledge bases) retriever
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.retrievalconfig.html#langchain_aws.retrievers.bedrock.retrievalconfig
retrievalconfig # class langchain_aws.retrievers.bedrock. retrievalconfig [source] # bases: basemodel configuration retrieval. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param nexttoken : str | none = none # param vectorsearchconfiguration : vectorsearchconfig [required] #
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.searchfilter.html#langchain_aws.retrievers.bedrock.searchfilter
searchfilter # class langchain_aws.retrievers.bedrock. searchfilter [source] # bases: basemodel filter configuration retrieval. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param andall : list [ searchfilter ] | none = none # param equal : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param greaterthan : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param greaterthanorequals : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param in_ : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none (alias 'in') # param lessthan : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param lessthanorequals : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param listcontains : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param notequals : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param notin : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param orall : list [ searchfilter ] | none = none # param startswith : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none # param stringcontains : dict [ str , dict [ str , ] | list [ ] | int | float | str | bool | none ] | none = none #
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.vectorsearchconfig.html#langchain_aws.retrievers.bedrock.vectorsearchconfig
vectorsearchconfig # class langchain_aws.retrievers.bedrock. vectorsearchconfig [source] # bases: basemodel configuration vector search. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param filter : searchfilter | none = none # param numberofresults : int = 4 # param overridesearchtype : literal [ 'hybrid' , 'semantic' ] | none = none #
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.additionalresultattribute.html#langchain_aws.retrievers.kendra.additionalresultattribute
additionalresultattribute # class langchain_aws.retrievers.kendra. additionalresultattribute [source] # bases: basemodel additional result attribute. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param key : str [required] # key attribute. param value : additionalresultattributevalue [required] # value attribute. param valuetype : literal [ 'text_with_highlights_value' ] [required] # type value. get_value_text ( ) â†’ str [source] # return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.additionalresultattributevalue.html#langchain_aws.retrievers.kendra.additionalresultattributevalue
additionalresultattributevalue # class langchain_aws.retrievers.kendra. additionalresultattributevalue [source] # bases: basemodel value additional result attribute. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param textwithhighlightsvalue : textwithhighlights [required] # text highlight value.
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.amazonkendraretriever.html#langchain_aws.retrievers.kendra.amazonkendraretriever
amazonkendraretriever # class langchain_aws.retrievers.kendra. amazonkendraretriever [source] # bases: baseretriever amazon kendra index retriever. parameter : index_id â€“ kendra index id region_name â€“ aws region e.g., us-west-2 .
fallsback aws_default_region env variable
region specified ~/.aws/config. credentials_profile_name â€“ name profile ~/.aws/credentials
~/.aws/config files, either access key role information
ec2 instance, credential imds used. top_k â€“ result return attribute_filter â€“ additional filtering result based metadata
see: https://docs.aws.amazon.com/kendra/latest/apireference page_content_formatter â€“ generates document page_content
allowing access result item attributes. default, us
itemâ€™s title excerpt. client â€“ boto3 client kendra user_context â€“ provides information user context
see: https://docs.aws.amazon.com/kendra/latest/apireference example retriever = amazonkendraretriever ( index_id = "c0806df7-e76b-4bce-9b5c-d5582f6b1a03" ) note amazonkendraretriever implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param attribute_filter : dict | none = none # param client : = none # param credentials_profile_name : str | none = none # param index_id : str [required] # param metadata : dict [ str , ] | none = none # optional metadata associated retriever. default none.
use case. param min_score_confidence : float | none = none # constraint : minimum = 0.0 maximum = 1.0 param page_content_formatter : callable [ [ resultitem ] , str ] = # param region_name : str | none = none # param tag : list [ str ] | none = none # optional list tag associated retriever. default none.
use case. param top_k : int = 3 # param user_context : dict | none = none # async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
subclass override method support streaming output. parameter : input ( input ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use runnable. default none. kwargs ( | none ) â€“ additional keyword argument pas runnable. yield : output runnable. return type : iterator [ output ] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented example using amazonkendraretriever aws amazon kendra
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.documentattribute.html#langchain_aws.retrievers.kendra.documentattribute
documentattribute # class langchain_aws.retrievers.kendra. documentattribute [source] # bases: basemodel document attribute. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param key : str [required] # key attribute. param value : documentattributevalue [required] # value attribute.
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.documentattributevalue.html#langchain_aws.retrievers.kendra.documentattributevalue
documentattributevalue # class langchain_aws.retrievers.kendra. documentattributevalue [source] # bases: basemodel value document attribute. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param datevalue : str | none = none # date expressed iso 8601 string. param longvalue : int | none = none # long value. param stringlistvalue : list [ str ] | none = none # string list value. param stringvalue : str | none = none # string value. property value : str | int | list [ str ] | none # defined document attribute value none.
according amazon kendra, provide one
value document attribute.
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.highlight.html#langchain_aws.retrievers.kendra.highlight
highlight # class langchain_aws.retrievers.kendra. highlight [source] # bases: basemodel information highlight keywords excerpt. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param beginoffset : int [required] # zero-based location excerpt highlight starts. param endoffset : int [required] # zero-based location excerpt highlight ends. param topanswer : bool | none = none # indicates whether result best one. param type : str | none = none # highlight type: standard thesaurus_synonym.
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.queryresult.html#langchain_aws.retrievers.kendra.queryresult
queryresult # class langchain_aws.retrievers.kendra. queryresult [source] # bases: basemodel amazon kendra query api search result. composed of: relevant suggested answers: either text excerpt table excerpt. matching faq questions-answer faq file. document including excerpt document title. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param resultitems : list [ queryresultitem ] [required] # result items.
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.queryresultitem.html#langchain_aws.retrievers.kendra.queryresultitem
queryresultitem # class langchain_aws.retrievers.kendra. queryresultitem [source] # bases: resultitem query api result item. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param additionalattributes : list [ additionalresultattribute ] | none = [] # one additional attribute associated result. param documentattributes : list [ documentattribute ] | none = [] # document attributes. param documentexcerpt : textwithhighlights | none = none # excerpt document text. param documentid : str | none = none # document id. param documenttitle : textwithhighlights [required] # document title. param documenturi : str | none = none # document uri. param feedbacktoken : str | none = none # identifies particular result particular query. param format : str | none = none # type answer, format either: table: table excerpt returned tableexcerpt; text: text excerpt returned documentexcerpt. param id : str | none = none # id relevant result item. param scoreattributes : dict | none = none # kendra score confidence param type : str | none = none # type result: document question_answer answer get_additional_metadata ( ) â†’ dict [source] # document additional metadata dict.
return extra metadata except these: result_id document_id source title excerpt document_attributes return type : dict get_attribute_value ( ) â†’ str [source] # return type : str get_document_attributes_dict ( ) â†’ dict [ str , str | int | list [ str ] | none ] # document attribute dict. return type : dict [str, str | int | list [str] | none] get_excerpt ( ) â†’ str [source] # document excerpt passage original content retrieved kendra. return type : str get_score_attribute ( ) â†’ str # document score confidence return type : str get_title ( ) â†’ str [source] # document title. return type : str to_doc ( page_content_formatter: ~typing.callable[[~langchain_aws.retrievers.kendra.resultitem], str] = ) â†’ document # convert item document. parameter : page_content_formatter ( callable [ [ resultitem ] , str ] ) â€“ return type : document
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.resultitem.html#langchain_aws.retrievers.kendra.resultitem
resultitem # class langchain_aws.retrievers.kendra. resultitem [source] # bases: basemodel , abc base class result item. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param documentattributes : list [ documentattribute ] | none = [] # document attributes. param documentid : str | none = none # document id. param documenturi : str | none = none # document uri. param id : str | none = none # id relevant result item. param scoreattributes : dict | none = none # kendra score confidence get_additional_metadata ( ) â†’ dict [source] # document additional metadata dict.
return extra metadata except these: result_id document_id source title excerpt document_attributes return type : dict get_document_attributes_dict ( ) â†’ dict [ str , str | int | list [ str ] | none ] [source] # document attribute dict. return type : dict [str, str | int | list [str] | none] abstract get_excerpt ( ) â†’ str [source] # document excerpt passage original content retrieved kendra. return type : str get_score_attribute ( ) â†’ str [source] # document score confidence return type : str abstract get_title ( ) â†’ str [source] # document title. return type : str to_doc ( page_content_formatter: ~typing.callable[[~langchain_aws.retrievers.kendra.resultitem], str] = ) â†’ document [source] # convert item document. parameter : page_content_formatter ( callable [ [ resultitem ] , str ] ) â€“ return type : document
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.retrieveresult.html#langchain_aws.retrievers.kendra.retrieveresult
retrieveresult # class langchain_aws.retrievers.kendra. retrieveresult [source] # bases: basemodel amazon kendra retrieve api search result. composed of: relevant passage text excerpt given input query. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param queryid : str [required] # id query. param resultitems : list [ retrieveresultitem ] [required] # result items.
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.retrieveresultitem.html#langchain_aws.retrievers.kendra.retrieveresultitem
retrieveresultitem # class langchain_aws.retrievers.kendra. retrieveresultitem [source] # bases: resultitem retrieve api result item. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param content : str | none = none # content item. param documentattributes : list [ documentattribute ] | none = [] # document attributes. param documentid : str | none = none # document id. param documenttitle : str | none = none # document title. param documenturi : str | none = none # document uri. param id : str | none = none # id relevant result item. param scoreattributes : dict | none = none # kendra score confidence get_additional_metadata ( ) â†’ dict # document additional metadata dict.
return extra metadata except these: result_id document_id source title excerpt document_attributes return type : dict get_document_attributes_dict ( ) â†’ dict [ str , str | int | list [ str ] | none ] # document attribute dict. return type : dict [str, str | int | list [str] | none] get_excerpt ( ) â†’ str [source] # document excerpt passage original content retrieved kendra. return type : str get_score_attribute ( ) â†’ str # document score confidence return type : str get_title ( ) â†’ str [source] # document title. return type : str to_doc ( page_content_formatter: ~typing.callable[[~langchain_aws.retrievers.kendra.resultitem], str] = ) â†’ document # convert item document. parameter : page_content_formatter ( callable [ [ resultitem ] , str ] ) â€“ return type : document
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.textwithhighlights.html#langchain_aws.retrievers.kendra.textwithhighlights
textwithhighlights # class langchain_aws.retrievers.kendra. textwithhighlights [source] # bases: basemodel text highlights. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param highlight : | none = none # highlights. param text : str [required] # text.
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.clean_excerpt.html#langchain_aws.retrievers.kendra.clean_excerpt
clean_excerpt # langchain_aws.retrievers.kendra. clean_excerpt ( excerpt : str ) â†’ str [source] # clean excerpt kendra. parameter : excerpt ( str ) â€“ excerpt clean. return : cleaned excerpt. return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/retrievers/langchain_aws.retrievers.kendra.combined_text.html#langchain_aws.retrievers.kendra.combined_text
combined_text # langchain_aws.retrievers.kendra. combined_text ( item : resultitem ) â†’ str [source] # combine resultitem title excerpt single string. parameter : item ( resultitem ) â€“ resultitem kendra search. return : combined text title excerpt given item. return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/utilities.html#langchain-aws-utilities
utility # class utilities.redis.tokenescaper ([escape_chars_re]) escape punctuation within input string. utilities.utils.distancestrategy (value[, ...]) enumerator distance strategy calculating distance vectors. function utilities.math.cosine_similarity (x, y) row-wise cosine similarity two equal-width matrices. utilities.math.cosine_similarity_top_k (x, y) row-wise cosine similarity optional top-k score threshold filtering. utilities.redis.get_client (redis_url, **kwargs) get redis client connection url given. utilities.utils.filter_complex_metadata (...) filter metadata type supported vector store. utilities.utils.maximal_marginal_relevance (...) calculate maximal marginal relevance.
page: https://python.langchain.com/v0.2/api_reference/aws/utilities/langchain_aws.utilities.redis.tokenescaper.html#langchain_aws.utilities.redis.tokenescaper
tokenescaper # class langchain_aws.utilities.redis. tokenescaper ( escape_chars_re : pattern | none = none ) [source] # escape punctuation within input string. attribute default_escaped_chars method __init__ ([escape_chars_re]) escape (value) parameter : escape_chars_re ( optional [ pattern ] ) â€“ __init__ ( escape_chars_re : pattern | none = none ) [source] # parameter : escape_chars_re ( pattern | none ) â€“ escape ( value : str ) â†’ str [source] # parameter : value ( str ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/utilities/langchain_aws.utilities.utils.distancestrategy.html#langchain_aws.utilities.utils.distancestrategy
distancestrategy # class langchain_aws.utilities.utils. distancestrategy ( value , name = none , * , module = none , qualname = none , type = none , start = 1 , boundary = none ) [source] # enumerator distance strategy calculating distance
vectors. euclidean_distance = 'euclidean_distance' # max_inner_product = 'max_inner_product' # dot_product = 'dot_product' # jaccard = 'jaccard' # cosine = 'cosine' # example using distancestrategy kinetica vectorstore api oracle ai vector search: vector store sap hana cloud vector engine semadb singlestoredb
page: https://python.langchain.com/v0.2/api_reference/aws/utilities/langchain_aws.utilities.math.cosine_similarity.html#langchain_aws.utilities.math.cosine_similarity
cosine_similarity # langchain_aws.utilities.math. cosine_similarity ( x : list [ list [ float ] ] | list [ ndarray ] | ndarray , : list [ list [ float ] ] | list [ ndarray ] | ndarray ) â†’ ndarray [source] # row-wise cosine similarity two equal-width matrices. parameter : x ( list [ list [ float ] ] | list [ ndarray ] | ndarray ) â€“ ( list [ list [ float ] ] | list [ ndarray ] | ndarray ) â€“ return type : ndarray example using cosine_similarity route sub-chains
page: https://python.langchain.com/v0.2/api_reference/aws/utilities/langchain_aws.utilities.math.cosine_similarity_top_k.html#langchain_aws.utilities.math.cosine_similarity_top_k
cosine_similarity_top_k # langchain_aws.utilities.math. cosine_similarity_top_k ( x : list [ list [ float ] ] | list [ ndarray ] | ndarray , : list [ list [ float ] ] | list [ ndarray ] | ndarray , top_k : int | none = 5 , score_threshold : float | none = none ) â†’ tuple [ list [ tuple [ int , int ] ] , list [ float ] ] [source] # row-wise cosine similarity optional top-k score threshold filtering. parameter : x ( list [ list [ float ] ] | list [ ndarray ] | ndarray ) â€“ matrix. ( list [ list [ float ] ] | list [ ndarray ] | ndarray ) â€“ matrix, width x. top_k ( int | none ) â€“ max number result return. score_threshold ( float | none ) â€“ minimum cosine similarity results. return : tuple two lists. first contains two-tuples index (x_idx, y_idx), second contains corresponding cosine similarities. return type : tuple [ list [ tuple [int, int]], list [float]]
page: https://python.langchain.com/v0.2/api_reference/aws/utilities/langchain_aws.utilities.redis.get_client.html#langchain_aws.utilities.redis.get_client
get_client # langchain_aws.utilities.redis. get_client ( redis_url : str , ** kwargs : ) â†’ redistype [source] # get redis client connection url given. helper accepts
url redis server (tcp with/without tl unixsocket) well
redis sentinel connections. creating connection existence database driver checked
valueerror raised otherwise. use, redis python package installed. example langchain_community.utilities.redis import get_client redis_client = get_client ( redis_url = "redis://username:password@localhost:6379" index_name = "my-index" , embedding_function = embeddings . embed_query , ) parameter : redis_url ( str ) â€“ kwargs ( ) â€“ return type : redistype
page: https://python.langchain.com/v0.2/api_reference/aws/utilities/langchain_aws.utilities.utils.filter_complex_metadata.html#langchain_aws.utilities.utils.filter_complex_metadata
filter_complex_metadata # langchain_aws.utilities.utils. filter_complex_metadata ( documents: ~typing.list[~langchain_core.documents.base.document], *, allowed_types: ~typing.tuple[~typing.type, ...] = (, , , ) ) â†’ list [ document ] [source] # filter metadata type supported vector store. parameter : document ( list [ document ] ) â€“ allowed_types ( tuple [ type , ... ] ) â€“ return type : list [ document ]
page: https://python.langchain.com/v0.2/api_reference/aws/utilities/langchain_aws.utilities.utils.maximal_marginal_relevance.html#langchain_aws.utilities.utils.maximal_marginal_relevance
maximal_marginal_relevance # langchain_aws.utilities.utils. maximal_marginal_relevance ( query_embedding : ndarray , embedding_list : list , lambda_mult : float = 0.5 , k : int = 4 ) â†’ list [ int ] [source] # calculate maximal marginal relevance. parameter : query_embedding ( ndarray ) â€“ embedding_list ( list ) â€“ lambda_mult ( float ) â€“ k ( int ) â€“ return type : list [int]
page: https://python.langchain.com/v0.2/api_reference/aws/utils.html#langchain-aws-utils
utils # function utils.enforce_stop_tokens (text, stop) cut text soon stop word occur. utils.get_num_tokens_anthropic (text) get number token string text. utils.get_token_ids_anthropic (text) get token id string text.
page: https://python.langchain.com/v0.2/api_reference/aws/utils/langchain_aws.utils.enforce_stop_tokens.html#langchain_aws.utils.enforce_stop_tokens
enforce_stop_tokens # langchain_aws.utils. enforce_stop_tokens ( text : str , stop : list [ str ] ) â†’ str [source] # cut text soon stop word occur. parameter : text ( str ) â€“ stop ( list [ str ] ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/utils/langchain_aws.utils.get_num_tokens_anthropic.html#langchain_aws.utils.get_num_tokens_anthropic
get_num_tokens_anthropic # langchain_aws.utils. get_num_tokens_anthropic ( text : str ) â†’ int [source] # get number token string text. parameter : text ( str ) â€“ return type : int
page: https://python.langchain.com/v0.2/api_reference/aws/utils/langchain_aws.utils.get_token_ids_anthropic.html#langchain_aws.utils.get_token_ids_anthropic
get_token_ids_anthropic # langchain_aws.utils. get_token_ids_anthropic ( text : str ) â†’ list [ int ] [source] # get token id string text. parameter : text ( str ) â€“ return type : list [int]
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores.html#langchain-aws-vectorstores
vectorstores # class vectorstores.inmemorydb.base.inmemoryvectorstore (...) inmemoryvectorstore vector database. vectorstores.inmemorydb.base.inmemoryvectorstoreretriever retriever inmemoryvectorstore. vectorstores.inmemorydb.filters.inmemorydbfilter () collection inmemorydbfilterfields. vectorstores.inmemorydb.filters.inmemorydbfilterexpression ([...]) logical expression inmemorydbfilterfields. vectorstores.inmemorydb.filters.inmemorydbfilterfield (field) base class inmemorydbfilterfields. vectorstores.inmemorydb.filters.inmemorydbfilteroperator (value) inmemorydbfilteroperator enumerator used create inmemorydbfilterexpressions vectorstores.inmemorydb.filters.inmemorydbnum (field) inmemorydbfilterfield representing numeric field inmemorydb index. vectorstores.inmemorydb.filters.inmemorydbtag (field) inmemorydbfilterfield representing tag inmemorydb index. vectorstores.inmemorydb.filters.inmemorydbtext (field) inmemorydbfilterfield representing text field inmemorydb index. vectorstores.inmemorydb.schema.flatvectorfield schema flat vector field redis. vectorstores.inmemorydb.schema.hnswvectorfield schema hnsw vector field redis. vectorstores.inmemorydb.schema.inmemorydbdistancemetric (value) distance metric redis vector fields. vectorstores.inmemorydb.schema.inmemorydbfield base class redis fields. vectorstores.inmemorydb.schema.inmemorydbmodel schema memorydb index. vectorstores.inmemorydb.schema.inmemorydbvectorfield base class redis vector fields. vectorstores.inmemorydb.schema.numericfieldschema schema numeric field redis. vectorstores.inmemorydb.schema.tagfieldschema schema tag field redis. vectorstores.inmemorydb.schema.textfieldschema schema text field redis. function vectorstores.inmemorydb.base.check_index_exists (...) check memorydb index exists. vectorstores.inmemorydb.filters.check_operator_misuse (func) decorator check misuse equality operators. vectorstores.inmemorydb.schema.read_schema (...) read index schema dict yaml file.
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.base.inmemoryvectorstore.html#langchain_aws.vectorstores.inmemorydb.base.inmemoryvectorstore
inmemoryvectorstore # class langchain_aws.vectorstores.inmemorydb.base. inmemoryvectorstore ( redis_url : str , index_name : str , embedding : embeddings , index_schema : dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none = none , vector_schema : dict [ str , str | int ] | none = none , relevance_score_fn : callable [ [ float ] , float ] | none = none , key_prefix : str | none = none , ** kwargs : ) [source] # inmemoryvectorstore vector database. use, redis python package installed
aws memorydb running, connect memorydb server following url schemas:
- redis://: # simple connection
- redis://:@: # connection authentication
- rediss://: # connection ssl
- rediss://:@: # connection ssl auth examples: following example show various way use redis vectorstore
langchain. following example assume following imports: langchain_aws.vectorstores import inmemoryvectorstore initialize, create index, load document langchain_aws.vectorstores import inmemoryvectorstore rds = inmemoryvectorstore . from_documents ( document , # list document object loader created embeddings , # embeddings object redis_url = "redis://cluster_endpoint:6379" , ) initialize, create index, load document metadata rds = inmemoryvectorstore . from_texts ( text , # list string metadata , # list metadata dicts embeddings , # embeddings object redis_url = "redis://cluster_endpoint:6379" , ) initialize, create index, load document metadata return key rds , key = inmemoryvectorstore . from_texts_return_keys ( text , # list string metadata , # list metadata dicts embeddings , # embeddings object redis_url = "redis://cluster_endpoint:6379" , ) use case index need stay alive, initialize
index name itâ€™s easier reference later rds = inmemoryvectorstore . from_texts ( text , # list string metadata , # list metadata dicts embeddings , # embeddings object index_name = "my-index" , redis_url = "redis://cluster_endpoint:6379" , ) initialize connect existing index (from above) # must pas schema key_prefix another index existing_rds = inmemoryvectorstore . from_existing_index ( embeddings , # embeddings object index_name = "my-index" , schema = rds . schema , # schema dumped another index key_prefix = rds . key_prefix , # key prefix another index redis_url = "redis://cluster_endpoint:6379" , ) advanced examples: custom vector schema supplied change way
memorydb creates underlying vector schema. useful
production use case want optimize
vector schema use case. ex. using hnsw instead
flat (knn) default vector_schema = { "algorithm" : "hnsw" } rds = inmemoryvectorstore . from_texts ( text , # list string metadata , # list metadata dicts embeddings , # embeddings object vector_schema = vector_schema , redis_url = "redis://cluster_endpoint:6379" , ) custom index schema supplied change way
metadata indexed. useful would like use
hybrid querying (filtering) capability memorydb. default, implementation automatically generate index
schema according following rules: string indexed text field number indexed numeric field list string indexed tag field (joined langchain_aws.vectorstores.inmemorydb.constants.inmemorydb_tag_separator) none value indexed still stored memorydb retrievable interface here, raw memorydb client
used retrieve them. type indexed override rules, pas custom index schema like following tag : - name : credit_score text : - name : user - name : job typically, credit_score field would text field since itâ€™s string,
however, override behavior specifying field type shown
yaml config (can also dictionary) code below. rds = inmemoryvectorstore . from_texts ( text , # list string metadata , # list metadata dicts embeddings , # embeddings object index_schema = "path/to/index_schema.yaml" , # also dictionary redis_url = "redis://cluster_endpoint:6379" , ) connecting existing index custom schema applied, itâ€™s
important pas schema from_existing_index method.
otherwise, schema newly added sample incorrect metadata
returned. initialize memorydb vector store necessary components. attribute default_vector_schema embeddings access query embedding object available. schema return schema index. method __init__ (redis_url, index_name, embedding[, ...]) initialize memorydb vector store necessary components. aadd_documents (documents, **kwargs) async run document embeddings add vectorstore. aadd_texts (texts[, metadatas]) async run text embeddings add vectorstore. add_documents (documents, **kwargs) add update document vectorstore. add_texts (texts[, metadatas, embeddings, ...]) add text vectorstore. adelete ([ids]) async delete vector id criteria. afrom_documents (documents, embedding, **kwargs) async return vectorstore initialized document embeddings. afrom_texts (texts, embedding[, metadatas]) async return vectorstore initialized text embeddings. aget_by_ids (ids, /) async get document ids. amax_marginal_relevance_search (query[, k, ...]) async return doc selected using maximal marginal relevance. amax_marginal_relevance_search_by_vector (...) async return doc selected using maximal marginal relevance. as_retriever (**kwargs) return vectorstoreretriever initialized vectorstore. asearch (query, search_type, **kwargs) async return doc similar query using specified search type. asimilarity_search (query[, k]) async return doc similar query. asimilarity_search_by_vector (embedding[, k]) async return doc similar embedding vector. asimilarity_search_with_relevance_scores (query) async return doc relevance score range [0, 1]. asimilarity_search_with_score (*args, **kwargs) async run similarity search distance. delete ([ids]) delete inmemoryvectorstore entry. drop_index (index_name, delete_documents, ...) drop inmemoryvectorstore search index. from_documents (documents, embedding, **kwargs) return vectorstore initialized document embeddings. from_existing_index (embedding, index_name, ...) connect existing inmemoryvectorstore index. from_texts (texts, embedding[, metadatas, ...]) create inmemoryvectorstore vectorstore list texts. from_texts_return_keys (texts, embedding[, ...]) create inmemoryvectorstore vectorstore raw documents. get_by_ids (ids, /) get document ids. max_marginal_relevance_search (query[, k, ...]) return doc selected using maximal marginal relevance. max_marginal_relevance_search_by_vector (...) return doc selected using maximal marginal relevance. search (query, search_type, **kwargs) return doc similar query using specified search type. similarity_search (query[, k, filter, ...]) run similarity search similarity_search_by_vector (embedding[, k, ...]) run similarity search query vector indexed vectors. similarity_search_limit_score (query[, k, ...]) deprecated since version langchain-aws==0.0.1: use similarity_search(distance_threshold=0.1) instead. similarity_search_with_relevance_scores (query) return doc relevance score range [0, 1]. similarity_search_with_score (query[, k, ...]) run similarity search vector distance . write_schema (path) write schema yaml file. parameter : redis_url ( str ) â€“ index_name ( str ) â€“ embedding ( embeddings ) â€“ index_schema ( optional [ union [ dict [ str , listofdict ] , str , os.pathlike ] ] ) â€“ vector_schema ( optional [ dict [ str , union [ str , int ] ] ] ) â€“ relevance_score_fn ( optional [ callable [ [ float ] , float ] ] ) â€“ key_prefix ( optional [ str ] ) â€“ kwargs ( ) â€“ __init__ ( redis_url : str , index_name : str , embedding : embeddings , index_schema : dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none = none , vector_schema : dict [ str , str | int ] | none = none , relevance_score_fn : callable [ [ float ] , float ] | none = none , key_prefix : str | none = none , ** kwargs : ) [source] # initialize memorydb vector store necessary components. parameter : redis_url ( str ) â€“ index_name ( str ) â€“ embedding ( embeddings ) â€“ index_schema ( dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none ) â€“ vector_schema ( dict [ str , str | int ] | none ) â€“ relevance_score_fn ( callable [ [ float ] , float ] | none ) â€“ key_prefix ( str | none ) â€“ kwargs ( ) â€“ async aadd_documents ( document : list [ document ] , ** kwargs : ) â†’ list [ str ] # async run document embeddings add
id kwargs receive precedence. return : list id added texts. raise : valueerror â€“ number id match number documents. return type : list[str] add_texts ( text : iterable [ str ] , metadata : list [ dict ] | none = none , embeddings : list [ list [ float ] ] | none = none , batch_size : int = 1000 , clean_metadata : bool = true , ** kwargs : ) â†’ list [ str ] [source] # add text vectorstore. parameter : text ( iterable [ str ] ) â€“ iterable strings/text add vectorstore. metadata ( optional [ list [ dict ] ] , optional ) â€“ optional list metadatas.
default none. embeddings ( optional [ list [ list [ float ] ] ] , optional ) â€“ optional pre-generated
embeddings. default none. key ( list [ str ] ) id ( list [ str ] ) â€“ identifier entries.
default none. batch_size ( int , optional ) â€“ batch size use writes. default 1000. clean_metadata ( bool ) â€“ kwargs ( ) â€“ return : list id added vectorstore return type : list[str] async adelete ( id : list [ str ] | none = none , ** kwargs : ) â†’ bool | none # async delete vector id criteria. parameter : id ( list [ str ] | none ) â€“ list id delete. none, delete all. default none. **kwargs ( ) â€“ keyword argument subclass might use. return : true deletion successful,
default 0.5. **kwargs ( ) â€“ argument pas search method. return : list document selected maximal marginal relevance. return type : list[ document ] as_retriever ( ** kwargs : ) â†’ inmemoryvectorstoreretriever [source] # return vectorstoreretriever initialized vectorstore. parameter : **kwargs ( ) â€“ keyword argument pas search function.
score_threshold: optional, floating point value 0 1 filter resulting set retrieved doc return : list tuples (doc, similarity_score) return type : list[tuple[ document , float]] async asimilarity_search_with_score ( * args : , ** kwargs : ) â†’ list [ tuple [ document , float ] ] # async run similarity search distance. parameter : *args ( ) â€“ argument pas search method. **kwargs ( ) â€“ argument pas search method. return : list tuples (doc, similarity_score). return type : list[tuple[ document , float]] static delete ( id : list [ str ] | none = none , ** kwargs : ) â†’ bool [source] # delete inmemoryvectorstore entry. parameter : id ( list [ str ] | none ) â€“ list id (keys redis) delete. redis_url â€“ redis connection url. passed kwargs
set environment variable: redis_url. kwargs ( ) â€“ return : whether deletion successful. return type : bool raise : valueerror â€“ redis python package installed. valueerror â€“ id (keys redis) provided static drop_index ( index_name : str , delete_documents : bool , ** kwargs : ) â†’ bool [source] # drop inmemoryvectorstore search index. parameter : index_name ( str ) â€“ name index drop. delete_documents ( bool ) â€“ whether drop associated documents. kwargs ( ) â€“ return : whether drop successful. return type : bool classmethod from_documents ( document : list [ document ] , embedding : embeddings , ** kwargs : ) â†’ vst # return vectorstore initialized document embeddings. parameter : document ( list [ document ] ) â€“ list document add vectorstore. embedding ( embeddings ) â€“ embedding function use. kwargs ( ) â€“ additional keyword arguments. return : vectorstore initialized document embeddings. return type : vectorstore classmethod from_existing_index ( embedding : embeddings , index_name : str , schema : dict [ str , list [ dict [ str , str ] ] ] | str | pathlike , key_prefix : str | none = none , ** kwargs : ) â†’ inmemoryvectorstore [source] # connect existing inmemoryvectorstore index. example langchain_aws.vectorstores import inmemoryvectorstore embeddings = openaiembeddings () # must pas schema key_prefix another index existing_rds = inmemoryvectorstore . from_existing_index ( embeddings , index_name = "my-index" , schema = rds . schema , # schema dumped another index key_prefix = rds . key_prefix , # key prefix another index redis_url = "redis://username:password@cluster_endpoint:6379" , ) parameter : embedding ( embeddings ) â€“ embedding model class (i.e. openaiembeddings)
embedding queries. index_name ( str ) â€“ name index connect to. schema ( union [ dict [ str , str ] , str , os.pathlike , dict [ str , listofdict ] ] ) â€“ schema index vector schema. dict, path
yaml file. key_prefix ( optional [ str ] ) â€“ prefix use key associated ( inmemoryvectorstore ) â€“ index. **kwargs ( ) â€“ additional keyword argument pas redis client. return : inmemoryvectorstore vectorstore instance. return type : inmemoryvectorstore raise : valueerror â€“ index exist. importerror â€“ redis python package installed. classmethod from_texts ( text : list [ str ] , embedding : embeddings , metadata : list [ dict ] | none = none , index_name : str | none = none , index_schema : dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none = none , vector_schema : dict [ str , str | int ] | none = none , ** kwargs : ) â†’ inmemoryvectorstore [source] # create inmemoryvectorstore vectorstore list texts. user-friendly interface that: embeds documents. creates new inmemoryvectorstore index doesnâ€™t already exist add document newly created inmemoryvectorstore index. method generate schema based metadata passed
index_schema defined. index_schema defined,
compare generated schema warn
differences. purposefully defining schema
metadata, ignore warning. examine schema options, initialize instance class
print schema using inmemoryvectorstore.schema` property.
include content content_vector class
always present langchain schema. example langchain_aws.vectorstores import inmemoryvectorstore embeddings = openaiembeddings () parameter : text ( list [ str ] ) â€“ list text add vectorstore. embedding ( embeddings ) â€“ embedding model class (i.e. openaiembeddings)
embedding queries. metadata ( optional [ list [ dict ] ] , optional ) â€“ optional list metadata dicts
add vectorstore. default none. index_name ( optional [ str ] , optional ) â€“ optional name index create
add to. default none. ( optional [ union [ dict [ str ( index_schema ) â€“ optional):
optional field index within metadata. override generated
schema. default none. listofdict ] â€“ optional):
schema. default none. str â€“ optional):
schema. default none. os.pathlike ] ] â€“ optional):
schema. default none. index_schema ( dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none ) â€“ vector_schema ( dict [ str , str | int ] | none ) â€“ kwargs ( ) â€“ return type : inmemoryvectorstore :param optional): optional field index within metadata. override generated
schema. default none. parameter : vector_schema ( optional [ dict [ str , union [ str , int ] ] ] , optional ) â€“ optional
vector schema use. default none. **kwargs ( ) â€“ additional keyword argument pas client. ( inmemoryvectorstore ) â€“ text ( list [ str ] ) â€“ embedding ( embeddings ) â€“ metadata ( list [ dict ] | none ) â€“ index_name ( str | none ) â€“ index_schema ( dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none ) â€“ return : inmemoryvectorstore vectorstore instance. return type : inmemoryvectorstore raise : valueerror â€“ number metadata match number texts. importerror â€“ redis python package installed. classmethod from_texts_return_keys ( text : list [ str ] , embedding : embeddings , metadata : list [ dict ] | none = none , index_name : str | none = none , index_schema : dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none = none , vector_schema : dict [ str , str | int ] | none = none , ** kwargs : ) â†’ tuple [ inmemoryvectorstore , list [ str ] ] [source] # create inmemoryvectorstore vectorstore raw documents. user-friendly interface that: embeds documents. creates new inmemoryvectorstore index doesnâ€™t already exist add document newly created inmemoryvectorstore index. return key newly created document stored. method generate schema based metadata passed
always present langchain schema. example langchain_aws.vectorstores import inmemoryvectorstore embeddings = openaiembeddings () redis , key = inmemoryvectorstore . from_texts_return_keys ( text , embeddings , redis_url = "redis://cluster_endpoint:6379" ) parameter : text ( list [ str ] ) â€“ list text add vectorstore. embedding ( embeddings ) â€“ embeddings use vectorstore. metadata ( optional [ list [ dict ] ] , optional ) â€“ optional list metadata
dicts add vectorstore. default none. index_name ( optional [ str ] , optional ) â€“ optional name index
create add to. default none. ( optional [ union [ dict [ str ( index_schema ) â€“ optional):
schema. default none. index_schema ( dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none ) â€“ vector_schema ( dict [ str , str | int ] | none ) â€“ kwargs ( ) â€“ return type : tuple [ inmemoryvectorstore , list [str]] :param optional): optional field index within metadata. override generated
vector schema use. default none. **kwargs ( ) â€“ additional keyword argument pas redis client. returns: tuple [ inmemoryvectorstore â€“ list [ str ] ] â€“ ( tuple inmemoryvectorstore instance key ) â€“ documents. ( newly created ) â€“ text ( list [ str ] ) â€“ embedding ( embeddings ) â€“ metadata ( list [ dict ] | none ) â€“ index_name ( str | none ) â€“ index_schema ( dict [ str , list [ dict [ str , str ] ] ] | str | pathlike | none ) â€“ raise : valueerror â€“ number metadata match number texts. return type : tuple [ inmemoryvectorstore , list [str]] get_by_ids ( id : sequence [ str ] , / ) â†’ list [ document ] # get document ids. returned document expected id field set id
ids. parameter : id ( sequence [ str ] ) â€“ list id retrieve. return : list documents. return type : list[ document ] new version 0.2.11. max_marginal_relevance_search ( query : str , k : int = 4 , fetch_k : int = 20 , lambda_mult : float = 0.5 , filter : inmemorydbfilterexpression | none = none , return_metadata : bool = true , distance_threshold : float | none = none , ** kwargs : ) â†’ list [ document ] [source] # return doc selected using maximal marginal relevance. maximal marginal relevance optimizes similarity query diversity among selected documents. parameter : query ( str ) â€“ text look document similar to. k ( int ) â€“ number document return. default 4. fetch_k ( int ) â€“ number document fetch pas mmr algorithm. lambda_mult ( float ) â€“ number 0 1 determines degree
default 0.5. filter ( inmemorydbfilterexpression , optional ) â€“ optional metadata filter.
default none. return_metadata ( bool , optional ) â€“ whether return metadata.
default true. distance_threshold ( optional [ float ] , optional ) â€“ maximum vector distance
selected document query vector. default none. kwargs ( ) â€“ return : list document selected maximal marginal relevance. return type : list[ document ] max_marginal_relevance_search_by_vector ( embedding : list [ float ] , k : int = 4 , fetch_k : int = 20 , lambda_mult : float = 0.5 , ** kwargs : ) â†’ list [ document ] # return doc selected using maximal marginal relevance. maximal marginal relevance optimizes similarity query diversity
â€œmmrâ€, â€œsimilarity_score_thresholdâ€. return type : list[ document ] similarity_search ( query : str , k : int = 4 , filter : inmemorydbfilterexpression | none = none , return_metadata : bool = true , distance_threshold : float | none = none , ** kwargs : ) â†’ list [ document ] [source] # run similarity search parameter : query ( str ) â€“ query text find similar documents. k ( int ) â€“ number document return. default 4. filter ( inmemorydbfilterexpression , optional ) â€“ optional metadata filter.
selected document query vector. default none. kwargs ( ) â€“ return : list document similar query text. return type : list[ document ] similarity_search_by_vector ( embedding : list [ float ] , k : int = 4 , filter : inmemorydbfilterexpression | none = none , return_metadata : bool = true , distance_threshold : float | none = none , ** kwargs : ) â†’ list [ document ] [source] # run similarity search query vector indexed vectors. parameter : embedding ( list [ float ] ) â€“ query vector find similar
documents. k ( int ) â€“ number document return. default 4. filter ( inmemorydbfilterexpression , optional ) â€“ optional metadata filter.
selected document query vector. default none. kwargs ( ) â€“ return : list document similar query text. return type : list[ document ] similarity_search_limit_score ( query : str , k : int = 4 , score_threshold : float = 0.2 , ** kwargs : ) â†’ list [ document ] [source] # deprecated since version langchain-aws==0.0.1: use similarity_search(distance_threshold=0.1) instead. return similar indexed document query text within
score_threshold range. deprecated: use similarity_search distance_threshold instead. parameter : query ( str ) â€“ query text find similar documents. k ( int ) â€“ number document return. default 4. score_threshold ( float ) â€“ minimum matching distance required
document considered match. default 0.2. kwargs ( ) â€“ return : list document similar query text including match score document. return type : list[ document ] note document satisfy score_threshold value,
empty list returned. similarity_search_with_relevance_scores ( query : str , k : int = 4 , ** kwargs : ) â†’ list [ tuple [ document , float ] ] # return doc relevance score range [0, 1]. 0 dissimilar, 1 similar. parameter : query ( str ) â€“ input text. k ( int ) â€“ number document return. default 4. **kwargs ( ) â€“ kwargs passed similarity search. include:
score_threshold: optional, floating point value 0 1 filter resulting set retrieved docs. return : list tuples (doc, similarity_score). return type : list[tuple[ document , float]] similarity_search_with_score ( query : str , k : int = 4 , filter : inmemorydbfilterexpression | none = none , return_metadata : bool = true , ** kwargs : ) â†’ list [ tuple [ document , float ] ] [source] # run similarity search vector distance . â€œscoresâ€ returned function raw vector
distance query vector. similarity scores, use similarity_search_with_relevance_scores . parameter : query ( str ) â€“ query text find similar documents. k ( int ) â€“ number document return. default 4. filter ( inmemorydbfilterexpression , optional ) â€“ optional metadata filter.
default true. kwargs ( ) â€“ return : list document similar query distance document. return type : list[tuple[ document , float]] write_schema ( path : str | pathlike ) â†’ none [source] # write schema yaml file. parameter : path ( str | pathlike ) â€“ return type : none example using inmemoryvectorstore ai21embeddings aws amazon memorydb azureopenaiembeddings cohereembeddings fireworksembeddings convert runnables tool mistralaiembeddings nomicembeddings ollamaembeddings openaiembeddings togetherembeddings
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.base.inmemoryvectorstoreretriever.html#langchain_aws.vectorstores.inmemorydb.base.inmemoryvectorstoreretriever
inmemoryvectorstoreretriever # class langchain_aws.vectorstores.inmemorydb.base. inmemoryvectorstoreretriever [source] # bases: vectorstoreretriever retriever inmemoryvectorstore. note inmemoryvectorstoreretriever implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param metadata : dict [ str , ] | none = none # optional metadata associated retriever. default none.
use case. param search_kwargs : dict [ str , ] = {'distance_threshold': none, 'k': 4, 'score_threshold': 0.9} # default search kwargs. param search_type : str = 'similarity' # type search perform. either
â€˜similarityâ€™,
â€˜similarity_distance_thresholdâ€™,
â€˜similarity_score_thresholdâ€™ param tag : list [ str ] | none = none # optional list tag associated retriever. default none.
use case. param vectorstore : inmemoryvectorstore [required] # inmemoryvectorstore. async aadd_documents ( document : list [ document ] , ** kwargs : ) â†’ list [ str ] [source] # add document vectorstore. parameter : document ( list [ document ] ) â€“ kwargs ( ) â€“ return type : list [str] async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
default false. kwargs ( | none ) â€“ additional keyword argument pas runnable. yield : tuple index input output runnable. return type : asynciterator [ tuple [int, output | exception]] add_documents ( document : list [ document ] , ** kwargs : ) â†’ list [ str ] [source] # add document vectorstore. parameter : document ( list [ document ] ) â€“ kwargs ( ) â€“ return type : list [str] async aget_relevant_documents ( query : str , * , callback : callback = none , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , run_name : str | none = none , ** kwargs : ) â†’ list [ document ] # deprecated since version langchain-core==0.1.46: use ainvoke instead. asynchronously get document relevant query. user favor using .ainvoke .abatch rather aget_relevant_documents directly . parameter : query ( str ) â€“ string find relevant document for. callback ( callback ) â€“ callback manager list callbacks. tag ( optional [ list [ str ] ] ) â€“ optional list tag associated retriever.
subclass override method support streaming output. parameter : input ( input ) â€“ input runnable. config ( runnableconfig | none ) â€“ config use runnable. default none. kwargs ( | none ) â€“ additional keyword argument pas runnable. yield : output runnable. return type : iterator [ output ] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented allowed_search_types : classvar [ collection [ str ] ] = ['similarity', 'similarity_distance_threshold', 'similarity_score_threshold', 'mmr'] # allowed search types.
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.filters.inmemorydbfilter.html#langchain_aws.vectorstores.inmemorydb.filters.inmemorydbfilter
inmemorydbfilter # class langchain_aws.vectorstores.inmemorydb.filters. inmemorydbfilter [source] # collection inmemorydbfilterfields. method __init__ () num (field) tag (field) text (field) __init__ ( ) # static num ( field : str ) â†’ inmemorydbnum [source] # parameter : field ( str ) â€“ return type : inmemorydbnum static tag ( field : str ) â†’ inmemorydbtag [source] # parameter : field ( str ) â€“ return type : inmemorydbtag static text ( field : str ) â†’ inmemorydbtext [source] # parameter : field ( str ) â€“ return type : inmemorydbtext
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.filters.inmemorydbfilterexpression.html#langchain_aws.vectorstores.inmemorydb.filters.inmemorydbfilterexpression
inmemorydbfilterexpression # class langchain_aws.vectorstores.inmemorydb.filters. inmemorydbfilterexpression ( _filter : str | none = none , operator : inmemorydbfilteroperator | none = none , left : inmemorydbfilterexpression | none = none , right : inmemorydbfilterexpression | none = none ) [source] # logical expression inmemorydbfilterfields. inmemorydbfilterexpressions combined using & | operator create
complex logical expression evaluate inmemorydb query language. present interface user create complex query
without know inmemorydb query language. filter expression initialized directly. instead built
combining inmemorydbfilterfields using & | operators. examples:
>>> langchain_aws.vectorstores.inmemorydb import (
â€¦ inmemorydbtag, inmemorydbnum
â€¦ )
>>> brand_is_nike = inmemorydbtag(â€œbrandâ€) == â€œnikeâ€
>>> price_is_under_100 = inmemorydbnum(â€œpriceâ€) < 100
>>> filter = brand_is_nike & price_is_under_100
>>> print(str(filter))
(@brand:{nike} @price:[-inf (100)]) method __init__ ([_filter, operator, left, right]) format_expression (left, right, operator_str) parameter : _filter ( str | none ) â€“ operator ( inmemorydbfilteroperator | none ) â€“ left ( inmemorydbfilterexpression | none ) â€“ right ( inmemorydbfilterexpression | none ) â€“ __init__ ( _filter : str | none = none , operator : inmemorydbfilteroperator | none = none , left : inmemorydbfilterexpression | none = none , right : inmemorydbfilterexpression | none = none ) [source] # parameter : _filter ( str | none ) â€“ operator ( inmemorydbfilteroperator | none ) â€“ left ( inmemorydbfilterexpression | none ) â€“ right ( inmemorydbfilterexpression | none ) â€“ static format_expression ( left : inmemorydbfilterexpression , right : inmemorydbfilterexpression , operator_str : str ) â†’ str [source] # parameter : left ( inmemorydbfilterexpression ) â€“ right ( inmemorydbfilterexpression ) â€“ operator_str ( str ) â€“ return type : str
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.filters.inmemorydbfilterfield.html#langchain_aws.vectorstores.inmemorydb.filters.inmemorydbfilterfield
inmemorydbfilterfield # class langchain_aws.vectorstores.inmemorydb.filters. inmemorydbfilterfield ( field : str ) [source] # base class inmemorydbfilterfields. attribute operator escaper method __init__ (field) equal (other) parameter : field ( str ) â€“ __init__ ( field : str ) [source] # parameter : field ( str ) â€“ equal ( : inmemorydbfilterfield ) â†’ bool [source] # parameter : ( inmemorydbfilterfield ) â€“ return type : bool
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.filters.inmemorydbfilteroperator.html#langchain_aws.vectorstores.inmemorydb.filters.inmemorydbfilteroperator
inmemorydbfilteroperator # class langchain_aws.vectorstores.inmemorydb.filters. inmemorydbfilteroperator ( value , name = none , * , module = none , qualname = none , type = none , start = 1 , boundary = none ) [source] # inmemorydbfilteroperator enumerator used create
inmemorydbfilterexpressions eq = 1 # ne = 2 # lt = 3 # gt = 4 # le = 5 # ge = 6 # = 7 # = 8 # like = 9 # = 10 #
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.filters.inmemorydbnum.html#langchain_aws.vectorstores.inmemorydb.filters.inmemorydbnum
inmemorydbnum # class langchain_aws.vectorstores.inmemorydb.filters. inmemorydbnum ( field : str ) [source] # inmemorydbfilterfield representing numeric field inmemorydb index. attribute operator operator_map supported_val_types escaper method __init__ (field) equal (other) parameter : field ( str ) â€“ __init__ ( field : str ) # parameter : field ( str ) â€“ equal ( : inmemorydbfilterfield ) â†’ bool # parameter : ( inmemorydbfilterfield ) â€“ return type : bool
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.filters.inmemorydbtag.html#langchain_aws.vectorstores.inmemorydb.filters.inmemorydbtag
inmemorydbtag # class langchain_aws.vectorstores.inmemorydb.filters. inmemorydbtag ( field : str ) [source] # inmemorydbfilterfield representing tag inmemorydb index. create inmemorydbtag filterfield. parameter : field ( str ) â€“ name inmemorydbtag field index queried
against. attribute operator operator_map supported_val_types escaper method __init__ (field) create inmemorydbtag filterfield. equal (other) __init__ ( field : str ) [source] # create inmemorydbtag filterfield. parameter : field ( str ) â€“ name inmemorydbtag field index queried
against. equal ( : inmemorydbfilterfield ) â†’ bool # parameter : ( inmemorydbfilterfield ) â€“ return type : bool
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.filters.inmemorydbtext.html#langchain_aws.vectorstores.inmemorydb.filters.inmemorydbtext
inmemorydbtext # class langchain_aws.vectorstores.inmemorydb.filters. inmemorydbtext ( field : str ) [source] # inmemorydbfilterfield representing text field inmemorydb index. attribute operator operator_map supported_val_types escaper method __init__ (field) equal (other) parameter : field ( str ) â€“ __init__ ( field : str ) # parameter : field ( str ) â€“ equal ( : inmemorydbfilterfield ) â†’ bool # parameter : ( inmemorydbfilterfield ) â€“ return type : bool
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.flatvectorfield.html#langchain_aws.vectorstores.inmemorydb.schema.flatvectorfield
flatvectorfield # class langchain_aws.vectorstores.inmemorydb.schema. flatvectorfield [source] # bases: inmemorydbvectorfield schema flat vector field redis. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param algorithm : literal [ 'flat' ] = 'flat' # param block_size : int | none = none # param datatype : str = 'float32' # param dims : int [required] # param distance_metric : inmemorydbdistancemetric = 'cosine' # param initial_cap : int | none = none # param name : str [required] # as_field ( ) â†’ vectorfield [source] # return type : vectorfield
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.hnswvectorfield.html#langchain_aws.vectorstores.inmemorydb.schema.hnswvectorfield
hnswvectorfield # class langchain_aws.vectorstores.inmemorydb.schema. hnswvectorfield [source] # bases: inmemorydbvectorfield schema hnsw vector field redis. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param algorithm : literal [ 'hnsw' ] = 'hnsw' # param datatype : str = 'float32' # param dims : int [required] # param distance_metric : inmemorydbdistancemetric = 'cosine' # param ef_construction : int = 200 # param ef_runtime : int = 10 # param epsilon : float = 0.01 # param initial_cap : int | none = none # param : int = 16 # param name : str [required] # as_field ( ) â†’ vectorfield [source] # return type : vectorfield
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.inmemorydbdistancemetric.html#langchain_aws.vectorstores.inmemorydb.schema.inmemorydbdistancemetric
inmemorydbdistancemetric # class langchain_aws.vectorstores.inmemorydb.schema. inmemorydbdistancemetric ( value , name = none , * , module = none , qualname = none , type = none , start = 1 , boundary = none ) [source] # distance metric redis vector fields. l2 = 'l2' # cosine = 'cosine' # ip = 'ip' #
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.inmemorydbfield.html#langchain_aws.vectorstores.inmemorydb.schema.inmemorydbfield
inmemorydbfield # class langchain_aws.vectorstores.inmemorydb.schema. inmemorydbfield [source] # bases: basemodel base class redis fields. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param name : str [required] #
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.inmemorydbmodel.html#langchain_aws.vectorstores.inmemorydb.schema.inmemorydbmodel
inmemorydbmodel # class langchain_aws.vectorstores.inmemorydb.schema. inmemorydbmodel [source] # bases: basemodel schema memorydb index. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param content_key : str = 'content' # param content_vector_key : str = 'content_vector' # param extra : list [ inmemorydbfield ] | none = none # param numeric : list [ numericfieldschema ] | none = none # param tag : list [ tagfieldschema ] | none = none # param text : list [ textfieldschema ] = [textfieldschema(name='content', weight=1, no_stem=false, phonetic_matcher=none, withsuffixtrie=false, no_index=false, sortable=false)] # param vector : list [ flatvectorfield | hnswvectorfield ] | none = none # add_content_field ( ) â†’ none [source] # return type : none add_vector_field ( vector_field : dict [ str , ] ) â†’ none [source] # parameter : vector_field ( dict [ str , ] ) â€“ return type : none as_dict ( ) â†’ dict [ str , list [ ] ] [source] # return type : dict [str, list [ ]] get_fields ( ) â†’ list [ inmemorydbfield ] [source] # return type : list [ inmemorydbfield ] property content_vector : flatvectorfield | hnswvectorfield # property is_empty : bool # property metadata_keys : list [ str ] # property vector_dtype : dtype #
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.inmemorydbvectorfield.html#langchain_aws.vectorstores.inmemorydb.schema.inmemorydbvectorfield
inmemorydbvectorfield # class langchain_aws.vectorstores.inmemorydb.schema. inmemorydbvectorfield [source] # bases: inmemorydbfield base class redis vector fields. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param algorithm : object [required] # param datatype : str = 'float32' # param dims : int [required] # param distance_metric : inmemorydbdistancemetric = 'cosine' # param initial_cap : int | none = none # param name : str [required] #
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.numericfieldschema.html#langchain_aws.vectorstores.inmemorydb.schema.numericfieldschema
numericfieldschema # class langchain_aws.vectorstores.inmemorydb.schema. numericfieldschema [source] # bases: inmemorydbfield schema numeric field redis. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param name : str [required] # param no_index : bool = false # param sortable : bool | none = false # as_field ( ) â†’ numericfield [source] # return type : numericfield
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.tagfieldschema.html#langchain_aws.vectorstores.inmemorydb.schema.tagfieldschema
tagfieldschema # class langchain_aws.vectorstores.inmemorydb.schema. tagfieldschema [source] # bases: inmemorydbfield schema tag field redis. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param case_sensitive : bool = false # param name : str [required] # param no_index : bool = false # param separator : str = ',' # param sortable : bool | none = false # as_field ( ) â†’ tagfield [source] # return type : tagfield
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.textfieldschema.html#langchain_aws.vectorstores.inmemorydb.schema.textfieldschema
textfieldschema # class langchain_aws.vectorstores.inmemorydb.schema. textfieldschema [source] # bases: inmemorydbfield schema text field redis. create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param name : str [required] # param no_index : bool = false # param no_stem : bool = false # param phonetic_matcher : str | none = none # param sortable : bool | none = false # param weight : float = 1 # param withsuffixtrie : bool = false # as_field ( ) â†’ textfield [source] # return type : textfield
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.base.check_index_exists.html#langchain_aws.vectorstores.inmemorydb.base.check_index_exists
check_index_exists # langchain_aws.vectorstores.inmemorydb.base. check_index_exists ( client : inmemorydbtype , index_name : str ) â†’ bool [source] # check memorydb index exists. parameter : client ( inmemorydbtype ) â€“ index_name ( str ) â€“ return type : bool
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.filters.check_operator_misuse.html#langchain_aws.vectorstores.inmemorydb.filters.check_operator_misuse
check_operator_misuse # langchain_aws.vectorstores.inmemorydb.filters. check_operator_misuse ( func : callable ) â†’ callable [source] # decorator check misuse equality operators. parameter : func ( callable ) â€“ return type : callable
page: https://python.langchain.com/v0.2/api_reference/aws/vectorstores/langchain_aws.vectorstores.inmemorydb.schema.read_schema.html#langchain_aws.vectorstores.inmemorydb.schema.read_schema
read_schema # langchain_aws.vectorstores.inmemorydb.schema. read_schema ( index_schema : dict [ str , list [ ] ] | str | pathlike | none ) â†’ dict [ str , ] [source] # read index schema dict yaml file. check dict return redismodel otherwise, check itâ€™s path
read file assuming itâ€™s yaml file return redismodel parameter : index_schema ( dict [ str , list [ ] ] | str | pathlike | none ) â€“ return type : dict [str, ]
page: https://python.langchain.com/v0.2/api_reference/mistralai/index.html
langchain-mistralai: 0.1.13 # chat_models # class chat_models.chatmistralai chat model us mistralai api. function chat_models.acompletion_with_retry (llm[, ...]) use tenacity retry async completion call. embeddings # class embeddings.dummytokenizer () dummy tokenizer tokenizer cannot accessed (e.g., via huggingface) embeddings.mistralaiembeddings mistralai embedding model integration.
page: https://python.langchain.com/v0.2/api_reference/mistralai/chat_models.html#langchain-mistralai-chat-models
chat_models # class chat_models.chatmistralai chat model us mistralai api. function chat_models.acompletion_with_retry (llm[, ...]) use tenacity retry async completion call.
page: https://python.langchain.com/v0.2/api_reference/mistralai/chat_models/langchain_mistralai.chat_models.chatmistralai.html#langchain_mistralai.chat_models.chatmistralai
chatmistralai # class langchain_mistralai.chat_models. chatmistralai [source] # bases: basechatmodel chat model us mistralai api. note chatmistralai implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. â€œtool_callingâ€, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param endpoint : str | none = none (alias 'base_url') # param max_concurrent_requests : int = 64 # param max_retries : int = 5 # param max_tokens : int | none = none # param metadata : dict [ str , ] | none = none # metadata add run trace. param mistral_api_key : secretstr | none [optional] (alias 'api_key') # constraint : type = string writeonly = true format = password param model : str = 'mistral-small' (alias 'model_name') # param random_seed : int | none = none # param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param safe_mode : bool = false # param streaming : bool = false # param tag : list [ str ] | none = none # tag add run trace. param temperature : float = 0.7 # param timeout : int = 120 # param top_p : float = 1 # decode using nucleus sampling: consider smallest set token whose
yielding result complete. parameter : input ( sequence [ input ] ) â€“ config ( runnableconfig | sequence [ runnableconfig ] | none ) â€“ return_exceptions ( bool ) â€“ kwargs ( | none ) â€“ return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model. assumes model compatible openai tool-calling api. parameter : tool ( sequence [ dict [ str , ] | type | callable | basetool ] ) â€“ list tool definition bind chat model.
subclass override method support streaming output. parameter : input ( languagemodelinput ) â€“ input runnable. config ( optional [ runnableconfig ] ) â€“ config use runnable. default none. kwargs ( ) â€“ additional keyword argument pas runnable. stop ( optional [ list [ str ] ] ) â€“ yield : output runnable. return type : iterator[ basemessagechunk ] to_json ( ) â†’ serializedconstructor | serializednotimplemented # serialize runnable json. return : json-serializable representation runnable. return type : serializedconstructor | serializednotimplemented with_structured_output ( schema : dict | type | none = none , * , method : literal [ 'function_calling' , 'json_mode' ] = 'function_calling' , include_raw : bool = false , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , dict | basemodel ] [source] # model wrapper return output formatted match given schema. args: schema: output schema. passed as: openai function/tool schema, json schema, typeddict class (support added 0.1.12), pydantic class. schema pydantic class model output
# } parameter : schema ( dict | type | none ) â€“ method ( literal [ 'function_calling' , 'json_mode' ] ) â€“ include_raw ( bool ) â€“ kwargs ( ) â€“ return type : runnable [ promptvalue | str | sequence [ basemessage | list [str] | tuple [str, str] | str | dict [str, ]], dict | basemodel ] example using chatmistralai build extraction chain chatmistralai mistralai response metadata
page: https://python.langchain.com/v0.2/api_reference/mistralai/chat_models/langchain_mistralai.chat_models.acompletion_with_retry.html#langchain_mistralai.chat_models.acompletion_with_retry
acompletion_with_retry # async langchain_mistralai.chat_models. acompletion_with_retry ( llm : _basevertexmaasmodelgarden , run_manager : asynccallbackmanagerforllmrun | none = none , ** kwargs : ) â†’ [source] # use tenacity retry async completion call. parameter : llm ( _basevertexmaasmodelgarden ) â€“ run_manager ( asynccallbackmanagerforllmrun | none ) â€“ kwargs ( ) â€“ return type :
page: https://python.langchain.com/v0.2/api_reference/mistralai/embeddings.html#langchain-mistralai-embeddings
embeddings # class embeddings.dummytokenizer () dummy tokenizer tokenizer cannot accessed (e.g., via huggingface) embeddings.mistralaiembeddings mistralai embedding model integration.
page: https://python.langchain.com/v0.2/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.dummytokenizer.html#langchain_mistralai.embeddings.dummytokenizer
dummytokenizer # class langchain_mistralai.embeddings. dummytokenizer [source] # dummy tokenizer tokenizer cannot accessed (e.g., via huggingface) method __init__ () encode_batch (texts) __init__ ( ) # encode_batch ( text : list [ str ] ) â†’ list [ list [ str ] ] [source] # parameter : text ( list [ str ] ) â€“ return type : list [ list [str]]
page: https://python.langchain.com/v0.2/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.mistralaiembeddings.html#langchain_mistralai.embeddings.mistralaiembeddings
mistralaiembeddings # class langchain_mistralai.embeddings. mistralaiembeddings [source] # bases: basemodel , embeddings mistralai embedding model integration. setup: install langchain_mistralai set environment variable mistral_api_key . pip install -u langchain_mistralai export mistral_api_key = "your-api-key" key init args â€” completion params: model: str name mistralai model use. key init args â€” client params: api_key: optional[secretstr] api key mistralai api. provided, read
environment variable mistral_api_key . max_retries: int number time retry request fails. timeout: int number second wait response timing out. max_concurrent_requests: int maximum number concurrent request make mistral api. see full list supported init args description params section. instantiate: __module_name__ import mistralaiembeddings embed = mistralaiembeddings ( model = "mistral-embed" , # api_key="...", # params... ) embed single text: input_text = "the meaning life 42" vector = embed . embed_query ( input_text ) print ( vector [: 3 ]) [ - 0.024603435769677162 , - 0.007543657906353474 , 0.0039630369283258915 ] embed multiple text: input_texts = [ "document 1..." , "document 2..." ] vector = embed . embed_documents ( input_texts ) print ( len ( vector )) # first 3 coordinate first vector print ( vector [ 0 ][: 3 ]) 2 [ - 0.024603435769677162 , - 0.007543657906353474 , 0.0039630369283258915 ] async: vector = await embed . aembed_query ( input_text ) print ( vector [: 3 ]) # multiple: # await embed.aembed_documents(input_texts) [ - 0.009100092574954033 , 0.005071679595857859 , - 0.0029193938244134188 ] create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param endpoint : str = 'https://api.mistral.ai/v1/' # param max_concurrent_requests : int = 64 # param max_retries : int = 5 # param mistral_api_key : secretstr [optional] (alias 'api_key') # constraint : type = string writeonly = true format = password param model : str = 'mistral-embed' # param timeout : int = 120 # param tokenizer : tokenizer = none # async aembed_documents ( text : list [ str ] ) â†’ list [ list [ float ] ] [source] # embed list document texts. parameter : text ( list [ str ] ) â€“ list text embed. return : list embeddings, one text. return type : list [ list [float]] async aembed_query ( text : str ) â†’ list [ float ] [source] # embed single query text. parameter : text ( str ) â€“ text embed. return : embedding text. return type : list [float] embed_documents ( text : list [ str ] ) â†’ list [ list [ float ] ] [source] # embed list document texts. parameter : text ( list [ str ] ) â€“ list text embed. return : list embeddings, one text. return type : list [ list [float]] embed_query ( text : str ) â†’ list [ float ] [source] # embed single query text. parameter : text ( str ) â€“ text embed. return : embedding text. return type : list [float] example using mistralaiembeddings mistralai mistralaiembeddings
page: https://python.langchain.com/v0.2/api_reference/huggingface/index.html
langchain-huggingface: 0.0.3 # chat_models # class chat_models.huggingface.chathuggingface hugging face llm's chatmodels. chat_models.huggingface.tgi_message (role, ...) message send textgeninference api. chat_models.huggingface.tgi_response (...) response textgeninference api. embeddings # class embeddings.huggingface.huggingfaceembeddings huggingface sentence_transformers embedding models. embeddings.huggingface_endpoint.huggingfaceendpointembeddings huggingfacehub embedding models. llm # class llms.huggingface_endpoint.huggingfaceendpoint huggingface endpoint. llms.huggingface_pipeline.huggingfacepipeline huggingface pipeline api.
page: https://python.langchain.com/v0.2/api_reference/huggingface/chat_models.html#langchain-huggingface-chat-models
chat_models # class chat_models.huggingface.chathuggingface hugging face llm's chatmodels. chat_models.huggingface.tgi_message (role, ...) message send textgeninference api. chat_models.huggingface.tgi_response (...) response textgeninference api.
page: https://python.langchain.com/v0.2/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.chathuggingface.html#langchain_huggingface.chat_models.huggingface.chathuggingface
chathuggingface # class langchain_huggingface.chat_models.huggingface. chathuggingface [source] # bases: basechatmodel hugging face llmâ€™s chatmodels. work huggingfacetextgeninference , huggingfaceendpoint , huggingfacehub , huggingfacepipeline llms. upon instantiating class, model_id resolved url
provided llm, appropriate tokenizer loaded
huggingface hub. setup: install langchain-huggingface ensure hugging face token
saved. pip install langchain-huggingface huggingface_hub import login login () # prompted hf key, saved locally key init args â€” completion params: llm: huggingfacetextgeninference , huggingfaceendpoint , huggingfacehub , â€˜huggingfacepipelineâ€™ llm used. key init args â€” client params: custom_get_token_ids: optional[callable[[str], list[int]]] optional encoder use counting tokens. metadata: optional[dict[str, any]] metadata add run trace. tags: optional[list[str]] tag add run trace. tokenizer:
verbose: bool whether print response text. see full list supported init args description params
section. instantiate: langchain_huggingface import huggingfaceendpoint , chathuggingface llm = huggingfaceendpoint ( repo_id = "microsoft/phi-3-mini-4k-instruct" , task = "text-generation" , max_new_tokens = 512 , do_sample = false , repetition_penalty = 1.03 , ) chat = chathuggingface ( llm = llm , verbose = true ) invoke: message = [ ( "system" , "you helpful translator. translate user sentence french . "), ( "human" , "i love programming." ), ] chat ( ... ) . invoke ( message ) aimessage ( content = 'je ai une passion pour le programme. french, use â€œaiâ€ masculine subject â€œaâ€ feminine
subjects. since â€œprogrammingâ€ gender-neutral english,
go masculine â€œprogrammeâ€. confirmation: â€œjâ€™aime le programme.â€ commonly used. sentence
technically accurate, less commonly used spoken french
â€œaiâ€ used less frequently everyday speech.â€™,
response_metadata={â€˜token_usageâ€™: chatcompletionoutputusage
(completion_tokens=100, prompt_tokens=55, total_tokens=155),
â€˜modelâ€™: â€˜â€™, â€˜finish_reasonâ€™: â€˜lengthâ€™},
id=â€™run-874c24b7-0272-4c99-b259-5d6d7facbc56-0â€™) stream: chunk chat . stream ( message ): print ( chunk ) content = 'je ai une passion pour le programme. french, use â€œaiâ€ masculine subject â€œaâ€ feminine subjects.
since â€œprogrammingâ€ gender-neutral english,
technically accurate, less commonly used spoken
french â€œaiâ€ used less frequently everyday speech.â€™
â€˜modelâ€™: â€˜â€™, â€˜finish_reasonâ€™: â€˜lengthâ€™}
id=â€™run-7d7b1967-9612-4f9a-911a-b2b5ca85046a-0â€™ async: await chat . ainvoke ( message ) aimessage ( content = 'je dÃ©aime le programming. littÃ©rale je (jâ€™aime) dÃ©aime (le) programming. note: â€œprogrammingâ€ french â€œprogrammationâ€. here, used â€œprogrammingâ€ instead
â€œprogrammationâ€ user said â€œi love programmingâ€
instead â€œi love programming (in french)â€, would
â€œjâ€™aime la programmationâ€. translating sentence
literally, preserved original meaning userâ€™s
sentence.â€™, id=â€™run-fd850318-e299-4735-b4c6-3496dc930b1d-0â€™) tool calling: langchain_core.pydantic_v1 import basemodel, field
class getweather(basemodel):
'''get current weather given location'''
location: str = field(..., description="the city state,
e.g. san francisco, ca")
class getpopulation(basemodel):
'''get current population given location'''
chat_with_tools = chat.bind_tools([getweather, getpopulation])
ai_msg = chat_with_tools.invoke("which city hotter today
bigger: la ny?")
ai_msg.tool_calls [{ 'name' : 'getpopulation' , 'args' : { 'location' : 'los angeles, ca' }, 'id' : '0' }] response metadata ai_msg = chat . invoke ( message ) ai_msg . response_metadata note chathuggingface implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # deprecated since version 0.1.7: use callback instead. callback manager add run trace. param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param disable_streaming : bool | literal [ 'tool_calling' ] = false # whether disable streaming model. streaming bypassed, stream()/astream() defer invoke()/ainvoke() . true, always bypass streaming case. â€œtool_callingâ€, bypass streaming case model called
tool keyword argument. false (default), always use streaming case available. param llm : = none # llm, must type huggingfacetextgeninference, huggingfaceendpoint,
huggingfacehub, huggingfacepipeline. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_id : str | none = none # param rate_limiter : baseratelimiter | none = none # optional rate limiter use limiting number requests. param system_message : systemmessage = systemmessage(content='you helpful, respectful, honest assistant.') # param tag : list [ str ] | none = none # tag add run trace. param tokenizer : = none # param verbose : bool [optional] # whether print response text. __call__ ( message : list [ basemessage ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , ** kwargs : ) â†’ basemessage # deprecated since version langchain-core==0.1.7: use invoke instead. parameter : message ( list [ basemessage ] ) â€“ stop ( list [ str ] | none ) â€“ callback ( list [ basecallbackhandler ] | basecallbackmanager | none ) â€“ kwargs ( ) â€“ return type : basemessage async abatch ( input : list [ input ] , config : runnableconfig | list [ runnableconfig ] | none = none , * , return_exceptions : bool = false , ** kwargs : | none ) â†’ list [ output ] # default implementation run ainvoke parallel using asyncio.gather. default implementation batch work well io bound runnables. subclass override method batch efficiently;
yielding result complete. parameter : input ( sequence [ input ] ) â€“ config ( runnableconfig | sequence [ runnableconfig ] | none ) â€“ return_exceptions ( bool ) â€“ kwargs ( | none ) â€“ return type : iterator [ tuple [int, output | exception]] bind_tools ( tool : sequence [ dict [ str , ] | type | callable | basetool ] , * , tool_choice : dict | str | literal [ 'auto' , 'none' ] | bool | none = none , ** kwargs : ) â†’ runnable [ promptvalue | str | sequence [ basemessage | list [ str ] | tuple [ str , str ] | str | dict [ str , ] ] , basemessage ] [source] # bind tool-like object chat model. assumes model compatible openai tool-calling api. parameter : tool ( sequence [ dict [ str , ] | type | callable | basetool ] ) â€“ list tool definition bind chat model.
support tool definition handled langchain_core.utils.function_calling.convert_to_openai_tool() . tool_choice ( dict | str | literal [ 'auto' , 'none' ] | bool | none ) â€“ tool require model call.
page: https://python.langchain.com/v0.2/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.tgi_message.html#langchain_huggingface.chat_models.huggingface.tgi_message
tgi_message # class langchain_huggingface.chat_models.huggingface. tgi_message ( role : str , content : str , tool_calls : list [ dict ] ) [source] # message send textgeninference api. attribute method __init__ (role, content, tool_calls) parameter : role ( str ) â€“ content ( str ) â€“ tool_calls ( list [ dict ] ) â€“ __init__ ( role : str , content : str , tool_calls : list [ dict ] ) â†’ none # parameter : role ( str ) â€“ content ( str ) â€“ tool_calls ( list [ dict ] ) â€“ return type : none
page: https://python.langchain.com/v0.2/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.tgi_response.html#langchain_huggingface.chat_models.huggingface.tgi_response
tgi_response # class langchain_huggingface.chat_models.huggingface. tgi_response ( choice : list [ ] , usage : dict ) [source] # response textgeninference api. attribute method __init__ (choices, usage) parameter : choice ( list [ ] ) â€“ usage ( dict ) â€“ __init__ ( choice : list [ ] , usage : dict ) â†’ none # parameter : choice ( list [ ] ) â€“ usage ( dict ) â€“ return type : none
page: https://python.langchain.com/v0.2/api_reference/huggingface/embeddings.html#langchain-huggingface-embeddings
embeddings # class embeddings.huggingface.huggingfaceembeddings huggingface sentence_transformers embedding models. embeddings.huggingface_endpoint.huggingfaceendpointembeddings huggingfacehub embedding models.
page: https://python.langchain.com/v0.2/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface.huggingfaceembeddings.html#langchain_huggingface.embeddings.huggingface.huggingfaceembeddings
huggingfaceembeddings # class langchain_huggingface.embeddings.huggingface. huggingfaceembeddings [source] # bases: basemodel , embeddings huggingface sentence_transformers embedding models. use, sentence_transformers python package installed. example langchain_huggingface import huggingfaceembeddings model_name = "sentence-transformers/all-mpnet-base-v2" model_kwargs = { 'device' : 'cpu' } encode_kwargs = { 'normalize_embeddings' : false } hf = huggingfaceembeddings ( model_name = model_name , model_kwargs = model_kwargs , encode_kwargs = encode_kwargs ) initialize sentence_transformer. param cache_folder : str | none = none # path store models.
also set sentence_transformers_home environment variable. param encode_kwargs : dict [ str , ] [optional] # keyword argument pas calling encode method sentence
transformer model, prompt_name , prompt , batch_size , precision , normalize_embeddings , more.
see also sentence transformer documentation: https://sbert.net/docs/package_reference/sentencetransformer.html#sentence_transformers.sentencetransformer.encode param model_kwargs : dict [ str , ] [optional] # keyword argument pas sentence transformer model, device , prompt , default_prompt_name , revision , trust_remote_code , token .
see also sentence transformer documentation: https://sbert.net/docs/package_reference/sentencetransformer.html#sentence_transformers.sentencetransformer param model_name : str = 'sentence-transformers/all-mpnet-base-v2' # model name use. param multi_process : bool = false # run encode() multiple gpus. param show_progress : bool = false # whether show progress bar. async aembed_documents ( text : list [ str ] ) â†’ list [ list [ float ] ] # asynchronous embed search docs. parameter : text ( list [ str ] ) â€“ list text embed. return : list embeddings. return type : list [ list [float]] async aembed_query ( text : str ) â†’ list [ float ] # asynchronous embed query text. parameter : text ( str ) â€“ text embed. return : embedding. return type : list [float] embed_documents ( text : list [ str ] ) â†’ list [ list [ float ] ] [source] # compute doc embeddings using huggingface transformer model. parameter : text ( list [ str ] ) â€“ list text embed. return : list embeddings, one text. return type : list [ list [float]] embed_query ( text : str ) â†’ list [ float ] [source] # compute query embeddings using huggingface transformer model. parameter : text ( str ) â€“ text embed. return : embeddings text. return type : list [float] example using huggingfaceembeddings aerospike self-query-qdrant
page: https://python.langchain.com/v0.2/api_reference/huggingface/embeddings/langchain_huggingface.embeddings.huggingface_endpoint.huggingfaceendpointembeddings.html#langchain_huggingface.embeddings.huggingface_endpoint.huggingfaceendpointembeddings
huggingfaceendpointembeddings # class langchain_huggingface.embeddings.huggingface_endpoint. huggingfaceendpointembeddings [source] # bases: basemodel , embeddings huggingfacehub embedding models. use, huggingface_hub python package installed,
environment variable huggingfacehub_api_token set api token, pas
named parameter constructor. example langchain_huggingface import huggingfaceendpointembeddings model = "sentence-transformers/all-mpnet-base-v2" hf = huggingfaceendpointembeddings ( model = model , task = "feature-extraction" , huggingfacehub_api_token = "my-api-key" , ) create new model parsing validating input data keyword arguments. raise validationerror input data cannot parsed form valid model. param huggingfacehub_api_token : str | none = none # param model : str | none = none # model name use. param model_kwargs : dict | none = none # keyword argument pas model. param repo_id : str | none = none # huggingfacehub repository id, backward compatibility. param task : str | none = 'feature-extraction' # task call model with. async aembed_documents ( text : list [ str ] ) â†’ list [ list [ float ] ] [source] # async call huggingfacehubâ€™s embedding endpoint embedding search docs. parameter : text ( list [ str ] ) â€“ list text embed. return : list embeddings, one text. return type : list [ list [float]] async aembed_query ( text : str ) â†’ list [ float ] [source] # async call huggingfacehubâ€™s embedding endpoint embedding query text. parameter : text ( str ) â€“ text embed. return : embeddings text. return type : list [float] embed_documents ( text : list [ str ] ) â†’ list [ list [ float ] ] [source] # call huggingfacehubâ€™s embedding endpoint embedding search docs. parameter : text ( list [ str ] ) â€“ list text embed. return : list embeddings, one text. return type : list [ list [float]] embed_query ( text : str ) â†’ list [ float ] [source] # call huggingfacehubâ€™s embedding endpoint embedding query text. parameter : text ( str ) â€“ text embed. return : embeddings text. return type : list [float]
page: https://python.langchain.com/v0.2/api_reference/huggingface/llms.html#langchain-huggingface-llms
llm # class llms.huggingface_endpoint.huggingfaceendpoint huggingface endpoint. llms.huggingface_pipeline.huggingfacepipeline huggingface pipeline api.
page: https://python.langchain.com/v0.2/api_reference/huggingface/llms/langchain_huggingface.llms.huggingface_endpoint.huggingfaceendpoint.html#langchain_huggingface.llms.huggingface_endpoint.huggingfaceendpoint
huggingfaceendpoint # class langchain_huggingface.llms.huggingface_endpoint. huggingfaceendpoint [source] # bases: llm huggingface endpoint. use class, installed huggingface_hub package,
environment variable huggingfacehub_api_token set api token,
given named parameter constructor. example # basic example (no streaming) llm = huggingfaceendpoint ( endpoint_url = "http://localhost:8010/" , max_new_tokens = 512 , top_k = 10 , top_p = 0.95 , typical_p = 0.95 , temperature = 0.01 , repetition_penalty = 1.03 , huggingfacehub_api_token = "my-api-key" ) print ( llm . invoke ( "what deep learning?" )) # streaming response example langchain_core.callbacks.streaming_stdout import streamingstdoutcallbackhandler callback = [ streamingstdoutcallbackhandler ()] llm = huggingfaceendpoint ( endpoint_url = "http://localhost:8010/" , max_new_tokens = 512 , top_k = 10 , top_p = 0.95 , typical_p = 0.95 , temperature = 0.01 , repetition_penalty = 1.03 , callback = callback , streaming = true , huggingfacehub_api_token = "my-api-key" ) print ( llm . invoke ( "what deep learning?" )) note huggingfaceendpoint implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param async_client : = none # param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param client : = none # param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param do_sample : bool = false # activate logits sampling param endpoint_url : str | none = none # endpoint url use. repo_id specified need given
pas env variable hf_inference_endpoint param huggingfacehub_api_token : str | none = none # param inference_server_url : str = '' # text-generation-inference instance base url param max_new_tokens : int = 512 # maximum number generated token param metadata : dict [ str , ] | none = none # metadata add run trace. param model : str [required] # param model_kwargs : dict [ str , ] [optional] # hold model parameter valid call explicitly specified param repetition_penalty : float | none = none # parameter repetition penalty. 1.0 mean penalty.
see [this paper]( https://arxiv.org/pdf/1909.05858.pdf ) details. param repo_id : str | none = none # repo use. endpoint_url specified need given param return_full_text : bool = false # whether prepend prompt generated text param seed : int | none = none # random sampling seed param server_kwargs : dict [ str , ] [optional] # hold text-generation-inference server parameter explicitly specified param stop_sequences : list [ str ] [optional] # stop generating token member stop_sequences generated param streaming : bool = false # whether generate stream token asynchronously param tag : list [ str ] | none = none # tag add run trace. param task : str | none = none # task call model with.
task return generated_text summary_text . param temperature : float | none = 0.8 # value used module logits distribution. param timeout : int = 120 # timeout second param top_k : int | none = none # number highest probability vocabulary token keep
top-k-filtering. param top_p : float | none = 0.95 # set < 1, smallest set probable token probability
add top_p higher kept generation. param truncate : int | none = none # truncate input token given size param typical_p : float | none = 0.95 # typical decoding mass. see [typical decoding natural language
generation]( https://arxiv.org/abs/2202.00666 ) information. param verbose : bool [optional] # whether print response text. param watermark : bool = false # watermarking [a watermark large language models]
( https://arxiv.org/abs/2301.10226 ) __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) â€“ prompt generate from. stop ( list [ str ] | none ) â€“ stop word use generating. model output cut
page: https://python.langchain.com/v0.2/api_reference/huggingface/llms/langchain_huggingface.llms.huggingface_pipeline.huggingfacepipeline.html#langchain_huggingface.llms.huggingface_pipeline.huggingfacepipeline
huggingfacepipeline # class langchain_huggingface.llms.huggingface_pipeline. huggingfacepipeline [source] # bases: basellm huggingface pipeline api. use, transformer python package installed. support text-generation , text2text-generation , summarization translation now. example using from_model_id: langchain_huggingface import huggingfacepipeline hf = huggingfacepipeline . from_model_id ( model_id = "gpt2" , task = "text-generation" , pipeline_kwargs = { "max_new_tokens" : 10 }, ) example passing pipeline directly: langchain_huggingface import huggingfacepipeline transformer import automodelforcausallm , autotokenizer , pipeline model_id = "gpt2" tokenizer = autotokenizer . from_pretrained ( model_id ) model = automodelforcausallm . from_pretrained ( model_id ) pipe = pipeline ( "text-generation" , model = model , tokenizer = tokenizer , max_new_tokens = 10 ) hf = huggingfacepipeline ( pipeline = pipe ) note huggingfacepipeline implement standard runnable interface . ðŸƒ runnable interface additional method available runnables, with_types , with_retry , assign , bind , get_graph , more. param batch_size : int = 4 # batch size use passing multiple document generate. param cache : basecache | bool | none = none # whether cache response. true, use global cache. false, use cache none, use global cache itâ€™s set, otherwise cache. instance basecache, use provided cache. caching currently supported streaming method models. param callback_manager : basecallbackmanager | none = none # [deprecated] param callback : callback = none # callback add run trace. param custom_get_token_ids : callable [ [ str ] , list [ int ] ] | none = none # optional encoder use counting tokens. param metadata : dict [ str , ] | none = none # metadata add run trace. param model_id : str = 'gpt2' # model name use. param model_kwargs : dict | none = none # keyword argument passed model. param pipeline_kwargs : dict | none = none # keyword argument passed pipeline. param tag : list [ str ] | none = none # tag add run trace. param verbose : bool [optional] # whether print response text. __call__ ( prompt : str , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none = none , * , tag : list [ str ] | none = none , metadata : dict [ str , ] | none = none , ** kwargs : ) â†’ str # deprecated since version langchain-core==0.1.7: use invoke instead. check cache run llm given prompt input. parameter : prompt ( str ) â€“ prompt generate from. stop ( list [ str ] | none ) â€“ stop word use generating. model output cut
return runnable instances. return : new runnable alternative configured. return type : runnableserializable [ input , output ] langchain_anthropic import chatanthropic langchain_core.runnables.utils import configurablefield langchain_openai import chatopenai model = chatanthropic ( model_name = "claude-3-sonnet-20240229" ) . configurable_alternatives ( configurablefield ( id = "llm" ), default_key = "anthropic" , openai = chatopenai () ) # us default model chatanthropic print ( model . invoke ( "which organization created you?" ) . content ) # us chatopenai print ( model . with_config ( configurable = { "llm" : "openai" } ) . invoke ( "which organization created you?" ) . content ) configurable_fields ( ** kwargs : configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) â†’ runnableserializable [ input , output ] # configure particular runnable field runtime. parameter : **kwargs ( configurablefield | configurablefieldsingleoption | configurablefieldmultioption ) â€“ dictionary configurablefield instance configure. return : new runnable field configured. return type : runnableserializable [ input , output ] langchain_core.runnables import configurablefield langchain_openai import chatopenai model = chatopenai ( max_tokens = 20 ) . configurable_fields ( max_tokens = configurablefield ( id = "output_token_number" , name = "max token output" , description = "the maximum number token output" , ) ) # max_tokens = 20 print ( "max_tokens_20: " , model . invoke ( "tell something chess" ) . content ) # max_tokens = 200 print ( "max_tokens_200: " , model . with_config ( configurable = { "output_token_number" : 200 } ) . invoke ( "tell something chess" ) . content ) classmethod from_model_id ( model_id : str , task : str , backend : str = 'default' , device : int | none = -1 , device_map : str | none = none , model_kwargs : dict | none = none , pipeline_kwargs : dict | none = none , batch_size : int = 4 , ** kwargs : ) â†’ huggingfacepipeline [source] # construct pipeline object model_id task. parameter : model_id ( str ) â€“ task ( str ) â€“ backend ( str ) â€“ device ( int | none ) â€“ device_map ( str | none ) â€“ model_kwargs ( dict | none ) â€“ pipeline_kwargs ( dict | none ) â€“ batch_size ( int ) â€“ kwargs ( ) â€“ return type : huggingfacepipeline generate ( prompt : list [ str ] , stop : list [ str ] | none = none , callback : list [ basecallbackhandler ] | basecallbackmanager | none | list [ list [ basecallbackhandler ] | basecallbackmanager | none ] = none , * , tag : list [ str ] | list [ list [ str ] ] | none = none , metadata : dict [ str , ] | list [ dict [ str , ] ] | none = none , run_name : str | list [ str ] | none = none , run_id : uuid | list [ uuid | none ] | none = none , ** kwargs : ) â†’ llmresult # pas sequence prompt model return generations. method make use batched call model expose batched
