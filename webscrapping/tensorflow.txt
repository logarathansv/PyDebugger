Page: https://www.tensorflow.org/api_docs/python/tf
View source on GitHub TensorFlow pip install tensorflow Modules audio module: Public API for tf._api.v2.audio namespace autodiff module: Public API for tf._api.v2.autodiff namespace autograph module: Public API for tf._api.v2.autograph namespace bitwise module: Public API for tf._api.v2.bitwise namespace compat module: Public API for tf._api.v2.compat namespace config module: Public API for tf._api.v2.config namespace data module: Public API for tf._api.v2.data namespace debugging module: Public API for tf._api.v2.debugging namespace distribute module: Public API for tf._api.v2.distribute namespace dtypes module: Public API for tf._api.v2.dtypes namespace errors module: Public API for tf._api.v2.errors namespace experimental module: Public API for tf._api.v2.experimental namespace feature_column module: Public API for tf._api.v2.feature_column namespace graph_util module: Public API for tf._api.v2.graph_util namespace image module: Public API for tf._api.v2.image namespace io module: Public API for tf._api.v2.io namespace keras module: DO NOT EDIT. linalg module: Public API for tf._api.v2.linalg namespace lite module: Public API for tf._api.v2.lite namespace lookup module: Public API for tf._api.v2.lookup namespace math module: Public API for tf._api.v2.math namespace mlir module: Public API for tf._api.v2.mlir namespace nest module: Public API for tf._api.v2.nest namespace nn module: Public API for tf._api.v2.nn namespace profiler module: Public API for tf._api.v2.profiler namespace quantization module: Public API for tf._api.v2.quantization namespace queue module: Public API for tf._api.v2.queue namespace ragged module: Public API for tf._api.v2.ragged namespace random module: Public API for tf._api.v2.random namespace raw_ops module: Public API for tf._api.v2.raw_ops namespace saved_model module: Public API for tf._api.v2.saved_model namespace sets module: Public API for tf._api.v2.sets namespace signal module: Public API for tf._api.v2.signal namespace sparse module: Public API for tf._api.v2.sparse namespace strings module: Public API for tf._api.v2.strings namespace summary module: Public API for tf._api.v2.summary namespace sysconfig module: Public API for tf._api.v2.sysconfig namespace test module: Public API for tf._api.v2.test namespace tpu module: Public API for tf._api.v2.tpu namespace train module: Public API for tf._api.v2.train namespace types module: Public API for tf._api.v2.types namespace version module: Public API for tf._api.v2.version namespace xla module: Public API for tf._api.v2.xla namespace Classes class AggregationMethod : A class listing aggregation methods used to combine gradients. class CriticalSection : Critical section. class DType : Represents the type of the elements in a Tensor . class DeviceSpec : Represents a (possibly partial) specification for a TensorFlow device. class GradientTape : Record operations for automatic differentiation. class Graph : A TensorFlow computation, represented as a dataflow graph. class IndexedSlices : A sparse representation of a set of tensor slices at given indices. class IndexedSlicesSpec : Type specification for a tf.IndexedSlices . class Module : Base neural network module class. class Operation : Represents a graph node that performs computation on tensors. class OptionalSpec : Type specification for tf.experimental.Optional . class RaggedTensor : Represents a ragged tensor. class RaggedTensorSpec : Type specification for a tf.RaggedTensor . class RegisterGradient : A decorator for registering the gradient function for an op type. class SparseTensor : Represents a sparse tensor. class SparseTensorSpec : Type specification for a tf.sparse.SparseTensor . class Tensor : A tf.Tensor represents a multidimensional array of elements. class TensorArray : Class wrapping dynamic-sized, per-time-step, Tensor arrays. class TensorArraySpec : Type specification for a tf.TensorArray . class TensorShape : Represents the shape of a Tensor . class TensorSpec : Describes the type of a tf.Tensor. class TypeSpec : Specifies a TensorFlow value type. class UnconnectedGradients : Controls how gradient computation behaves when y does not depend on x. class Variable : See the variable guide . class VariableAggregation : Indicates how a distributed variable will be aggregated. class VariableSynchronization : Indicates when a distributed variable will be synced. class constant_initializer : Initializer that generates tensors with constant values. class name_scope : A context manager for use when defining a Python op. class ones_initializer : Initializer that generates tensors initialized to 1. class random_normal_initializer : Initializer that generates tensors with a normal distribution. class random_uniform_initializer : Initializer that generates tensors with a uniform distribution. class zeros_initializer : Initializer that generates tensors initialized to 0. Functions Assert(...) : Asserts that the given condition is true. abs(...) : Computes the absolute value of a tensor. acos(...) : Computes acos of x element-wise. acosh(...) : Computes inverse hyperbolic cosine of x element-wise. add(...) : Returns x + y element-wise. add_n(...) : Returns the element-wise sum of a list of tensors. approx_top_k(...) : Returns min/max k values and their indices of the input operand in an approximate manner. argmax(...) : Returns the index with the largest value across axes of a tensor. argmin(...) : Returns the index with the smallest value across axes of a tensor. argsort(...) : Returns the indices of a tensor that give its sorted order along an axis. as_dtype(...) : Converts the given type_value to a tf.DType . as_string(...) : Converts each entry in the given tensor to strings. asin(...) : Computes the trignometric inverse sine of x element-wise. asinh(...) : Computes inverse hyperbolic sine of x element-wise. assert_equal(...) : Assert the condition x == y holds element-wise. assert_greater(...) : Assert the condition x > y holds element-wise. assert_less(...) : Assert the condition x < y holds element-wise. assert_rank(...) : Assert that x has rank equal to rank . atan(...) : Computes the trignometric inverse tangent of x element-wise. atan2(...) : Computes arctangent of y/x element-wise, respecting signs of the arguments. atanh(...) : Computes inverse hyperbolic tangent of x element-wise. batch_to_space(...) : BatchToSpace for N-D tensors of type T. bitcast(...) : Bitcasts a tensor from one type to another without copying data. boolean_mask(...) : Apply boolean mask to tensor. broadcast_dynamic_shape(...) : Computes the shape of a broadcast given symbolic shapes. broadcast_static_shape(...) : Computes the shape of a broadcast given known shapes. broadcast_to(...) : Broadcast an array for a compatible shape. case(...) : Create a case operation. cast(...) : Casts a tensor to a new type. clip_by_global_norm(...) : Clips values of multiple tensors by the ratio of the sum of their norms. clip_by_norm(...) : Clips tensor values to a maximum L2-norm. clip_by_value(...) : Clips tensor values to a specified min and max. complex(...) : Converts two real numbers to a complex number. concat(...) : Concatenates tensors along one dimension. cond(...) : Return true_fn() if the predicate pred is true else false_fn() . constant(...) : Creates a constant tensor from a tensor-like object. control_dependencies(...) : Wrapper for Graph.control_dependencies() using the default graph. conv(...) : Computes a N-D convolution given (N+1+batch_dims)-D input and (N+2)-D filter tensors. conv2d_backprop_filter_v2(...) : Computes the gradients of convolution with respect to the filter. conv2d_backprop_input_v2(...) : Computes the gradients of convolution with respect to the input. convert_to_tensor(...) : Converts the given value to a Tensor . cos(...) : Computes cos of x element-wise. cosh(...) : Computes hyperbolic cosine of x element-wise. cumsum(...) : Compute the cumulative sum of the tensor x along axis . custom_gradient(...) : Decorator to define a function with a custom gradient. device(...) : Specifies the device for ops created/executed in this context. divide(...) : Computes Python style division of x by y . dynamic_partition(...) : Partitions data into num_partitions tensors using indices from partitions . dynamic_stitch(...) : Interleave the values from the data tensors into a single tensor. edit_distance(...) : Computes the Levenshtein distance between sequences. eig(...) : Computes the eigen decomposition of a batch of matrices. eigvals(...) : Computes the eigenvalues of one or more matrices. einsum(...) : Tensor contraction over specified indices and outer product. ensure_shape(...) : Updates the shape of a tensor and checks at runtime that the shape holds. equal(...) : Returns the truth value of (x == y) element-wise. executing_eagerly(...) : Checks whether the current thread has eager execution enabled. exp(...) : Computes exponential of x element-wise.  \(y = e^x\). expand_dims(...) : Returns a tensor with a length 1 axis inserted at index axis . extract_volume_patches(...) : Extract patches from input and put them in the "depth" output dimension. 3D extension of extract_image_patches . eye(...) : Construct an identity matrix, or a batch of matrices. fftnd(...) : ND fast Fourier transform. fill(...) : Creates a tensor filled with a scalar value. fingerprint(...) : Generates fingerprint values. floor(...) : Returns element-wise largest integer not greater than x. foldl(...) : foldl on the list of tensors unpacked from elems on dimension 0. (deprecated argument values) foldr(...) : foldr on the list of tensors unpacked from elems on dimension 0. (deprecated argument values) function(...) : Compiles a function into a callable TensorFlow graph. (deprecated arguments) (deprecated arguments) (deprecated arguments) gather(...) : Gather slices from params axis axis according to indices. (deprecated arguments) gather_nd(...) : Gather slices from params into a Tensor with shape specified by indices . get_current_name_scope(...) : Returns current full name scope specified by tf.name_scope(...) s. get_logger(...) : Return TF logger instance. get_static_value(...) : Returns the constant value of the given tensor, if efficiently calculable. grad_pass_through(...) : Creates a grad-pass-through op with the forward behavior provided in f. gradients(...) : Constructs symbolic derivatives of sum of ys w.r.t. x in xs . greater(...) : Returns the truth value of (x > y) element-wise. greater_equal(...) : Returns the truth value of (x >= y) element-wise. group(...) : Create an op that groups multiple operations. guarantee_const(...) : Promise to the TF runtime that the input tensor is a constant. (deprecated) hessians(...) : Constructs the Hessian of sum of ys with respect to x in xs . histogram_fixed_width(...) : Return histogram of values. histogram_fixed_width_bins(...) : Bins the given values for use in a histogram. identity(...) : Return a Tensor with the same shape and contents as input. identity_n(...) : Returns a list of tensors with the same shapes and contents as the input ifftnd(...) : ND inverse fast Fourier transform. import_graph_def(...) : Imports the graph from graph_def into the current default Graph . (deprecated arguments) init_scope(...) : A context manager that lifts ops out of control-flow scopes and function-building graphs. inside_function(...) : Indicates whether the caller code is executing inside a tf.function . irfftnd(...) : ND inverse real fast Fourier transform. is_symbolic_tensor(...) : Test if tensor is a symbolic Tensor. is_tensor(...) : Checks whether x is a TF-native type that can be passed to many TF ops. less(...) : Returns the truth value of (x < y) element-wise. less_equal(...) : Returns the truth value of (x <= y) element-wise. linspace(...) : Generates evenly-spaced values in an interval along a given axis. load_library(...) : Loads a TensorFlow plugin. load_op_library(...) : Loads a TensorFlow plugin, containing custom ops and kernels. logical_and(...) : Returns the truth value of x AND y element-wise. logical_not(...) : Returns the truth value of NOT x element-wise. logical_or(...) : Returns the truth value of x OR y element-wise. make_ndarray(...) : Create a numpy ndarray from a tensor. make_tensor_proto(...) : Create a TensorProto. map_fn(...) : Transforms elems by applying fn to each element unstacked on axis 0. (deprecated arguments) matmul(...) : Multiplies matrix a by matrix b , producing a * b . matrix_square_root(...) : Computes the matrix square root of one or more square matrices: maximum(...) : Returns the max of x and y (i.e. x > y ? x : y) element-wise. meshgrid(...) : Broadcasts parameters for evaluation on an N-D grid. minimum(...) : Returns the min of x and y (i.e. x < y ? x : y) element-wise. multiply(...) : Returns an element-wise x * y. negative(...) : Computes numerical negative value element-wise. no_gradient(...) : Specifies that ops of type op_type is not differentiable. no_op(...) : Does nothing. Only useful as a placeholder for control edges. nondifferentiable_batch_function(...) : Batches the computation done by the decorated function. norm(...) : Computes the norm of vectors, matrices, and tensors. not_equal(...) : Returns the truth value of (x != y) element-wise. numpy_function(...) : Wraps a python function and uses it as a TensorFlow op. one_hot(...) : Returns a one-hot tensor. ones(...) : Creates a tensor with all elements set to one (1). ones_like(...) : Creates a tensor of all ones that has the same shape as the input. pad(...) : Pads a tensor. parallel_stack(...) : Stacks a list of rank- R tensors into one rank- (R+1) tensor in parallel. pow(...) : Computes the power of one value to another. print(...) : Print the specified inputs. py_function(...) : Wraps a python function into a TensorFlow op that executes it eagerly. ragged_fill_empty_rows(...) ragged_fill_empty_rows_grad(...) random_index_shuffle(...) : Outputs the position of value in a permutation of [0, ..., max_index]. range(...) : Creates a sequence of numbers. rank(...) : Returns the rank of a tensor. realdiv(...) : Returns x / y element-wise for real types. recompute_grad(...) : Defines a function as a recompute-checkpoint for the tape auto-diff. reduce_all(...) : Computes tf.math.logical_and of elements across dimensions of a tensor. reduce_any(...) : Computes tf.math.logical_or of elements across dimensions of a tensor. reduce_logsumexp(...) : Computes log(sum(exp(elements across dimensions of a tensor))). reduce_max(...) : Computes tf.math.maximum of elements across dimensions of a tensor. reduce_mean(...) : Computes the mean of elements across dimensions of a tensor. reduce_min(...) : Computes the tf.math.minimum of elements across dimensions of a tensor. reduce_prod(...) : Computes tf.math.multiply of elements across dimensions of a tensor. reduce_sum(...) : Computes the sum of elements across dimensions of a tensor. register_tensor_conversion_function(...) : Registers a function for converting objects of base_type to Tensor . repeat(...) : Repeat elements of input . required_space_to_batch_paddings(...) : Calculate padding required to make block_shape divide input_shape. reshape(...) : Reshapes a tensor. reverse(...) : Reverses specific dimensions of a tensor. reverse_sequence(...) : Reverses variable length slices. rfftnd(...) : ND fast real Fourier transform. roll(...) : Rolls the elements of a tensor along an axis. round(...) : Rounds the values of a tensor to the nearest integer, element-wise. saturate_cast(...) : Performs a safe saturating cast of value to dtype . scalar_mul(...) : Multiplies a scalar times a Tensor or IndexedSlices object. scan(...) : scan on the list of tensors unpacked from elems on dimension 0. (deprecated argument values) scatter_nd(...) : Scatters updates into a tensor of shape shape according to indices . searchsorted(...) : Searches for where a value would go in a sorted sequence. sequence_mask(...) : Returns a mask tensor representing the first N positions of each cell. shape(...) : Returns a tensor containing the shape of the input tensor. shape_n(...) : Returns shape of a list of tensors. sigmoid(...) : Computes sigmoid of x element-wise. sign(...) : Returns an element-wise indication of the sign of a number. sin(...) : Computes sine of x element-wise. sinh(...) : Computes hyperbolic sine of x element-wise. size(...) : Returns the size of a tensor. slice(...) : Extracts a slice from a tensor. sort(...) : Sorts a tensor. space_to_batch(...) : SpaceToBatch for N-D tensors of type T. space_to_batch_nd(...) : SpaceToBatch for N-D tensors of type T. split(...) : Splits a tensor value into a list of sub tensors. sqrt(...) : Computes element-wise square root of the input tensor. square(...) : Computes square of x element-wise. squeeze(...) : Removes dimensions of size 1 from the shape of a tensor. stack(...) : Stacks a list of rank- R tensors into one rank- (R+1) tensor. stop_gradient(...) : Stops gradient computation. strided_slice(...) : Extracts a strided slice of a tensor (generalized Python array indexing). subtract(...) : Returns x - y element-wise. switch_case(...) : Create a switch/case operation, i.e. tan(...) : Computes tan of x element-wise. tanh(...) : Computes hyperbolic tangent of x element-wise. tensor_scatter_nd_add(...) : Adds sparse updates to an existing tensor according to indices . tensor_scatter_nd_max(...) : Apply a sparse update to a tensor taking the element-wise maximum. tensor_scatter_nd_min(...) tensor_scatter_nd_sub(...) : Subtracts sparse updates from an existing tensor according to indices . tensor_scatter_nd_update(...) : Scatter updates into an existing tensor according to indices . tensordot(...) : Tensor contraction of a and b along specified axes and outer product. tile(...) : Constructs a tensor by tiling a given tensor. timestamp(...) : Provides the time since epoch in seconds. transpose(...) : Transposes a , where a is a Tensor. truediv(...) : Divides x / y elementwise (using Python 3 division operator semantics). truncatediv(...) : Returns x / y element-wise, rounded towards zero. truncatemod(...) : Returns element-wise remainder of division. tuple(...) : Groups tensors together. type_spec_from_value(...) : Returns a tf.TypeSpec that represents the given value . unique(...) : Finds unique elements in a 1-D tensor. unique_with_counts(...) : Finds unique elements in a 1-D tensor. unravel_index(...) : Converts an array of flat indices into a tuple of coordinate arrays. unstack(...) : Unpacks the given dimension of a rank- R tensor into rank- (R-1) tensors. variable_creator_scope(...) : Scope which defines a variable creation function to be used by variable(). vectorized_map(...) : Parallel map on the list of tensors unpacked from elems on dimension 0. where(...) : Returns the indices of non-zero elements, or multiplexes x and y . while_loop(...) : Repeat body while the condition cond is true. (deprecated argument values) zeros(...) : Creates a tensor with all elements set to zero. zeros_like(...) : Creates a tensor with all elements set to zero. Other Members version '2.16.1' bfloat16 Instance of tf.dtypes.DType 16-bit bfloat (brain floating point). bool Instance of tf.dtypes.DType Boolean. complex128 Instance of tf.dtypes.DType 128-bit complex. complex64 Instance of tf.dtypes.DType 64-bit complex. double Instance of tf.dtypes.DType 64-bit (double precision) floating-point. float16 Instance of tf.dtypes.DType 16-bit (half precision) floating-point. float32 Instance of tf.dtypes.DType 32-bit (single precision) floating-point. float64 Instance of tf.dtypes.DType 64-bit (double precision) floating-point. half Instance of tf.dtypes.DType 16-bit (half precision) floating-point. int16 Instance of tf.dtypes.DType Signed 16-bit integer. int32 Instance of tf.dtypes.DType Signed 32-bit integer. int64 Instance of tf.dtypes.DType Signed 64-bit integer. int8 Instance of tf.dtypes.DType Signed 8-bit integer. newaxis None qint16 Instance of tf.dtypes.DType Signed quantized 16-bit integer. qint32 Instance of tf.dtypes.DType signed quantized 32-bit integer. qint8 Instance of tf.dtypes.DType Signed quantized 8-bit integer. quint16 Instance of tf.dtypes.DType Unsigned quantized 16-bit integer. quint8 Instance of tf.dtypes.DType Unsigned quantized 8-bit integer. resource Instance of tf.dtypes.DType Handle to a mutable, dynamically allocated resource. string Instance of tf.dtypes.DType Variable-length string, represented as byte array. uint16 Instance of tf.dtypes.DType Unsigned 16-bit (word) integer. uint32 Instance of tf.dtypes.DType Unsigned 32-bit (dword) integer. uint64 Instance of tf.dtypes.DType Unsigned 64-bit (qword) integer. uint8 Instance of tf.dtypes.DType Unsigned 8-bit (byte) integer. variant Instance of tf.dtypes.DType Data of arbitrary type (known at runtime).


Page: https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/__init__.py



Page: https://www.tensorflow.org/api_docs/python/tf/audio
Public API for tf._api.v2.audio namespace Functions decode_wav(...) : Decode a 16-bit PCM WAV file to a float tensor. encode_wav(...) : Encode audio data using the WAV file format.


Page: https://www.tensorflow.org/api_docs/python/tf/autodiff
Public API for tf._api.v2.autodiff namespace Classes class ForwardAccumulator : Computes Jacobian-vector products ("JVP"s) using forward-mode autodiff. class GradientTape : Record operations for automatic differentiation.


Page: https://www.tensorflow.org/api_docs/python/tf/autograph
Public API for tf._api.v2.autograph namespace Modules experimental module: Public API for tf._api.v2.autograph.experimental namespace Functions set_verbosity(...) : Sets the AutoGraph verbosity level. to_code(...) : Returns the source code generated by AutoGraph, as a string. to_graph(...) : Converts a Python entity into a TensorFlow graph. trace(...) : Traces argument information at compilation time.


Page: https://www.tensorflow.org/api_docs/python/tf/bitwise
Public API for tf._api.v2.bitwise namespace Functions bitwise_and(...) : Elementwise computes the bitwise AND of x and y . bitwise_or(...) : Elementwise computes the bitwise OR of x and y . bitwise_xor(...) : Elementwise computes the bitwise XOR of x and y . invert(...) : Invert (flip) each bit of supported types; for example, type uint8 value 01010101 becomes 10101010. left_shift(...) : Elementwise computes the bitwise left-shift of x and y . right_shift(...) : Elementwise computes the bitwise right-shift of x and y .


Page: https://www.tensorflow.org/api_docs/python/tf/compat
Public API for tf._api.v2.compat namespace Modules v1 module: Bring in all of the public TensorFlow interface into this module. Functions as_bytes(...) : Converts bytearray , bytes , or unicode python input types to bytes . as_str(...) as_str_any(...) : Converts input to str type. as_text(...) : Converts any string-like python input types to unicode. dimension_at_index(...) : Compatibility utility required to allow for both V1 and V2 behavior in TF. dimension_value(...) : Compatibility utility required to allow for both V1 and V2 behavior in TF. forward_compatibility_horizon(...) : Context manager for testing forward compatibility of generated graphs. forward_compatible(...) : Return true if the forward compatibility window has expired. path_to_str(...) : Converts input which is a PathLike object to str type. Other Members bytes_or_text_types (<class 'bytes'>, <class 'str'>) complex_types (<class 'numbers.Complex'>, <class 'numpy.number'>) integral_types (<class 'numbers.Integral'>, <class 'numpy.integer'>) real_types (<class 'numbers.Real'>, <class 'numpy.integer'>, <class 'numpy.floating'>)


Page: https://www.tensorflow.org/api_docs/python/tf/config
Public API for tf._api.v2.config namespace Modules experimental module: Public API for tf._api.v2.config.experimental namespace optimizer module: Public API for tf._api.v2.config.optimizer namespace threading module: Public API for tf._api.v2.config.threading namespace Classes class LogicalDevice : Abstraction for a logical device initialized by the runtime. class LogicalDeviceConfiguration : Configuration class for a logical devices. class PhysicalDevice : Abstraction for a locally visible physical device. Functions experimental_connect_to_cluster(...) : Connects to the given cluster. experimental_connect_to_host(...) : Connects to a single machine to enable remote execution on it. experimental_functions_run_eagerly(...) : Returns the value of the experimental_run_functions_eagerly setting. (deprecated) experimental_run_functions_eagerly(...) : Enables / disables eager execution of tf.function s. (deprecated) functions_run_eagerly(...) : Returns the value of the run_functions_eagerly setting. get_logical_device_configuration(...) : Get the virtual device configuration for a tf.config.PhysicalDevice . get_soft_device_placement(...) : Return status of soft device placement flag. get_visible_devices(...) : Get the list of visible physical devices. list_logical_devices(...) : Return a list of logical devices created by runtime. list_physical_devices(...) : Return a list of physical devices visible to the host runtime. run_functions_eagerly(...) : Enables / disables eager execution of tf.function s. set_logical_device_configuration(...) : Set the logical device configuration for a tf.config.PhysicalDevice . set_soft_device_placement(...) : Enable or disable soft device placement. set_visible_devices(...) : Set the list of visible devices.


Page: https://www.tensorflow.org/api_docs/python/tf/data
Public API for tf._api.v2.data namespace Modules experimental module: Public API for tf._api.v2.data.experimental namespace Classes class Dataset : Represents a potentially large set of elements. class DatasetSpec : Type specification for tf.data.Dataset . class FixedLengthRecordDataset : A Dataset of fixed-length records from one or more binary files. class Iterator : Represents an iterator of a tf.data.Dataset . class IteratorSpec : Type specification for tf.data.Iterator . class NumpyIterator : Iterator over a dataset with elements converted to numpy. class Options : Represents options for tf.data.Dataset . class TFRecordDataset : A Dataset comprising records from one or more TFRecord files. class TextLineDataset : Creates a Dataset comprising lines from one or more text files. class ThreadingOptions : Represents options for dataset threading. Other Members AUTOTUNE -1 INFINITE_CARDINALITY -1 UNKNOWN_CARDINALITY -2


Page: https://www.tensorflow.org/api_docs/python/tf/debugging
Public API for tf._api.v2.debugging namespace Modules experimental module: Public API for tf._api.v2.debugging.experimental namespace Functions Assert(...) : Asserts that the given condition is true. assert_all_finite(...) : Assert that the tensor does not contain any NaN's or Inf's. assert_equal(...) : Assert the condition x == y holds element-wise. assert_greater(...) : Assert the condition x > y holds element-wise. assert_greater_equal(...) : Assert the condition x >= y holds element-wise. assert_integer(...) : Assert that x is of integer dtype. assert_less(...) : Assert the condition x < y holds element-wise. assert_less_equal(...) : Assert the condition x <= y holds element-wise. assert_near(...) : Assert the condition x and y are close element-wise. assert_negative(...) : Assert the condition x < 0 holds element-wise. assert_non_negative(...) : Assert the condition x >= 0 holds element-wise. assert_non_positive(...) : Assert the condition x <= 0 holds element-wise. assert_none_equal(...) : Assert the condition x != y holds element-wise. assert_positive(...) : Assert the condition x > 0 holds element-wise. assert_proper_iterable(...) : Static assert that values is a "proper" iterable. assert_rank(...) : Assert that x has rank equal to rank . assert_rank_at_least(...) : Assert that x has rank of at least rank . assert_rank_in(...) : Assert that x has a rank in ranks . assert_same_float_dtype(...) : Validate and return float type based on tensors and dtype . assert_scalar(...) : Asserts that the given tensor is a scalar. assert_shapes(...) : Assert tensor shapes and dimension size relationships between tensors. assert_type(...) : Asserts that the given Tensor is of the specified type. check_numerics(...) : Checks a tensor for NaN and Inf values. disable_check_numerics(...) : Disable the eager/graph unified numerics checking mechanism. disable_traceback_filtering(...) : Disable filtering out TensorFlow-internal frames in exception stack traces. enable_check_numerics(...) : Enable tensor numerics checking in an eager/graph unified fashion. enable_traceback_filtering(...) : Enable filtering out TensorFlow-internal frames in exception stack traces. get_log_device_placement(...) : Get if device placements are logged. is_numeric_tensor(...) : Returns True if the elements of tensor are numbers. is_traceback_filtering_enabled(...) : Check whether traceback filtering is currently enabled. set_log_device_placement(...) : Turns logging for device placement decisions on or off.


Page: https://www.tensorflow.org/api_docs/python/tf/distribute
Public API for tf._api.v2.distribute namespace Modules cluster_resolver module: Public API for tf._api.v2.distribute.cluster_resolver namespace coordinator module: Public API for tf._api.v2.distribute.coordinator namespace experimental module: Public API for tf._api.v2.distribute.experimental namespace Classes class CrossDeviceOps : Base class for cross-device reduction and broadcasting algorithms. class DistributedDataset : Represents a dataset distributed among devices and machines. class DistributedIterator : An iterator over tf.distribute.DistributedDataset . class DistributedValues : Base class for representing distributed values. class HierarchicalCopyAllReduce : Hierarchical copy all-reduce implementation of CrossDeviceOps. class InputContext : A class wrapping information needed by an input function. class InputOptions : Run options for experimental_distribute_dataset(s_from_function) . class InputReplicationMode : Replication mode for input function. class MirroredStrategy : Synchronous training across multiple replicas on one machine. class MultiWorkerMirroredStrategy : A distribution strategy for synchronous training on multiple workers. class NcclAllReduce : NCCL all-reduce implementation of CrossDeviceOps. class OneDeviceStrategy : A distribution strategy for running on a single device. class ParameterServerStrategy : An multi-worker tf.distribute strategy with parameter servers. class ReduceOp : Indicates how a set of values should be reduced. class ReductionToOneDevice : A CrossDeviceOps implementation that copies values to one device to reduce. class ReplicaContext : A class with a collection of APIs that can be called in a replica context. class RunOptions : Run options for strategy.run . class Server : An in-process TensorFlow server, for use in distributed training. class Strategy : A state & compute distribution policy on a list of devices. class StrategyExtended : Additional APIs for algorithms that need to be distribution-aware. class TPUStrategy : Synchronous training on TPUs and TPU Pods. Functions experimental_set_strategy(...) : Set a tf.distribute.Strategy as current without with strategy.scope() . get_replica_context(...) : Returns the current tf.distribute.ReplicaContext or None . get_strategy(...) : Returns the current tf.distribute.Strategy object. has_strategy(...) : Return if there is a current non-default tf.distribute.Strategy . in_cross_replica_context(...) : Returns True if in a cross-replica context.


Page: https://www.tensorflow.org/api_docs/python/tf/dtypes
Public API for tf._api.v2.dtypes namespace Modules experimental module: Public API for tf._api.v2.dtypes.experimental namespace Classes class DType : Represents the type of the elements in a Tensor . Functions as_dtype(...) : Converts the given type_value to a tf.DType . cast(...) : Casts a tensor to a new type. complex(...) : Converts two real numbers to a complex number. saturate_cast(...) : Performs a safe saturating cast of value to dtype . Other Members QUANTIZED_DTYPES { tf . qint16 , tf . qint16_ref , tf . qint32 , tf . qint32_ref , tf . qint8 , tf . qint8_ref , tf . quint16 , tf . quint16_ref , tf . quint8 , tf . quint8_ref } bfloat16 Instance of tf.dtypes.DType 16-bit bfloat (brain floating point). bool Instance of tf.dtypes.DType Boolean. complex128 Instance of tf.dtypes.DType 128-bit complex. complex64 Instance of tf.dtypes.DType 64-bit complex. double Instance of tf.dtypes.DType 64-bit (double precision) floating-point. float16 Instance of tf.dtypes.DType 16-bit (half precision) floating-point. float32 Instance of tf.dtypes.DType 32-bit (single precision) floating-point. float64 Instance of tf.dtypes.DType 64-bit (double precision) floating-point. half Instance of tf.dtypes.DType 16-bit (half precision) floating-point. int16 Instance of tf.dtypes.DType Signed 16-bit integer. int32 Instance of tf.dtypes.DType Signed 32-bit integer. int64 Instance of tf.dtypes.DType Signed 64-bit integer. int8 Instance of tf.dtypes.DType Signed 8-bit integer. qint16 Instance of tf.dtypes.DType Signed quantized 16-bit integer. qint32 Instance of tf.dtypes.DType signed quantized 32-bit integer. qint8 Instance of tf.dtypes.DType Signed quantized 8-bit integer. quint16 Instance of tf.dtypes.DType Unsigned quantized 16-bit integer. quint8 Instance of tf.dtypes.DType Unsigned quantized 8-bit integer. resource Instance of tf.dtypes.DType Handle to a mutable, dynamically allocated resource. string Instance of tf.dtypes.DType Variable-length string, represented as byte array. uint16 Instance of tf.dtypes.DType Unsigned 16-bit (word) integer. uint32 Instance of tf.dtypes.DType Unsigned 32-bit (dword) integer. uint64 Instance of tf.dtypes.DType Unsigned 64-bit (qword) integer. uint8 Instance of tf.dtypes.DType Unsigned 8-bit (byte) integer. variant Instance of tf.dtypes.DType Data of arbitrary type (known at runtime).


Page: https://www.tensorflow.org/api_docs/python/tf/errors
Public API for tf._api.v2.errors namespace Classes class AbortedError : Raised when an operation was aborted, typically due to a concurrent action. class AlreadyExistsError : Raised when an entity that we attempted to create already exists. class CancelledError : Raised when an operation is cancelled. class DataLossError : Raised when unrecoverable data loss or corruption is encountered. class DeadlineExceededError : Raised when a deadline expires before an operation could complete. class FailedPreconditionError : Raised when some prerequisites are not met when running an operation. class InternalError : Raised when the system experiences an internal error. class InvalidArgumentError : Raised when an operation receives an invalid argument. class NotFoundError : Raised when a requested entity (e.g., a file or directory) was not found. class OpError : The base class for TensorFlow exceptions. class OperatorNotAllowedInGraphError : Raised when an unsupported operator is present in Graph execution. class OutOfRangeError : Raised when an operation iterates past the valid range. class PermissionDeniedError : Raised when the caller does not have permission to run an operation. class ResourceExhaustedError : Raised when some resource has been exhausted while running operation. class UnauthenticatedError : Raised when the request does not have valid authentication credentials. class UnavailableError : Raised when the runtime is currently unavailable. class UnimplementedError : Raised when an operation has not been implemented. class UnknownError : Unknown error. Other Members ABORTED 10 ALREADY_EXISTS 6 CANCELLED 1 DATA_LOSS 15 DEADLINE_EXCEEDED 4 FAILED_PRECONDITION 9 INTERNAL 13 INVALID_ARGUMENT 3 NOT_FOUND 5 OK 0 OUT_OF_RANGE 11 PERMISSION_DENIED 7 RESOURCE_EXHAUSTED 8 UNAUTHENTICATED 16 UNAVAILABLE 14 UNIMPLEMENTED 12 UNKNOWN 2


Page: https://www.tensorflow.org/api_docs/python/tf/experimental
Public API for tf._api.v2.experimental namespace Modules dlpack module: Public API for tf._api.v2.experimental.dlpack namespace dtensor module: Public API for tf._api.v2.experimental.dtensor namespace extension_type module: Public API for tf._api.v2.experimental.extension_type namespace numpy module: Public API for tf._api.v2.experimental.numpy namespace tensorrt module: Public API for tf._api.v2.experimental.tensorrt namespace Classes class BatchableExtensionType : An ExtensionType that can be batched and unbatched. class DynamicRaggedShape : The shape of a ragged or dense tensor. class ExtensionType : Base class for TensorFlow ExtensionType classes. class ExtensionTypeBatchEncoder : Class used to encode and decode extension type values for batching. class ExtensionTypeSpec : Base class for tf.ExtensionType TypeSpec. class Optional : Represents a value that may or may not be present. class RowPartition : Partitioning of a sequence of values into contiguous subsequences ("rows"). class StructuredTensor : A multidimensional collection of structures with the same schema. Functions async_clear_error(...) : Clear pending operations and error statuses in async execution. async_scope(...) : Context manager for grouping async operations. dispatch_for_api(...) : Decorator that overrides the default implementation for a TensorFlow API. dispatch_for_binary_elementwise_apis(...) : Decorator to override default implementation for binary elementwise APIs. dispatch_for_binary_elementwise_assert_apis(...) : Decorator to override default implementation for binary elementwise assert APIs. dispatch_for_unary_elementwise_apis(...) : Decorator to override default implementation for unary elementwise APIs. enable_strict_mode(...) : If called, enables strict mode for all behaviors. function_executor_type(...) : Context manager for setting the executor of eager defined functions. register_filesystem_plugin(...) : Loads a TensorFlow FileSystem plugin. unregister_dispatch_for(...) : Unregisters a function that was registered with @dispatch_for_* . Other Members float8_e4m3fn Instance of tf.dtypes.DType 8-bit float with 4 exponent bits and 3 mantissa bits, with extended finite range.  This type has no representation for inf, and only two NaN values: 0xFF for negative NaN, and 0x7F for positive NaN. float8_e5m2 Instance of tf.dtypes.DType 8-bit float with 5 exponent bits and 2 mantissa bits. int4 Instance of tf.dtypes.DType Signed 4-bit integer. uint4 Instance of tf.dtypes.DType Unsigned 4-bit integer.


Page: https://www.tensorflow.org/api_docs/python/tf/feature_column
Public API for tf._api.v2.feature_column namespace Functions bucketized_column(...) : Represents discretized dense input bucketed by boundaries . (deprecated) categorical_column_with_hash_bucket(...) : Represents sparse feature where ids are set by hashing. (deprecated) categorical_column_with_identity(...) : A CategoricalColumn that returns identity values. (deprecated) categorical_column_with_vocabulary_file(...) : A CategoricalColumn with a vocabulary file. (deprecated) categorical_column_with_vocabulary_list(...) : A CategoricalColumn with in-memory vocabulary. (deprecated) crossed_column(...) : Returns a column for performing crosses of categorical features. (deprecated) embedding_column(...) : DenseColumn that converts from sparse, categorical input. (deprecated) indicator_column(...) : Represents multi-hot representation of given categorical column. (deprecated) make_parse_example_spec(...) : Creates parsing spec dictionary from input feature_columns. (deprecated) numeric_column(...) : Represents real valued or numerical features. (deprecated) sequence_categorical_column_with_hash_bucket(...) : A sequence of categorical terms where ids are set by hashing. (deprecated) sequence_categorical_column_with_identity(...) : Returns a feature column that represents sequences of integers. (deprecated) sequence_categorical_column_with_vocabulary_file(...) : A sequence of categorical terms where ids use a vocabulary file. (deprecated) sequence_categorical_column_with_vocabulary_list(...) : A sequence of categorical terms where ids use an in-memory list. (deprecated) sequence_numeric_column(...) : Returns a feature column that represents sequences of numeric data. (deprecated) shared_embeddings(...) : List of dense columns that convert from sparse, categorical input. (deprecated) weighted_categorical_column(...) : Applies weight values to a CategoricalColumn . (deprecated)


Page: https://www.tensorflow.org/api_docs/python/tf/graph_util
Public API for tf._api.v2.graph_util namespace Functions import_graph_def(...) : Imports the graph from graph_def into the current default Graph . (deprecated arguments)


Page: https://www.tensorflow.org/api_docs/python/tf/image
Public API for tf._api.v2.image namespace Classes class ResizeMethod : See tf.image.resize for details. Functions adjust_brightness(...) : Adjust the brightness of RGB or Grayscale images. adjust_contrast(...) : Adjust contrast of RGB or grayscale images. adjust_gamma(...) : Performs Gamma Correction . adjust_hue(...) : Adjust hue of RGB images. adjust_jpeg_quality(...) : Adjust jpeg encoding quality of an image. adjust_saturation(...) : Adjust saturation of RGB images. central_crop(...) : Crop the central region of the image(s). combined_non_max_suppression(...) : Greedily selects a subset of bounding boxes in descending order of score. convert_image_dtype(...) : Convert image to dtype , scaling its values if needed. crop_and_resize(...) : Extracts crops from the input image tensor and resizes them. crop_to_bounding_box(...) : Crops an image to a specified bounding box. decode_and_crop_jpeg(...) : Decode and Crop a JPEG-encoded image to a uint8 tensor. decode_bmp(...) : Decode the first frame of a BMP-encoded image to a uint8 tensor. decode_gif(...) : Decode the frame(s) of a GIF-encoded image to a uint8 tensor. decode_image(...) : Function for decode_bmp , decode_gif , decode_jpeg , and decode_png . decode_jpeg(...) : Decode a JPEG-encoded image to a uint8 tensor. decode_png(...) : Decode a PNG-encoded image to a uint8 or uint16 tensor. draw_bounding_boxes(...) : Draw bounding boxes on a batch of images. encode_jpeg(...) : JPEG-encode an image. encode_png(...) : PNG-encode an image. extract_glimpse(...) : Extracts a glimpse from the input tensor. extract_jpeg_shape(...) : Extract the shape information of a JPEG-encoded image. extract_patches(...) : Extract patches from images . flip_left_right(...) : Flip an image horizontally (left to right). flip_up_down(...) : Flip an image vertically (upside down). generate_bounding_box_proposals(...) : Generate bounding box proposals from encoded bounding boxes. grayscale_to_rgb(...) : Converts one or more images from Grayscale to RGB. hsv_to_rgb(...) : Convert one or more images from HSV to RGB. image_gradients(...) : Returns image gradients (dy, dx) for each color channel. is_jpeg(...) : Convenience function to check if the 'contents' encodes a JPEG image. non_max_suppression(...) : Greedily selects a subset of bounding boxes in descending order of score. non_max_suppression_overlaps(...) : Greedily selects a subset of bounding boxes in descending order of score. non_max_suppression_padded(...) : Greedily selects a subset of bounding boxes in descending order of score. non_max_suppression_with_scores(...) : Greedily selects a subset of bounding boxes in descending order of score. pad_to_bounding_box(...) : Pad image with zeros to the specified height and width . per_image_standardization(...) : Linearly scales each image in image to have mean 0 and variance 1. psnr(...) : Returns the Peak Signal-to-Noise Ratio between a and b. random_brightness(...) : Adjust the brightness of images by a random factor. random_contrast(...) : Adjust the contrast of an image or images by a random factor. random_crop(...) : Randomly crops a tensor to a given size. random_flip_left_right(...) : Randomly flip an image horizontally (left to right). random_flip_up_down(...) : Randomly flips an image vertically (upside down). random_hue(...) : Adjust the hue of RGB images by a random factor. random_jpeg_quality(...) : Randomly changes jpeg encoding quality for inducing jpeg noise. random_saturation(...) : Adjust the saturation of RGB images by a random factor. resize(...) : Resize images to size using the specified method . resize_with_crop_or_pad(...) : Crops and/or pads an image to a target width and height. resize_with_pad(...) : Resizes and pads an image to a target width and height. rgb_to_grayscale(...) : Converts one or more images from RGB to Grayscale. rgb_to_hsv(...) : Converts one or more images from RGB to HSV. rgb_to_yiq(...) : Converts one or more images from RGB to YIQ. rgb_to_yuv(...) : Converts one or more images from RGB to YUV. rot90(...) : Rotate image(s) by 90 degrees. sample_distorted_bounding_box(...) : Generate a single randomly distorted bounding box for an image. sobel_edges(...) : Returns a tensor holding Sobel edge maps. ssim(...) : Computes SSIM index between img1 and img2. ssim_multiscale(...) : Computes the MS-SSIM between img1 and img2. stateless_random_brightness(...) : Adjust the brightness of images by a random factor deterministically. stateless_random_contrast(...) : Adjust the contrast of images by a random factor deterministically. stateless_random_crop(...) : Randomly crops a tensor to a given size in a deterministic manner. stateless_random_flip_left_right(...) : Randomly flip an image horizontally (left to right) deterministically. stateless_random_flip_up_down(...) : Randomly flip an image vertically (upside down) deterministically. stateless_random_hue(...) : Adjust the hue of RGB images by a random factor deterministically. stateless_random_jpeg_quality(...) : Deterministically radomize jpeg encoding quality for inducing jpeg noise. stateless_random_saturation(...) : Adjust the saturation of RGB images by a random factor deterministically. stateless_sample_distorted_bounding_box(...) : Generate a randomly distorted bounding box for an image deterministically. total_variation(...) : Calculate and return the total variation for one or more images. transpose(...) : Transpose image(s) by swapping the height and width dimension. yiq_to_rgb(...) : Converts one or more images from YIQ to RGB. yuv_to_rgb(...) : Converts one or more images from YUV to RGB.


Page: https://www.tensorflow.org/api_docs/python/tf/io
Public API for tf._api.v2.io namespace Modules gfile module: Public API for tf._api.v2.io.gfile namespace Classes class FixedLenFeature : Configuration for parsing a fixed-length input feature. class FixedLenSequenceFeature : Configuration for parsing a variable-length input feature into a Tensor . class RaggedFeature : Configuration for passing a RaggedTensor input feature. class SparseFeature : Configuration for parsing a sparse input feature from an Example . class TFRecordOptions : Options used for manipulating TFRecord files. class TFRecordWriter : A class to write records to a TFRecords file. class VarLenFeature : Configuration for parsing a variable-length input feature. Functions decode_and_crop_jpeg(...) : Decode and Crop a JPEG-encoded image to a uint8 tensor. decode_base64(...) : Decode web-safe base64-encoded strings. decode_bmp(...) : Decode the first frame of a BMP-encoded image to a uint8 tensor. decode_compressed(...) : Decompress strings. decode_csv(...) : Convert CSV records to tensors. Each column maps to one tensor. decode_gif(...) : Decode the frame(s) of a GIF-encoded image to a uint8 tensor. decode_image(...) : Function for decode_bmp , decode_gif , decode_jpeg , and decode_png . decode_jpeg(...) : Decode a JPEG-encoded image to a uint8 tensor. decode_json_example(...) : Convert JSON-encoded Example records to binary protocol buffer strings. decode_png(...) : Decode a PNG-encoded image to a uint8 or uint16 tensor. decode_proto(...) : The op extracts fields from a serialized protocol buffers message into tensors. decode_raw(...) : Convert raw bytes from input tensor into numeric tensors. deserialize_many_sparse(...) : Deserialize and concatenate SparseTensors from a serialized minibatch. encode_base64(...) : Encode strings into web-safe base64 format. encode_jpeg(...) : JPEG-encode an image. encode_png(...) : PNG-encode an image. encode_proto(...) : The op serializes protobuf messages provided in the input tensors. extract_jpeg_shape(...) : Extract the shape information of a JPEG-encoded image. is_jpeg(...) : Convenience function to check if the 'contents' encodes a JPEG image. match_filenames_once(...) : Save the list of files matching pattern, so it is only computed once. matching_files(...) : Returns the set of files matching one or more glob patterns. parse_example(...) : Parses Example protos into a dict of tensors. parse_sequence_example(...) : Parses a batch of SequenceExample protos. parse_single_example(...) : Parses a single Example proto. parse_single_sequence_example(...) : Parses a single SequenceExample proto. parse_tensor(...) : Transforms a serialized tensorflow.TensorProto proto into a Tensor. read_file(...) : Reads the contents of file. serialize_many_sparse(...) : Serialize N -minibatch SparseTensor into an [N, 3] Tensor . serialize_sparse(...) : Serialize a SparseTensor into a 3-vector (1-D Tensor ) object. serialize_tensor(...) : Transforms a Tensor into a serialized TensorProto proto. write_file(...) : Writes contents to the file at input filename . write_graph(...) : Writes a graph proto to a file.


Page: https://www.tensorflow.org/api_docs/python/tf/keras
DO NOT EDIT. This file was autogenerated. Do not edit it by hand,
since your modifications would be overwritten. Modules activations module: DO NOT EDIT. applications module: DO NOT EDIT. backend module: DO NOT EDIT. callbacks module: DO NOT EDIT. config module: DO NOT EDIT. constraints module: DO NOT EDIT. datasets module: DO NOT EDIT. distribution module: DO NOT EDIT. dtype_policies module: DO NOT EDIT. export module: DO NOT EDIT. initializers module: DO NOT EDIT. layers module: DO NOT EDIT. legacy module: DO NOT EDIT. losses module: DO NOT EDIT. metrics module: DO NOT EDIT. mixed_precision module: DO NOT EDIT. models module: DO NOT EDIT. ops module: DO NOT EDIT. optimizers module: DO NOT EDIT. preprocessing module: DO NOT EDIT. quantizers module: DO NOT EDIT. random module: DO NOT EDIT. regularizers module: DO NOT EDIT. tree module: DO NOT EDIT. utils module: DO NOT EDIT. Classes class DTypePolicy : A dtype policy for a Keras layer. class FloatDTypePolicy : A dtype policy for a Keras layer. class Function : Class that encapsulates a computation graph of Keras operations. class Initializer : Initializer base class: all Keras initializers inherit from this class. class InputSpec : Specifies the rank, dtype and shape of every input to a layer. class KerasTensor : Symbolic tensor -- encapsulates a shape and a dtype. class Layer : This is the class from which all layers inherit. class Loss : Loss base class. class Metric : Encapsulates metric logic and state. class Model : A model grouping layers into an object with training/inference features. class Operation class Optimizer : A class for Tensorflow specific optimizer logic. class Quantizer class Regularizer : Regularizer base class. class Sequential : Sequential groups a linear stack of layers into a Model . class StatelessScope : Scope to prevent any update to Keras Variables. class Variable : Represents a backend-agnostic variable in Keras. class name_scope : Creates a sub-namespace for variable paths. Functions Input(...) : Used to instantiate a Keras tensor. device(...) version(...) Other Members version '3.3.3'


Page: https://www.tensorflow.org/api_docs/python/tf/linalg
Public API for tf._api.v2.linalg namespace Modules experimental module: Public API for tf._api.v2.linalg.experimental namespace Classes class LinearOperator : Base class defining a [batch of] linear operator[s]. class LinearOperatorAdjoint : LinearOperator representing the adjoint of another operator. class LinearOperatorBlockDiag : Combines one or more LinearOperators in to a Block Diagonal matrix. class LinearOperatorBlockLowerTriangular : Combines LinearOperators into a blockwise lower-triangular matrix. class LinearOperatorCirculant : LinearOperator acting like a circulant matrix. class LinearOperatorCirculant2D : LinearOperator acting like a block circulant matrix. class LinearOperatorCirculant3D : LinearOperator acting like a nested block circulant matrix. class LinearOperatorComposition : Composes one or more LinearOperators . class LinearOperatorDiag : LinearOperator acting like a [batch] square diagonal matrix. class LinearOperatorFullMatrix : LinearOperator that wraps a [batch] matrix. class LinearOperatorHouseholder : LinearOperator acting like a [batch] of Householder transformations. class LinearOperatorIdentity : LinearOperator acting like a [batch] square identity matrix. class LinearOperatorInversion : LinearOperator representing the inverse of another operator. class LinearOperatorKronecker : Kronecker product between two LinearOperators . class LinearOperatorLowRankUpdate : Perturb a LinearOperator with a rank K update. class LinearOperatorLowerTriangular : LinearOperator acting like a [batch] square lower triangular matrix. class LinearOperatorPermutation : LinearOperator acting like a [batch] of permutation matrices. class LinearOperatorScaledIdentity : LinearOperator acting like a scaled [batch] identity matrix A = c I . class LinearOperatorToeplitz : LinearOperator acting like a [batch] of toeplitz matrices. class LinearOperatorTridiag : LinearOperator acting like a [batch] square tridiagonal matrix. class LinearOperatorZeros : LinearOperator acting like a [batch] zero matrix. Functions adjoint(...) : Transposes the last two dimensions of and conjugates tensor matrix . band_part(...) : Copy a tensor setting everything outside a central band in each innermost matrix to zero. banded_triangular_solve(...) : Solve triangular systems of equations with a banded solver. cholesky(...) : Computes the Cholesky decomposition of one or more square matrices. cholesky_solve(...) : Solves systems of linear eqns A X = RHS , given Cholesky factorizations. cross(...) : Compute the pairwise cross product. det(...) : Computes the determinant of one or more square matrices. diag(...) : Returns a batched diagonal tensor with given batched diagonal values. diag_part(...) : Returns the batched diagonal part of a batched tensor. eig(...) : Computes the eigen decomposition of a batch of matrices. eigh(...) : Computes the eigen decomposition of a batch of self-adjoint matrices. eigh_tridiagonal(...) : Computes the eigenvalues of a Hermitian tridiagonal matrix. eigvals(...) : Computes the eigenvalues of one or more matrices. eigvalsh(...) : Computes the eigenvalues of one or more self-adjoint matrices. einsum(...) : Tensor contraction over specified indices and outer product. expm(...) : Computes the matrix exponential of one or more square matrices. eye(...) : Construct an identity matrix, or a batch of matrices. global_norm(...) : Computes the global norm of multiple tensors. inv(...) : Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes). l2_normalize(...) : Normalizes along dimension axis using an L2 norm. (deprecated arguments) logdet(...) : Computes log of the determinant of a hermitian positive definite matrix. logm(...) : Computes the matrix logarithm of one or more square matrices: lstsq(...) : Solves one or more linear least-squares problems. lu(...) : Computes the LU decomposition of one or more square matrices. lu_matrix_inverse(...) : Computes the inverse given the LU decomposition(s) of one or more matrices. lu_reconstruct(...) : The reconstruct one or more matrices from their LU decomposition(s). lu_solve(...) : Solves systems of linear eqns A X = RHS , given LU factorizations. matmul(...) : Multiplies matrix a by matrix b , producing a * b . matrix_rank(...) : Compute the matrix rank of one or more matrices. matrix_transpose(...) : Transposes last two dimensions of tensor a . matvec(...) : Multiplies matrix a by vector b , producing a * b . norm(...) : Computes the norm of vectors, matrices, and tensors. normalize(...) : Normalizes tensor along dimension axis using specified norm. pinv(...) : Compute the Moore-Penrose pseudo-inverse of one or more matrices. qr(...) : Computes the QR decompositions of one or more matrices. set_diag(...) : Returns a batched matrix tensor with new batched diagonal values. slogdet(...) : Computes the sign and the log of the absolute value of the determinant of solve(...) : Solves systems of linear equations. sqrtm(...) : Computes the matrix square root of one or more square matrices: svd(...) : Computes the singular value decompositions of one or more matrices. tensor_diag(...) : Returns a diagonal tensor with a given diagonal values. tensor_diag_part(...) : Returns the diagonal part of the tensor. tensordot(...) : Tensor contraction of a and b along specified axes and outer product. trace(...) : Compute the trace of a tensor x . triangular_solve(...) : Solve systems of linear equations with upper or lower triangular matrices. tridiagonal_matmul(...) : Multiplies tridiagonal matrix by matrix. tridiagonal_solve(...) : Solves tridiagonal systems of equations.


Page: https://www.tensorflow.org/api_docs/python/tf/lite
Public API for tf._api.v2.lite namespace Modules experimental module: Public API for tf._api.v2.lite.experimental namespace Classes class Interpreter : Interpreter interface for running TensorFlow Lite models. class OpsSet : Enum class defining the sets of ops available to generate TFLite models. class Optimize : Enum defining the optimizations to apply when generating a tflite model. class RepresentativeDataset : Representative dataset used to optimize the model. class TFLiteConverter : Converts a TensorFlow model into TensorFlow Lite model. class TargetSpec : Specification of target device used to optimize the model.


Page: https://www.tensorflow.org/api_docs/python/tf/lookup
Public API for tf._api.v2.lookup namespace Modules experimental module: Public API for tf._api.v2.lookup.experimental namespace Classes class KeyValueTensorInitializer : Table initializers given keys and values tensors. class StaticHashTable : A generic hash table that is immutable once initialized. class StaticVocabularyTable : String to Id table that assigns out-of-vocabulary keys to hash buckets. class TextFileIndex : The key and value content to get from each line. class TextFileInitializer : Table initializers from a text file.


Page: https://www.tensorflow.org/api_docs/python/tf/math
Public API for tf._api.v2.math namespace Modules special module: Public API for tf._api.v2.math.special namespace Functions abs(...) : Computes the absolute value of a tensor. accumulate_n(...) : Returns the element-wise sum of a list of tensors. (deprecated) acos(...) : Computes acos of x element-wise. acosh(...) : Computes inverse hyperbolic cosine of x element-wise. add(...) : Returns x + y element-wise. add_n(...) : Returns the element-wise sum of a list of tensors. angle(...) : Returns the element-wise argument of a complex (or real) tensor. approx_max_k(...) : Returns max k values and their indices of the input operand in an approximate manner. approx_min_k(...) : Returns min k values and their indices of the input operand in an approximate manner. argmax(...) : Returns the index with the largest value across axes of a tensor. argmin(...) : Returns the index with the smallest value across axes of a tensor. asin(...) : Computes the trignometric inverse sine of x element-wise. asinh(...) : Computes inverse hyperbolic sine of x element-wise. atan(...) : Computes the trignometric inverse tangent of x element-wise. atan2(...) : Computes arctangent of y/x element-wise, respecting signs of the arguments. atanh(...) : Computes inverse hyperbolic tangent of x element-wise. bessel_i0(...) : Computes the Bessel i0 function of x element-wise. bessel_i0e(...) : Computes the Bessel i0e function of x element-wise. bessel_i1(...) : Computes the Bessel i1 function of x element-wise. bessel_i1e(...) : Computes the Bessel i1e function of x element-wise. betainc(...) : Compute the regularized incomplete beta integral \(I_x(a, b)\). bincount(...) : Counts the number of occurrences of each value in an integer array. ceil(...) : Return the ceiling of the input, element-wise. confusion_matrix(...) : Computes the confusion matrix from predictions and labels. conj(...) : Returns the complex conjugate of a complex number. cos(...) : Computes cos of x element-wise. cosh(...) : Computes hyperbolic cosine of x element-wise. count_nonzero(...) : Computes number of nonzero elements across dimensions of a tensor. cumprod(...) : Compute the cumulative product of the tensor x along axis . cumsum(...) : Compute the cumulative sum of the tensor x along axis . cumulative_logsumexp(...) : Compute the cumulative log-sum-exp of the tensor x along axis . digamma(...) : Computes Psi, the derivative of Lgamma (the log of the absolute value of divide(...) : Computes Python style division of x by y . divide_no_nan(...) : Computes a safe divide which returns 0 if y (denominator) is zero. equal(...) : Returns the truth value of (x == y) element-wise. erf(...) : Computes the Gauss error function of x element-wise. In statistics, for non-negative values of \(x\), the error function has the following interpretation: for a random variable \(Y\) that is normally distributed with mean 0 and variance \(1/\sqrt{2}\), \(erf(x)\) is the probability that \(Y\) falls in the range \([x, x]\). erfc(...) : Computes the complementary error function of x element-wise. erfcinv(...) : Computes the inverse of complementary error function. erfinv(...) : Compute inverse error function. exp(...) : Computes exponential of x element-wise.  \(y = e^x\). expm1(...) : Computes exp(x) - 1 element-wise. floor(...) : Returns element-wise largest integer not greater than x. floordiv(...) : Divides x / y elementwise, rounding toward the most negative integer. floormod(...) : Returns element-wise remainder of division. greater(...) : Returns the truth value of (x > y) element-wise. greater_equal(...) : Returns the truth value of (x >= y) element-wise. igamma(...) : Compute the lower regularized incomplete Gamma function P(a, x) . igammac(...) : Compute the upper regularized incomplete Gamma function Q(a, x) . imag(...) : Returns the imaginary part of a complex (or real) tensor. in_top_k(...) : Outputs whether the targets are in the top K predictions. invert_permutation(...) : Computes the inverse permutation of a tensor. is_finite(...) : Returns which elements of x are finite. is_inf(...) : Returns which elements of x are Inf. is_nan(...) : Returns which elements of x are NaN. is_non_decreasing(...) : Returns True if x is non-decreasing. is_strictly_increasing(...) : Returns True if x is strictly increasing. l2_normalize(...) : Normalizes along dimension axis using an L2 norm. (deprecated arguments) lbeta(...) : Computes \(ln(|Beta(x)|)\), reducing along the last dimension. less(...) : Returns the truth value of (x < y) element-wise. less_equal(...) : Returns the truth value of (x <= y) element-wise. lgamma(...) : Computes the log of the absolute value of Gamma(x) element-wise. log(...) : Computes natural logarithm of x element-wise. log1p(...) : Computes natural logarithm of (1 + x) element-wise. log_sigmoid(...) : Computes log sigmoid of x element-wise. log_softmax(...) : Computes log softmax activations. logical_and(...) : Returns the truth value of x AND y element-wise. logical_not(...) : Returns the truth value of NOT x element-wise. logical_or(...) : Returns the truth value of x OR y element-wise. logical_xor(...) : Logical XOR function. maximum(...) : Returns the max of x and y (i.e. x > y ? x : y) element-wise. minimum(...) : Returns the min of x and y (i.e. x < y ? x : y) element-wise. mod(...) : Returns element-wise remainder of division. multiply(...) : Returns an element-wise x * y. multiply_no_nan(...) : Computes the product of x and y and returns 0 if the y is zero, even if x is NaN or infinite. ndtri(...) : Compute quantile of Standard Normal. negative(...) : Computes numerical negative value element-wise. nextafter(...) : Returns the next representable value of x1 in the direction of x2 , element-wise. not_equal(...) : Returns the truth value of (x != y) element-wise. polygamma(...) : Compute the polygamma function \(\psi^{(n)}(x)\). polyval(...) : Computes the elementwise value of a polynomial. pow(...) : Computes the power of one value to another. real(...) : Returns the real part of a complex (or real) tensor. reciprocal(...) : Computes the reciprocal of x element-wise. reciprocal_no_nan(...) : Performs a safe reciprocal operation, element wise. reduce_all(...) : Computes tf.math.logical_and of elements across dimensions of a tensor. reduce_any(...) : Computes tf.math.logical_or of elements across dimensions of a tensor. reduce_euclidean_norm(...) : Computes the Euclidean norm of elements across dimensions of a tensor. reduce_logsumexp(...) : Computes log(sum(exp(elements across dimensions of a tensor))). reduce_max(...) : Computes tf.math.maximum of elements across dimensions of a tensor. reduce_mean(...) : Computes the mean of elements across dimensions of a tensor. reduce_min(...) : Computes the tf.math.minimum of elements across dimensions of a tensor. reduce_prod(...) : Computes tf.math.multiply of elements across dimensions of a tensor. reduce_std(...) : Computes the standard deviation of elements across dimensions of a tensor. reduce_sum(...) : Computes the sum of elements across dimensions of a tensor. reduce_variance(...) : Computes the variance of elements across dimensions of a tensor. rint(...) : Returns element-wise integer closest to x. round(...) : Rounds the values of a tensor to the nearest integer, element-wise. rsqrt(...) : Computes reciprocal of square root of x element-wise. scalar_mul(...) : Multiplies a scalar times a Tensor or IndexedSlices object. segment_max(...) : Computes the maximum along segments of a tensor. segment_mean(...) : Computes the mean along segments of a tensor. segment_min(...) : Computes the minimum along segments of a tensor. segment_prod(...) : Computes the product along segments of a tensor. segment_sum(...) : Computes the sum along segments of a tensor. sigmoid(...) : Computes sigmoid of x element-wise. sign(...) : Returns an element-wise indication of the sign of a number. sin(...) : Computes sine of x element-wise. sinh(...) : Computes hyperbolic sine of x element-wise. sobol_sample(...) : Generates points from the Sobol sequence. softmax(...) : Computes softmax activations. softplus(...) : Computes elementwise softplus: softplus(x) = log(exp(x) + 1) . softsign(...) : Computes softsign: features / (abs(features) + 1) . sqrt(...) : Computes element-wise square root of the input tensor. square(...) : Computes square of x element-wise. squared_difference(...) : Returns conj(x - y)(x - y) element-wise. subtract(...) : Returns x - y element-wise. tan(...) : Computes tan of x element-wise. tanh(...) : Computes hyperbolic tangent of x element-wise. top_k(...) : Finds values and indices of the k largest entries for the last dimension. truediv(...) : Divides x / y elementwise (using Python 3 division operator semantics). unsorted_segment_max(...) : Computes the maximum along segments of a tensor. unsorted_segment_mean(...) : Computes the mean along segments of a tensor. unsorted_segment_min(...) : Computes the minimum along segments of a tensor. unsorted_segment_prod(...) : Computes the product along segments of a tensor. unsorted_segment_sqrt_n(...) : Computes the sum along segments of a tensor divided by the sqrt(N). unsorted_segment_sum(...) : Computes the sum along segments of a tensor. xdivy(...) : Computes x / y . xlog1py(...) : Compute x * log1p(y). xlogy(...) : Returns 0 if x == 0, and x * log(y) otherwise, elementwise. zero_fraction(...) : Returns the fraction of zeros in value . zeta(...) : Compute the Hurwitz zeta function \(\zeta(x, q)\).


Page: https://www.tensorflow.org/api_docs/python/tf/mlir
Public API for tf._api.v2.mlir namespace Modules experimental module: Public API for tf._api.v2.mlir.experimental namespace


Page: https://www.tensorflow.org/api_docs/python/tf/nest
Public API for tf._api.v2.nest namespace Functions assert_same_structure(...) : Asserts that two structures are nested in the same way. flatten(...) : Returns a flat list from a given structure. is_nested(...) : Returns true if its input is a nested structure. map_structure(...) : Creates a new structure by applying func to each atom in structure . pack_sequence_as(...) : Returns a given flattened sequence packed into a given structure.


Page: https://www.tensorflow.org/api_docs/python/tf/nn
Public API for tf._api.v2.nn namespace Modules experimental module: Public API for tf._api.v2.nn.experimental namespace Classes class RNNCellDeviceWrapper : Operator that ensures an RNNCell runs on a particular device. (deprecated) class RNNCellDropoutWrapper : Operator adding dropout to inputs and outputs of the given cell. (deprecated) class RNNCellResidualWrapper : RNNCell wrapper that ensures cell inputs are added to the outputs. (deprecated) Functions all_candidate_sampler(...) : Generate the set of all classes. approx_max_k(...) : Returns max k values and their indices of the input operand in an approximate manner. approx_min_k(...) : Returns min k values and their indices of the input operand in an approximate manner. atrous_conv2d(...) : Atrous convolution (a.k.a. convolution with holes or dilated convolution). atrous_conv2d_transpose(...) : The transpose of atrous_conv2d . avg_pool(...) : Performs the avg pooling on the input. avg_pool1d(...) : Performs the average pooling on the input. avg_pool2d(...) : Performs the average pooling on the input. avg_pool3d(...) : Performs the average pooling on the input. batch_norm_with_global_normalization(...) : Batch normalization. batch_normalization(...) : Batch normalization. bias_add(...) : Adds bias to value . collapse_repeated(...) : Merge repeated labels into single labels. compute_accidental_hits(...) : Compute the position ids in sampled_candidates matching true_classes . compute_average_loss(...) : Scales per-example losses with sample_weights and computes their average. conv1d(...) : Computes a 1-D convolution given 3-D input and filter tensors. conv1d_transpose(...) : The transpose of conv1d . conv2d(...) : Computes a 2-D convolution given input and 4-D filters tensors. conv2d_transpose(...) : The transpose of conv2d . conv3d(...) : Computes a 3-D convolution given 5-D input and filters tensors. conv3d_transpose(...) : The transpose of conv3d . conv_transpose(...) : The transpose of convolution . convolution(...) : Computes sums of N-D convolutions (actually cross-correlation). crelu(...) : Computes Concatenated ReLU. ctc_beam_search_decoder(...) : Performs beam search decoding on the logits given in input. ctc_greedy_decoder(...) : Performs greedy decoding on the logits given in input (best path). ctc_loss(...) : Computes CTC (Connectionist Temporal Classification) loss. ctc_unique_labels(...) : Get unique labels and indices for batched labels for tf.nn.ctc_loss . depth_to_space(...) : DepthToSpace for tensors of type T. depthwise_conv2d(...) : Depthwise 2-D convolution. depthwise_conv2d_backprop_filter(...) : Computes the gradients of depthwise convolution with respect to the filter. depthwise_conv2d_backprop_input(...) : Computes the gradients of depthwise convolution with respect to the input. dilation2d(...) : Computes the grayscale dilation of 4-D input and 3-D filters tensors. dropout(...) : Computes dropout: randomly sets elements to zero to prevent overfitting. elu(...) : Computes the exponential linear function. embedding_lookup(...) : Looks up embeddings for the given ids from a list of tensors. embedding_lookup_sparse(...) : Looks up embeddings for the given ids and weights from a list of tensors. erosion2d(...) : Computes the grayscale erosion of 4-D value and 3-D filters tensors. fixed_unigram_candidate_sampler(...) : Samples a set of classes using the provided (fixed) base distribution. fractional_avg_pool(...) : Performs fractional average pooling on the input. fractional_max_pool(...) : Performs fractional max pooling on the input. gelu(...) : Compute the Gaussian Error Linear Unit (GELU) activation function. in_top_k(...) : Outputs whether the targets are in the top K predictions. isotonic_regression(...) : Solves isotonic regression problems along the given axis. l2_loss(...) : L2 Loss. l2_normalize(...) : Normalizes along dimension axis using an L2 norm. (deprecated arguments) leaky_relu(...) : Compute the Leaky ReLU activation function. learned_unigram_candidate_sampler(...) : Samples a set of classes from a distribution learned during training. local_response_normalization(...) : Local Response Normalization. log_poisson_loss(...) : Computes log Poisson loss given log_input . log_softmax(...) : Computes log softmax activations. lrn(...) : Local Response Normalization. max_pool(...) : Performs max pooling on the input. max_pool1d(...) : Performs the max pooling on the input. max_pool2d(...) : Performs max pooling on 2D spatial data such as images. max_pool3d(...) : Performs the max pooling on the input. max_pool_with_argmax(...) : Performs max pooling on the input and outputs both max values and indices. moments(...) : Calculates the mean and variance of x . nce_loss(...) : Computes and returns the noise-contrastive estimation training loss. normalize_moments(...) : Calculate the mean and variance of based on the sufficient statistics. pool(...) : Performs an N-D pooling operation. relu(...) : Computes rectified linear: max(features, 0) . relu6(...) : Computes Rectified Linear 6: min(max(features, 0), 6) . safe_embedding_lookup_sparse(...) : Lookup embedding results, accounting for invalid IDs and empty features. sampled_softmax_loss(...) : Computes and returns the sampled softmax training loss. scale_regularization_loss(...) : Scales the sum of the given regularization losses by number of replicas. selu(...) : Computes scaled exponential linear: scale * alpha * (exp(features) - 1) separable_conv2d(...) : 2-D convolution with separable filters. sigmoid(...) : Computes sigmoid of x element-wise. sigmoid_cross_entropy_with_logits(...) : Computes sigmoid cross entropy given logits . silu(...) : Computes the SiLU or Swish activation function: x * sigmoid(beta * x) . softmax(...) : Computes softmax activations. softmax_cross_entropy_with_logits(...) : Computes softmax cross entropy between logits and labels . softplus(...) : Computes elementwise softplus: softplus(x) = log(exp(x) + 1) . softsign(...) : Computes softsign: features / (abs(features) + 1) . space_to_batch(...) : SpaceToBatch for N-D tensors of type T. space_to_depth(...) : SpaceToDepth for tensors of type T. sparse_softmax_cross_entropy_with_logits(...) : Computes sparse softmax cross entropy between logits and labels . sufficient_statistics(...) : Calculate the sufficient statistics for the mean and variance of x . swish(...) : Computes the SiLU or Swish activation function: x * sigmoid(beta * x) . tanh(...) : Computes hyperbolic tangent of x element-wise. top_k(...) : Finds values and indices of the k largest entries for the last dimension. weighted_cross_entropy_with_logits(...) : Computes a weighted cross entropy. weighted_moments(...) : Returns the frequency-weighted mean and variance of x . with_space_to_batch(...) : Performs op on the space-to-batch representation of input . zero_fraction(...) : Returns the fraction of zeros in value .


Page: https://www.tensorflow.org/api_docs/python/tf/profiler
Public API for tf._api.v2.profiler namespace Modules experimental module: Public API for tf._api.v2.profiler.experimental namespace


Page: https://www.tensorflow.org/api_docs/python/tf/quantization
Public API for tf._api.v2.quantization namespace Modules experimental module: Public API for tf._api.v2.quantization.experimental namespace Functions dequantize(...) : Dequantize the 'input' tensor into a float or bfloat16 Tensor. fake_quant_with_min_max_args(...) : Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same shape and type. fake_quant_with_min_max_args_gradient(...) : Compute gradients for a FakeQuantWithMinMaxArgs operation. fake_quant_with_min_max_vars(...) : Fake-quantize the 'inputs' tensor of type float via global float scalars fake_quant_with_min_max_vars_gradient(...) : Compute gradients for a FakeQuantWithMinMaxVars operation. fake_quant_with_min_max_vars_per_channel(...) : Fake-quantize the 'inputs' tensor of type float via per-channel floats fake_quant_with_min_max_vars_per_channel_gradient(...) : Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation. quantize(...) : Quantize the 'input' tensor of type float to 'output' tensor of type 'T'. quantize_and_dequantize(...) : Quantizes then dequantizes a tensor. (deprecated) quantize_and_dequantize_v2(...) : Quantizes then dequantizes a tensor. quantized_concat(...) : Concatenates quantized tensors along one dimension.


Page: https://www.tensorflow.org/api_docs/python/tf/queue
Public API for tf._api.v2.queue namespace Classes class FIFOQueue : A queue implementation that dequeues elements in first-in first-out order. class PaddingFIFOQueue : A FIFOQueue that supports batching variable-sized tensors by padding. class PriorityQueue : A queue implementation that dequeues elements in prioritized order. class QueueBase : Base class for queue implementations. class RandomShuffleQueue : A queue implementation that dequeues elements in a random order.


Page: https://www.tensorflow.org/api_docs/python/tf/ragged
Public API for tf._api.v2.ragged namespace Functions boolean_mask(...) : Applies a boolean mask to data without flattening the mask dimensions. constant(...) : Constructs a constant RaggedTensor from a nested Python list. cross(...) : Generates feature cross from a list of tensors. cross_hashed(...) : Generates hashed feature cross from a list of tensors. map_flat_values(...) : Applies op to the flat_values of one or more RaggedTensors. range(...) : Returns a RaggedTensor containing the specified sequences of numbers. row_splits_to_segment_ids(...) : Generates the segmentation corresponding to a RaggedTensor row_splits . segment_ids_to_row_splits(...) : Generates the RaggedTensor row_splits corresponding to a segmentation. stack(...) : Stacks a list of rank- R tensors into one rank- (R+1) RaggedTensor . stack_dynamic_partitions(...) : Stacks dynamic partitions of a Tensor or RaggedTensor.


Page: https://www.tensorflow.org/api_docs/python/tf/random
Public API for tf._api.v2.random namespace Modules experimental module: Public API for tf._api.v2.random.experimental namespace Classes class Algorithm : A random-number-generation (RNG) algorithm. class Generator : Random-number generator. Functions all_candidate_sampler(...) : Generate the set of all classes. categorical(...) : Draws samples from a categorical distribution. create_rng_state(...) : Creates a RNG state from an integer or a vector. fixed_unigram_candidate_sampler(...) : Samples a set of classes using the provided (fixed) base distribution. fold_in(...) : Folds in data to an RNG seed to form a new RNG seed. gamma(...) : Draws shape samples from each of the given Gamma distribution(s). get_global_generator(...) : Retrieves the global generator. learned_unigram_candidate_sampler(...) : Samples a set of classes from a distribution learned during training. log_uniform_candidate_sampler(...) : Samples a set of classes using a log-uniform (Zipfian) base distribution. normal(...) : Outputs random values from a normal distribution. poisson(...) : Draws shape samples from each of the given Poisson distribution(s). set_global_generator(...) : Replaces the global generator with another Generator object. set_seed(...) : Sets the global random seed. shuffle(...) : Randomly shuffles a tensor along its first dimension. split(...) : Splits an RNG seed into num new seeds by adding a leading axis. stateless_binomial(...) : Outputs deterministic pseudorandom values from a binomial distribution. stateless_categorical(...) : Draws deterministic pseudorandom samples from a categorical distribution. stateless_gamma(...) : Outputs deterministic pseudorandom values from a gamma distribution. stateless_normal(...) : Outputs deterministic pseudorandom values from a normal distribution. stateless_parameterized_truncated_normal(...) : Outputs random values from a truncated normal distribution. stateless_poisson(...) : Outputs deterministic pseudorandom values from a Poisson distribution. stateless_truncated_normal(...) : Outputs deterministic pseudorandom values, truncated normally distributed. stateless_uniform(...) : Outputs deterministic pseudorandom values from a uniform distribution. truncated_normal(...) : Outputs random values from a truncated normal distribution. uniform(...) : Outputs random values from a uniform distribution. uniform_candidate_sampler(...) : Samples a set of classes using a uniform base distribution.


Page: https://www.tensorflow.org/api_docs/python/tf/raw_ops
Public API for tf._api.v2.raw_ops namespace Note: tf.raw_ops provides direct/low level access to all TensorFlow ops.
See the RFC for details. Unless you are library writer, you likely do not need to use
these ops directly. Op Name Has Gradient GPU XLA Support Abort   Abs   AccumulateNV2   AccumulatorApplyGradient   AccumulatorNumAccumulated   AccumulatorSetGlobalStep   AccumulatorTakeGradient   Acos   Acosh   Add   AddManySparseToTensorsMap   AddN   AddSparseToTensorsMap   AddV2   AdjustContrast   AdjustContrastv2   AdjustHue   AdjustSaturation   All   AllCandidateSampler   AllToAll   Angle   AnonymousHashTable   AnonymousIterator   AnonymousIteratorV2   AnonymousIteratorV3   AnonymousMemoryCache   AnonymousMultiDeviceIterator   AnonymousMultiDeviceIteratorV3   AnonymousMutableDenseHashTable   AnonymousMutableHashTable   AnonymousMutableHashTableOfTensors   AnonymousRandomSeedGenerator   AnonymousSeedGenerator   Any   ApplyAdaMax   ApplyAdadelta   ApplyAdagrad   ApplyAdagradDA   ApplyAdagradV2   ApplyAdam   ApplyAddSign   ApplyCenteredRMSProp   ApplyFtrl   ApplyFtrlV2   ApplyGradientDescent   ApplyMomentum   ApplyPowerSign   ApplyProximalAdagrad   ApplyProximalGradientDescent   ApplyRMSProp   ApproxTopK   ApproximateEqual   ArgMax   ArgMin   AsString   Asin   Asinh   Assert   AssertCardinalityDataset   AssertNextDataset   AssertPrevDataset   Assign   AssignAdd   AssignAddVariableOp   AssignSub   AssignSubVariableOp   AssignVariableOp   AssignVariableXlaConcatND   Atan   Atan2   Atanh   AudioSpectrogram   AudioSummary   AudioSummaryV2   AutoShardDataset   AvgPool   AvgPool3D   AvgPool3DGrad   AvgPoolGrad   BandedTriangularSolve   Barrier   BarrierClose   BarrierIncompleteSize   BarrierInsertMany   BarrierReadySize   BarrierTakeMany   Batch   BatchCholesky   BatchCholeskyGrad   BatchDataset   BatchDatasetV2   BatchFFT   BatchFFT2D   BatchFFT3D   BatchFunction   BatchIFFT   BatchIFFT2D   BatchIFFT3D   BatchMatMul   BatchMatMulV2   BatchMatMulV3   BatchMatrixBandPart   BatchMatrixDeterminant   BatchMatrixDiag   BatchMatrixDiagPart   BatchMatrixInverse   BatchMatrixSetDiag   BatchMatrixSolve   BatchMatrixSolveLs   BatchMatrixTriangularSolve   BatchNormWithGlobalNormalization   BatchNormWithGlobalNormalizationGrad   BatchSelfAdjointEig   BatchSelfAdjointEigV2   BatchSvd   BatchToSpace   BatchToSpaceND   BesselI0   BesselI0e   BesselI1   BesselI1e   BesselJ0   BesselJ1   BesselK0   BesselK0e   BesselK1   BesselK1e   BesselY0   BesselY1   Betainc   BiasAdd   BiasAddGrad   BiasAddV1   Bincount   Bitcast   BitwiseAnd   BitwiseOr   BitwiseXor   BlockLSTM   BlockLSTMGrad   BlockLSTMGradV2   BlockLSTMV2   BoostedTreesAggregateStats   BoostedTreesBucketize   BoostedTreesCalculateBestFeatureSplit   BoostedTreesCalculateBestFeatureSplitV2   BoostedTreesCalculateBestGainsPerFeature   BoostedTreesCenterBias   BoostedTreesCreateEnsemble   BoostedTreesCreateQuantileStreamResource   BoostedTreesDeserializeEnsemble   BoostedTreesEnsembleResourceHandleOp   BoostedTreesExampleDebugOutputs   BoostedTreesFlushQuantileSummaries   BoostedTreesGetEnsembleStates   BoostedTreesMakeQuantileSummaries   BoostedTreesMakeStatsSummary   BoostedTreesPredict   BoostedTreesQuantileStreamResourceAddSummaries   BoostedTreesQuantileStreamResourceDeserialize   BoostedTreesQuantileStreamResourceFlush   BoostedTreesQuantileStreamResourceGetBucketBoundaries   BoostedTreesQuantileStreamResourceHandleOp   BoostedTreesSerializeEnsemble   BoostedTreesSparseAggregateStats   BoostedTreesSparseCalculateBestFeatureSplit   BoostedTreesTrainingPredict   BoostedTreesUpdateEnsemble   BoostedTreesUpdateEnsembleV2   BroadcastArgs   BroadcastGradientArgs   BroadcastTo   Bucketize   BytesProducedStatsDataset   CSRSparseMatrixComponents   CSRSparseMatrixToDense   CSRSparseMatrixToSparseTensor   CSVDataset   CSVDatasetV2   CTCBeamSearchDecoder   CTCGreedyDecoder   CTCLoss   CTCLossV2   CacheDataset   CacheDatasetV2   Case   Cast   Ceil   CheckNumerics   CheckNumericsV2   Cholesky   CholeskyGrad   ChooseFastestBranchDataset   ChooseFastestDataset   ClipByValue   CloseSummaryWriter   CollectiveAllToAllV2   CollectiveAllToAllV3   CollectiveAssignGroupV2   CollectiveBcastRecv   CollectiveBcastRecvV2   CollectiveBcastSend   CollectiveBcastSendV2   CollectiveGather   CollectiveGatherV2   CollectiveInitializeCommunicator   CollectivePermute   CollectiveReduce   CollectiveReduceScatterV2   CollectiveReduceV2   CollectiveReduceV3   CombinedNonMaxSuppression   Complex   ComplexAbs   CompositeTensorVariantFromComponents   CompositeTensorVariantToComponents   CompressElement   ComputeAccidentalHits   ComputeBatchSize   Concat   ConcatOffset   ConcatV2   ConcatenateDataset   ConditionalAccumulator   ConfigureDistributedTPU   ConfigureTPUEmbedding   Conj   ConjugateTranspose   Const   ConsumeMutexLock   ControlTrigger   Conv   Conv2D   Conv2DBackpropFilter   Conv2DBackpropFilterV2   Conv2DBackpropInput   Conv2DBackpropInputV2   Conv3D   Conv3DBackpropFilter   Conv3DBackpropFilterV2   Conv3DBackpropInput   Conv3DBackpropInputV2   ConvertToCooTensor   Copy   CopyHost   Cos   Cosh   CountUpTo   CreateSummaryDbWriter   CreateSummaryFileWriter   CropAndResize   CropAndResizeGradBoxes   CropAndResizeGradImage   Cross   CrossReplicaSum   CudnnRNN   CudnnRNNBackprop   CudnnRNNBackpropV2   CudnnRNNBackpropV3   CudnnRNNCanonicalToParams   CudnnRNNCanonicalToParamsV2   CudnnRNNParamsSize   CudnnRNNParamsToCanonical   CudnnRNNParamsToCanonicalV2   CudnnRNNV2   CudnnRNNV3   Cumprod   Cumsum   CumulativeLogsumexp   DataFormatDimMap   DataFormatVecPermute   DataServiceDataset   DataServiceDatasetV2   DataServiceDatasetV3   DataServiceDatasetV4   DatasetCardinality   DatasetFingerprint   DatasetFromGraph   DatasetToGraph   DatasetToGraphV2   DatasetToSingleElement   DatasetToTFRecord   Dawsn   DebugGradientIdentity   DebugGradientRefIdentity   DebugIdentity   DebugIdentityV2   DebugIdentityV3   DebugNanCount   DebugNumericSummary   DebugNumericSummaryV2   DecodeAndCropJpeg   DecodeBase64   DecodeBmp   DecodeCSV   DecodeCompressed   DecodeGif   DecodeImage   DecodeJSONExample   DecodeJpeg   DecodePaddedRaw   DecodePng   DecodeProtoV2   DecodeRaw   DecodeWav   DeepCopy   DeleteIterator   DeleteMemoryCache   DeleteMultiDeviceIterator   DeleteRandomSeedGenerator   DeleteSeedGenerator   DeleteSessionTensor   DenseBincount   DenseCountSparseOutput   DenseToCSRSparseMatrix   DenseToDenseSetOperation   DenseToSparseBatchDataset   DenseToSparseSetOperation   DepthToSpace   DepthwiseConv2dNative   DepthwiseConv2dNativeBackpropFilter   DepthwiseConv2dNativeBackpropInput   Dequantize   DeserializeIterator   DeserializeManySparse   DeserializeSparse   DestroyResourceOp   DestroyTemporaryVariable   DeviceIndex   Diag   DiagPart   Digamma   Dilation2D   Dilation2DBackpropFilter   Dilation2DBackpropInput   DirectedInterleaveDataset   DisableCopyOnRead   DistributedSave   Div   DivNoNan   DrawBoundingBoxes   DrawBoundingBoxesV2   DummyIterationCounter   DummyMemoryCache   DummySeedGenerator   DynamicEnqueueTPUEmbeddingArbitraryTensorBatch   DynamicEnqueueTPUEmbeddingRaggedTensorBatch   DynamicPartition   DynamicStitch   EagerPyFunc   EditDistance   Eig   Einsum   Elu   EluGrad   Empty   EmptyTensorList   EmptyTensorMap   EncodeBase64   EncodeJpeg   EncodeJpegVariableQuality   EncodePng   EncodeProto   EncodeWav   EnqueueTPUEmbeddingArbitraryTensorBatch   EnqueueTPUEmbeddingIntegerBatch   EnqueueTPUEmbeddingRaggedTensorBatch   EnqueueTPUEmbeddingSparseBatch   EnqueueTPUEmbeddingSparseTensorBatch   EnsureShape   Enter   Equal   Erf   Erfc   Erfinv   EuclideanNorm   Exit   Exp   ExpandDims   ExperimentalAssertNextDataset   ExperimentalAutoShardDataset   ExperimentalBytesProducedStatsDataset   ExperimentalCSVDataset   ExperimentalChooseFastestDataset   ExperimentalDatasetCardinality   ExperimentalDatasetToTFRecord   ExperimentalDenseToSparseBatchDataset   ExperimentalDirectedInterleaveDataset   ExperimentalGroupByReducerDataset   ExperimentalGroupByWindowDataset   ExperimentalIgnoreErrorsDataset   ExperimentalIteratorGetDevice   ExperimentalLMDBDataset   ExperimentalLatencyStatsDataset   ExperimentalMapAndBatchDataset   ExperimentalMapDataset   ExperimentalMatchingFilesDataset   ExperimentalMaxIntraOpParallelismDataset   ExperimentalNonSerializableDataset   ExperimentalParallelInterleaveDataset   ExperimentalParseExampleDataset   ExperimentalPrivateThreadPoolDataset   ExperimentalRandomDataset   ExperimentalRebatchDataset   ExperimentalScanDataset   ExperimentalSetStatsAggregatorDataset   ExperimentalSleepDataset   ExperimentalSlidingWindowDataset   ExperimentalSqlDataset   ExperimentalStatsAggregatorHandle   ExperimentalStatsAggregatorSummary   ExperimentalTakeWhileDataset   ExperimentalThreadPoolDataset   ExperimentalThreadPoolHandle   ExperimentalUnbatchDataset   ExperimentalUniqueDataset   Expint   Expm1   ExtractGlimpse   ExtractGlimpseV2   ExtractImagePatches   ExtractJpegShape   ExtractVolumePatches   FFT   FFT2D   FFT3D   FFTND   FIFOQueue   FIFOQueueV2   Fact   FakeParam   FakeQuantWithMinMaxArgs   FakeQuantWithMinMaxArgsGradient   FakeQuantWithMinMaxVars   FakeQuantWithMinMaxVarsGradient   FakeQuantWithMinMaxVarsPerChannel   FakeQuantWithMinMaxVarsPerChannelGradient   FakeQueue   FileSystemSetConfiguration   Fill   FilterByLastComponentDataset   FilterDataset   FinalizeDataset   Fingerprint   FixedLengthRecordDataset   FixedLengthRecordDatasetV2   FixedLengthRecordReader   FixedLengthRecordReaderV2   FixedUnigramCandidateSampler   FlatMapDataset   Floor   FloorDiv   FloorMod   FlushSummaryWriter   For   FractionalAvgPool   FractionalAvgPoolGrad   FractionalMaxPool   FractionalMaxPoolGrad   FresnelCos   FresnelSin   FusedBatchNorm   FusedBatchNormGrad   FusedBatchNormGradV2   FusedBatchNormGradV3   FusedBatchNormV2   FusedBatchNormV3   FusedPadConv2D   FusedResizeAndPadConv2D   GRUBlockCell   GRUBlockCellGrad   Gather   GatherNd   GatherV2   GenerateBoundingBoxProposals   GenerateVocabRemapping   GeneratorDataset   GetElementAtIndex   GetMinibatchSplitsWithPhysicalReplica   GetMinibatchesInCsrWithPhysicalReplica   GetOptions   GetSessionHandle   GetSessionHandleV2   GetSessionTensor   GlobalIterId   Greater   GreaterEqual   GroupByReducerDataset   GroupByWindowDataset   GuaranteeConst   HSVToRGB   HashTable   HashTableV2   HistogramFixedWidth   HistogramSummary   IFFT   IFFT2D   IFFT3D   IFFTND   IRFFT   IRFFT2D   IRFFT3D   IRFFTND   Identity   IdentityN   IdentityReader   IdentityReaderV2   If   Igamma   IgammaGradA   Igammac   IgnoreErrorsDataset   Imag   ImageProjectiveTransformV2   ImageProjectiveTransformV3   ImageSummary   ImmutableConst   ImportEvent   InTopK   InTopKV2   InfeedDequeue   InfeedDequeueTuple   InfeedEnqueue   InfeedEnqueuePrelinearizedBuffer   InfeedEnqueueTuple   InitializeTable   InitializeTableFromDataset   InitializeTableFromTextFile   InitializeTableFromTextFileV2   InitializeTableV2   InplaceAdd   InplaceSub   InplaceUpdate   InterleaveDataset   Inv   InvGrad   Invert   InvertPermutation   IsBoostedTreesEnsembleInitialized   IsBoostedTreesQuantileStreamResourceInitialized   IsFinite   IsInf   IsNan   IsTPUEmbeddingInitialized   IsVariableInitialized   IsotonicRegression   Iterator   IteratorFromStringHandle   IteratorFromStringHandleV2   IteratorGetDevice   IteratorGetNext   IteratorGetNextAsOptional   IteratorGetNextSync   IteratorToStringHandle   IteratorV2   KMC2ChainInitialization   KmeansPlusPlusInitialization   L2Loss   LMDBDataset   LMDBReader   LRN   LRNGrad   LSTMBlockCell   LSTMBlockCellGrad   LatencyStatsDataset   LeakyRelu   LeakyReluGrad   LearnedUnigramCandidateSampler   LeftShift   LegacyParallelInterleaveDatasetV2   Less   LessEqual   Lgamma   LinSpace   ListDataset   ListDiff   ListSnapshotChunksDataset   LoadAndRemapMatrix   LoadDataset   LoadTPUEmbeddingADAMParameters   LoadTPUEmbeddingAdadeltaParameters   LoadTPUEmbeddingAdagradMomentumParameters   LoadTPUEmbeddingAdagradParameters   LoadTPUEmbeddingCenteredRMSPropParameters   LoadTPUEmbeddingFTRLParameters   LoadTPUEmbeddingFrequencyEstimatorParameters   LoadTPUEmbeddingMDLAdagradLightParameters   LoadTPUEmbeddingMomentumParameters   LoadTPUEmbeddingProximalAdagradParameters   LoadTPUEmbeddingProximalYogiParameters   LoadTPUEmbeddingRMSPropParameters   LoadTPUEmbeddingStochasticGradientDescentParameters   Log   Log1p   LogMatrixDeterminant   LogSoftmax   LogUniformCandidateSampler   LogicalAnd   LogicalNot   LogicalOr   LookupTableExport   LookupTableExportV2   LookupTableFind   LookupTableFindV2   LookupTableImport   LookupTableImportV2   LookupTableInsert   LookupTableInsertV2   LookupTableRemoveV2   LookupTableSize   LookupTableSizeV2   LoopCond   LowerBound   Lu   MakeIterator   MapAndBatchDataset   MapClear   MapDataset   MapDefun   MapIncompleteSize   MapPeek   MapSize   MapStage   MapUnstage   MapUnstageNoKey   MatMul   MatchingFiles   MatchingFilesDataset   MatrixBandPart   MatrixDeterminant   MatrixDiag   MatrixDiagPart   MatrixDiagPartV2   MatrixDiagPartV3   MatrixDiagV2   MatrixDiagV3   MatrixExponential   MatrixInverse   MatrixLogarithm   MatrixSetDiag   MatrixSetDiagV2   MatrixSetDiagV3   MatrixSolve   MatrixSolveLs   MatrixSquareRoot   MatrixTriangularSolve   Max   MaxIntraOpParallelismDataset   MaxPool   MaxPool3D   MaxPool3DGrad   MaxPool3DGradGrad   MaxPoolGrad   MaxPoolGradGrad   MaxPoolGradGradV2   MaxPoolGradGradWithArgmax   MaxPoolGradV2   MaxPoolGradWithArgmax   MaxPoolV2   MaxPoolWithArgmax   Maximum   Mean   Merge   MergeSummary   MergeV2Checkpoints   Mfcc   Min   Minimum   MirrorPad   MirrorPadGrad   Mod   ModelDataset   Mul   MulNoNan   MultiDeviceIterator   MultiDeviceIteratorFromStringHandle   MultiDeviceIteratorGetNextFromShard   MultiDeviceIteratorInit   MultiDeviceIteratorToStringHandle   Multinomial   MutableDenseHashTable   MutableDenseHashTableV2   MutableHashTable   MutableHashTableOfTensors   MutableHashTableOfTensorsV2   MutableHashTableV2   MutexLock   MutexV2   NcclAllReduce   NcclBroadcast   NcclReduce   Ndtri   NearestNeighbors   Neg   NextAfter   NextIteration   NoOp   NonDeterministicInts   NonMaxSuppression   NonMaxSuppressionV2   NonMaxSuppressionV3   NonMaxSuppressionV4   NonMaxSuppressionV5   NonMaxSuppressionWithOverlaps   NonSerializableDataset   NotEqual   NthElement   OneHot   OneShotIterator   OnesLike   OptimizeDataset   OptimizeDatasetV2   OptionalFromValue   OptionalGetValue   OptionalHasValue   OptionalNone   OptionsDataset   OrderedMapClear   OrderedMapIncompleteSize   OrderedMapPeek   OrderedMapSize   OrderedMapStage   OrderedMapUnstage   OrderedMapUnstageNoKey   OutfeedDequeue   OutfeedDequeueTuple   OutfeedDequeueTupleV2   OutfeedDequeueV2   OutfeedEnqueue   OutfeedEnqueueTuple   Pack   Pad   PadV2   PaddedBatchDataset   PaddedBatchDatasetV2   PaddingFIFOQueue   PaddingFIFOQueueV2   ParallelBatchDataset   ParallelConcat   ParallelDynamicStitch   ParallelFilterDataset   ParallelInterleaveDataset   ParallelInterleaveDatasetV2   ParallelInterleaveDatasetV3   ParallelInterleaveDatasetV4   ParallelMapDataset   ParallelMapDatasetV2   ParameterizedTruncatedNormal   ParseExample   ParseExampleDataset   ParseExampleDatasetV2   ParseExampleV2   ParseSequenceExample   ParseSequenceExampleV2   ParseSingleExample   ParseSingleSequenceExample   ParseTensor   PartitionedCall   Placeholder   PlaceholderV2   PlaceholderWithDefault   Polygamma   PopulationCount   Pow   PrefetchDataset   Prelinearize   PrelinearizeTuple   PreventGradient   Print   PrintV2   PriorityQueue   PriorityQueueV2   PrivateThreadPoolDataset   Prod   PyFunc   PyFuncStateless   Qr   QuantizeAndDequantize   QuantizeAndDequantizeV2   QuantizeAndDequantizeV3   QuantizeAndDequantizeV4   QuantizeAndDequantizeV4Grad   QuantizeDownAndShrinkRange   QuantizeV2   QuantizedAdd   QuantizedAvgPool   QuantizedBatchNormWithGlobalNormalization   QuantizedBiasAdd   QuantizedConcat   QuantizedConv2D   QuantizedConv2DAndRelu   QuantizedConv2DAndReluAndRequantize   QuantizedConv2DAndRequantize   QuantizedConv2DPerChannel   QuantizedConv2DWithBias   QuantizedConv2DWithBiasAndRelu   QuantizedConv2DWithBiasAndReluAndRequantize   QuantizedConv2DWithBiasAndRequantize   QuantizedConv2DWithBiasSignedSumAndReluAndRequantize   QuantizedConv2DWithBiasSumAndRelu   QuantizedConv2DWithBiasSumAndReluAndRequantize   QuantizedDepthwiseConv2D   QuantizedDepthwiseConv2DWithBias   QuantizedDepthwiseConv2DWithBiasAndRelu   QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize   QuantizedInstanceNorm   QuantizedMatMul   QuantizedMatMulWithBias   QuantizedMatMulWithBiasAndDequantize   QuantizedMatMulWithBiasAndRelu   QuantizedMatMulWithBiasAndReluAndRequantize   QuantizedMatMulWithBiasAndRequantize   QuantizedMaxPool   QuantizedMul   QuantizedRelu   QuantizedRelu6   QuantizedReluX   QuantizedReshape   QuantizedResizeBilinear   QueueClose   QueueCloseV2   QueueDequeue   QueueDequeueMany   QueueDequeueManyV2   QueueDequeueUpTo   QueueDequeueUpToV2   QueueDequeueV2   QueueEnqueue   QueueEnqueueMany   QueueEnqueueManyV2   QueueEnqueueV2   QueueIsClosed   QueueIsClosedV2   QueueSize   QueueSizeV2   RFFT   RFFT2D   RFFT3D   RFFTND   RGBToHSV   RaggedBincount   RaggedCountSparseOutput   RaggedCross   RaggedFillEmptyRows   RaggedFillEmptyRowsGrad   RaggedGather   RaggedRange   RaggedTensorFromVariant   RaggedTensorToSparse   RaggedTensorToTensor   RaggedTensorToVariant   RaggedTensorToVariantGradient   RandomCrop   RandomDataset   RandomDatasetV2   RandomGamma   RandomGammaGrad   RandomIndexShuffle   RandomPoisson   RandomPoissonV2   RandomShuffle   RandomShuffleQueue   RandomShuffleQueueV2   RandomStandardNormal   RandomUniform   RandomUniformInt   Range   RangeDataset   Rank   ReadFile   ReadVariableOp   ReadVariableXlaSplitND   ReaderNumRecordsProduced   ReaderNumRecordsProducedV2   ReaderNumWorkUnitsCompleted   ReaderNumWorkUnitsCompletedV2   ReaderRead   ReaderReadUpTo   ReaderReadUpToV2   ReaderReadV2   ReaderReset   ReaderResetV2   ReaderRestoreState   ReaderRestoreStateV2   ReaderSerializeState   ReaderSerializeStateV2   Real   RealDiv   RebatchDataset   RebatchDatasetV2   Reciprocal   ReciprocalGrad   RecordInput   Recv   RecvTPUEmbeddingActivations   ReduceDataset   ReduceJoin   RefEnter   RefExit   RefIdentity   RefMerge   RefNextIteration   RefSelect   RefSwitch   RegexFullMatch   RegexReplace   RegisterDataset   RegisterDatasetV2   Relu   Relu6   Relu6Grad   ReluGrad   RemoteCall   RepeatDataset   RequantizationRange   RequantizationRangePerChannel   Requantize   RequantizePerChannel   Reshape   ResizeArea   ResizeBicubic   ResizeBicubicGrad   ResizeBilinear   ResizeBilinearGrad   ResizeNearestNeighbor   ResizeNearestNeighborGrad   ResourceAccumulatorApplyGradient   ResourceAccumulatorNumAccumulated   ResourceAccumulatorSetGlobalStep   ResourceAccumulatorTakeGradient   ResourceApplyAdaMax   ResourceApplyAdadelta   ResourceApplyAdagrad   ResourceApplyAdagradDA   ResourceApplyAdagradV2   ResourceApplyAdam   ResourceApplyAdamWithAmsgrad   ResourceApplyAddSign   ResourceApplyCenteredRMSProp   ResourceApplyFtrl   ResourceApplyFtrlV2   ResourceApplyGradientDescent   ResourceApplyKerasMomentum   ResourceApplyMomentum   ResourceApplyPowerSign   ResourceApplyProximalAdagrad   ResourceApplyProximalGradientDescent   ResourceApplyRMSProp   ResourceConditionalAccumulator   ResourceCountUpTo   ResourceGather   ResourceGatherNd   ResourceScatterAdd   ResourceScatterDiv   ResourceScatterMax   ResourceScatterMin   ResourceScatterMul   ResourceScatterNdAdd   ResourceScatterNdMax   ResourceScatterNdMin   ResourceScatterNdSub   ResourceScatterNdUpdate   ResourceScatterSub   ResourceScatterUpdate   ResourceSparseApplyAdadelta   ResourceSparseApplyAdagrad   ResourceSparseApplyAdagradDA   ResourceSparseApplyAdagradV2   ResourceSparseApplyCenteredRMSProp   ResourceSparseApplyFtrl   ResourceSparseApplyFtrlV2   ResourceSparseApplyKerasMomentum   ResourceSparseApplyMomentum   ResourceSparseApplyProximalAdagrad   ResourceSparseApplyProximalGradientDescent   ResourceSparseApplyRMSProp   ResourceStridedSliceAssign   Restore   RestoreSlice   RestoreV2   RetrieveTPUEmbeddingADAMParameters   RetrieveTPUEmbeddingAdadeltaParameters   RetrieveTPUEmbeddingAdagradMomentumParameters   RetrieveTPUEmbeddingAdagradParameters   RetrieveTPUEmbeddingCenteredRMSPropParameters   RetrieveTPUEmbeddingFTRLParameters   RetrieveTPUEmbeddingFrequencyEstimatorParameters   RetrieveTPUEmbeddingMDLAdagradLightParameters   RetrieveTPUEmbeddingMomentumParameters   RetrieveTPUEmbeddingProximalAdagradParameters   RetrieveTPUEmbeddingProximalYogiParameters   RetrieveTPUEmbeddingRMSPropParameters   RetrieveTPUEmbeddingStochasticGradientDescentParameters   Reverse   ReverseSequence   ReverseV2   RewriteDataset   RightShift   Rint   RngReadAndSkip   RngSkip   Roll   Round   Rsqrt   RsqrtGrad   SampleDistortedBoundingBox   SampleDistortedBoundingBoxV2   SamplingDataset   Save   SaveDataset   SaveDatasetV2   SaveSlices   SaveV2   ScalarSummary   ScaleAndTranslate   ScaleAndTranslateGrad   ScanDataset   ScatterAdd   ScatterDiv   ScatterMax   ScatterMin   ScatterMul   ScatterNd   ScatterNdAdd   ScatterNdMax   ScatterNdMin   ScatterNdNonAliasingAdd   ScatterNdSub   ScatterNdUpdate   ScatterSub   ScatterUpdate   SdcaFprint   SdcaOptimizer   SdcaOptimizerV2   SdcaShrinkL1   SegmentMax   SegmentMaxV2   SegmentMean   SegmentMin   SegmentMinV2   SegmentProd   SegmentProdV2   SegmentSum   SegmentSumV2   Select   SelectV2   SelfAdjointEig   SelfAdjointEigV2   Selu   SeluGrad   Send   SendTPUEmbeddingGradients   SerializeIterator   SerializeManySparse   SerializeSparse   SerializeTensor   SetSize   SetStatsAggregatorDataset   Shape   ShapeN   ShardDataset   ShardedFilename   ShardedFilespec   ShuffleAndRepeatDataset   ShuffleAndRepeatDatasetV2   ShuffleDataset   ShuffleDatasetV2   ShuffleDatasetV3   ShutdownDistributedTPU   Sigmoid   SigmoidGrad   Sign   Sin   Sinh   Size   SkipDataset   SleepDataset   Slice   SlidingWindowDataset   Snapshot   SnapshotChunkDataset   SnapshotDataset   SnapshotDatasetReader   SnapshotDatasetV2   SnapshotNestedDatasetReader   SobolSample   Softmax   SoftmaxCrossEntropyWithLogits   Softplus   SoftplusGrad   Softsign   SoftsignGrad   SpaceToBatch   SpaceToBatchND   SpaceToDepth   SparseAccumulatorApplyGradient   SparseAccumulatorTakeGradient   SparseAdd   SparseAddGrad   SparseApplyAdadelta   SparseApplyAdagrad   SparseApplyAdagradDA   SparseApplyAdagradV2   SparseApplyCenteredRMSProp   SparseApplyFtrl   SparseApplyFtrlV2   SparseApplyMomentum   SparseApplyProximalAdagrad   SparseApplyProximalGradientDescent   SparseApplyRMSProp   SparseBincount   SparseConcat   SparseConditionalAccumulator   SparseCountSparseOutput   SparseCross   SparseCrossHashed   SparseCrossV2   SparseDenseCwiseAdd   SparseDenseCwiseDiv   SparseDenseCwiseMul   SparseFillEmptyRows   SparseFillEmptyRowsGrad   SparseMatMul   SparseMatrixAdd   SparseMatrixMatMul   SparseMatrixMul   SparseMatrixNNZ   SparseMatrixOrderingAMD   SparseMatrixSoftmax   SparseMatrixSoftmaxGrad   SparseMatrixSparseCholesky   SparseMatrixSparseMatMul   SparseMatrixTranspose   SparseMatrixZeros   SparseReduceMax   SparseReduceMaxSparse   SparseReduceSum   SparseReduceSumSparse   SparseReorder   SparseReshape   SparseSegmentMean   SparseSegmentMeanGrad   SparseSegmentMeanGradV2   SparseSegmentMeanWithNumSegments   SparseSegmentSqrtN   SparseSegmentSqrtNGrad   SparseSegmentSqrtNGradV2   SparseSegmentSqrtNWithNumSegments   SparseSegmentSum   SparseSegmentSumGrad   SparseSegmentSumGradV2   SparseSegmentSumWithNumSegments   SparseSlice   SparseSliceGrad   SparseSoftmax   SparseSoftmaxCrossEntropyWithLogits   SparseSparseMaximum   SparseSparseMinimum   SparseSplit   SparseTensorDenseAdd   SparseTensorDenseMatMul   SparseTensorSliceDataset   SparseTensorToCSRSparseMatrix   SparseToDense   SparseToSparseSetOperation   Spence   Split   SplitV   SqlDataset   Sqrt   SqrtGrad   Square   SquaredDifference   Squeeze   Stack   StackClose   StackCloseV2   StackPop   StackPopV2   StackPush   StackPushV2   StackV2   Stage   StageClear   StagePeek   StageSize   StatefulPartitionedCall   StatefulRandomBinomial   StatefulStandardNormal   StatefulStandardNormalV2   StatefulTruncatedNormal   StatefulUniform   StatefulUniformFullInt   StatefulUniformInt   StatelessCase   StatelessIf   StatelessMultinomial   StatelessParameterizedTruncatedNormal   StatelessRandomBinomial   StatelessRandomGammaV2   StatelessRandomGammaV3   StatelessRandomGetAlg   StatelessRandomGetKeyCounter   StatelessRandomGetKeyCounterAlg   StatelessRandomNormal   StatelessRandomNormalV2   StatelessRandomPoisson   StatelessRandomUniform   StatelessRandomUniformFullInt   StatelessRandomUniformFullIntV2   StatelessRandomUniformInt   StatelessRandomUniformIntV2   StatelessRandomUniformV2   StatelessSampleDistortedBoundingBox   StatelessShuffle   StatelessTruncatedNormal   StatelessTruncatedNormalV2   StatelessWhile   StaticRegexFullMatch   StaticRegexReplace   StatsAggregatorHandle   StatsAggregatorHandleV2   StatsAggregatorSetSummaryWriter   StatsAggregatorSummary   StopGradient   StoreMinibatchStatisticsInFdo   StridedSlice   StridedSliceAssign   StridedSliceGrad   StringFormat   StringJoin   StringLength   StringLower   StringNGrams   StringSplit   StringSplitV2   StringStrip   StringToHashBucket   StringToHashBucketFast   StringToHashBucketStrong   StringToNumber   StringUpper   Sub   Substr   Sum   SummaryWriter   Svd   Switch   SymbolicGradient   SyncDevice   TFRecordDataset   TFRecordDatasetV2   TFRecordReader   TFRecordReaderV2   TPUAnnotateTensorsWithDynamicShape   TPUCompilationResult   TPUCopyWithDynamicShape   TPUEmbeddingActivations   TPUOrdinalSelector   TPUPartitionedCall   TPUPartitionedInput   TPUPartitionedInputV2   TPUPartitionedOutput   TPUPartitionedOutputV2   TPUReplicateMetadata   TPUReplicatedInput   TPUReplicatedOutput   TakeDataset   TakeManySparseFromTensorsMap   TakeWhileDataset   Tan   Tanh   TanhGrad   TemporaryVariable   TensorArray   TensorArrayClose   TensorArrayCloseV2   TensorArrayCloseV3   TensorArrayConcat   TensorArrayConcatV2   TensorArrayConcatV3   TensorArrayGather   TensorArrayGatherV2   TensorArrayGatherV3   TensorArrayGrad   TensorArrayGradV2   TensorArrayGradV3   TensorArrayGradWithShape   TensorArrayPack   TensorArrayRead   TensorArrayReadV2   TensorArrayReadV3   TensorArrayScatter   TensorArrayScatterV2   TensorArrayScatterV3   TensorArraySize   TensorArraySizeV2   TensorArraySizeV3   TensorArraySplit   TensorArraySplitV2   TensorArraySplitV3   TensorArrayUnpack   TensorArrayV2   TensorArrayV3   TensorArrayWrite   TensorArrayWriteV2   TensorArrayWriteV3   TensorDataset   TensorListConcat   TensorListConcatLists   TensorListConcatV2   TensorListElementShape   TensorListFromTensor   TensorListGather   TensorListGetItem   TensorListLength   TensorListPopBack   TensorListPushBack   TensorListPushBackBatch   TensorListReserve   TensorListResize   TensorListScatter   TensorListScatterIntoExistingList   TensorListScatterV2   TensorListSetItem   TensorListSplit   TensorListStack   TensorMapErase   TensorMapHasKey   TensorMapInsert   TensorMapLookup   TensorMapSize   TensorMapStackKeys   TensorScatterAdd   TensorScatterMax   TensorScatterMin   TensorScatterSub   TensorScatterUpdate   TensorSliceDataset   TensorStridedSliceUpdate   TensorSummary   TensorSummaryV2   TextLineDataset   TextLineReader   TextLineReaderV2   ThreadPoolDataset   ThreadPoolHandle   ThreadUnsafeUnigramCandidateSampler   Tile   TileGrad   Timestamp   ToBool   TopK   TopKV2   Transpose   TridiagonalMatMul   TridiagonalSolve   TruncateDiv   TruncateMod   TruncatedNormal   Unbatch   UnbatchDataset   UnbatchGrad   UncompressElement   UnicodeDecode   UnicodeDecodeWithOffsets   UnicodeEncode   UnicodeScript   UnicodeTranscode   UniformCandidateSampler   UniformDequantize   UniformQuantize   UniformQuantizedAdd   UniformQuantizedClipByValue   UniformQuantizedConvolution   UniformQuantizedConvolutionHybrid   UniformQuantizedDot   UniformQuantizedDotHybrid   UniformRequantize   Unique   UniqueDataset   UniqueV2   UniqueWithCounts   UniqueWithCountsV2   Unpack   UnravelIndex   UnsortedSegmentJoin   UnsortedSegmentMax   UnsortedSegmentMin   UnsortedSegmentProd   UnsortedSegmentSum   Unstage   UnwrapDatasetVariant   UpperBound   VarHandleOp   VarIsInitializedOp   Variable   VariableShape   VariableV2   Where   While   WholeFileReader   WholeFileReaderV2   WindowDataset   WindowOp   WorkerHeartbeat   WrapDatasetVariant   WriteAudioSummary   WriteFile   WriteGraphSummary   WriteHistogramSummary   WriteImageSummary   WriteRawProtoSummary   WriteScalarSummary   WriteSummary   Xdivy   XlaConcatND   XlaSparseCoreAdagrad   XlaSparseCoreAdagradMomentum   XlaSparseCoreAdam   XlaSparseCoreFtrl   XlaSparseCoreSgd   XlaSparseDenseMatmul   XlaSparseDenseMatmulGradWithAdagradAndCsrInput   XlaSparseDenseMatmulGradWithAdagradMomentumAndCsrInput   XlaSparseDenseMatmulGradWithAdamAndCsrInput   XlaSparseDenseMatmulGradWithFtrlAndCsrInput   XlaSparseDenseMatmulGradWithSgdAndCsrInput   XlaSparseDenseMatmulWithCsrInput   XlaSplitND   Xlog1py   Xlogy   ZerosLike   Zeta   ZipDataset  


Page: https://www.tensorflow.org/api_docs/python/tf/saved_model
Public API for tf._api.v2.saved_model namespace Modules experimental module: Public API for tf._api.v2.saved_model.experimental namespace Classes class Asset : Represents a file asset to hermetically include in a SavedModel. class LoadOptions : Options for loading a SavedModel. class SaveOptions : Options for saving to SavedModel. Functions contains_saved_model(...) : Checks whether the provided export directory could contain a SavedModel. load(...) : Load a SavedModel from export_dir . save(...) : Exports a tf.Module (and subclasses) obj to SavedModel format . Other Members ASSETS_DIRECTORY 'assets' ASSETS_KEY 'saved_model_assets' CLASSIFY_INPUTS 'inputs' CLASSIFY_METHOD_NAME 'tensorflow/serving/classify' CLASSIFY_OUTPUT_CLASSES 'classes' CLASSIFY_OUTPUT_SCORES 'scores' DEBUG_DIRECTORY 'debug' DEBUG_INFO_FILENAME_PB 'saved_model_debug_info.pb' DEFAULT_SERVING_SIGNATURE_DEF_KEY 'serving_default' GPU 'gpu' PREDICT_INPUTS 'inputs' PREDICT_METHOD_NAME 'tensorflow/serving/predict' PREDICT_OUTPUTS 'outputs' REGRESS_INPUTS 'inputs' REGRESS_METHOD_NAME 'tensorflow/serving/regress' REGRESS_OUTPUTS 'outputs' SAVED_MODEL_FILENAME_PB 'saved_model.pb' SAVED_MODEL_FILENAME_PBTXT 'saved_model.pbtxt' SAVED_MODEL_SCHEMA_VERSION 1 SERVING 'serve' TPU 'tpu' TRAINING 'train' VARIABLES_DIRECTORY 'variables' VARIABLES_FILENAME 'variables'


Page: https://www.tensorflow.org/api_docs/python/tf/sets
Public API for tf._api.v2.sets namespace Functions difference(...) : Compute set difference of elements in last dimension of a and b . intersection(...) : Compute set intersection of elements in last dimension of a and b . size(...) : Compute number of unique elements along last dimension of a . union(...) : Compute set union of elements in last dimension of a and b .


Page: https://www.tensorflow.org/api_docs/python/tf/signal
Public API for tf._api.v2.signal namespace Functions dct(...) : Computes the 1D [Discrete Cosine Transform (DCT)][dct] of input . fft(...) : Fast Fourier transform. fft2d(...) : 2D fast Fourier transform. fft3d(...) : 3D fast Fourier transform. fftnd(...) : ND fast Fourier transform. fftshift(...) : Shift the zero-frequency component to the center of the spectrum. frame(...) : Expands signal 's axis dimension into frames of frame_length . hamming_window(...) : Generate a [Hamming][hamming] window. hann_window(...) : Generate a [Hann window][hann]. idct(...) : Computes the 1D [Inverse Discrete Cosine Transform (DCT)][idct] of input . ifft(...) : Inverse fast Fourier transform. ifft2d(...) : Inverse 2D fast Fourier transform. ifft3d(...) : Inverse 3D fast Fourier transform. ifftnd(...) : ND inverse fast Fourier transform. ifftshift(...) : The inverse of fftshift. inverse_mdct(...) : Computes the inverse modified DCT of mdcts . inverse_stft(...) : Computes the inverse [Short-time Fourier Transform][stft] of stfts . inverse_stft_window_fn(...) : Generates a window function that can be used in inverse_stft . irfft(...) : Inverse real-valued fast Fourier transform. irfft2d(...) : Inverse 2D real-valued fast Fourier transform. irfft3d(...) : Inverse 3D real-valued fast Fourier transform. irfftnd(...) : ND inverse real fast Fourier transform. kaiser_bessel_derived_window(...) : Generate a [Kaiser Bessel derived window][kbd]. kaiser_window(...) : Generate a [Kaiser window][kaiser]. linear_to_mel_weight_matrix(...) : Returns a matrix to warp linear scale spectrograms to the [mel scale][mel]. mdct(...) : Computes the [Modified Discrete Cosine Transform][mdct] of signals . mfccs_from_log_mel_spectrograms(...) : Computes [MFCCs][mfcc] of log_mel_spectrograms . overlap_and_add(...) : Reconstructs a signal from a framed representation. rfft(...) : Real-valued fast Fourier transform. rfft2d(...) : 2D real-valued fast Fourier transform. rfft3d(...) : 3D real-valued fast Fourier transform. rfftnd(...) : ND fast real Fourier transform. stft(...) : Computes the [Short-time Fourier Transform][stft] of signals . vorbis_window(...) : Generate a [Vorbis power complementary window][vorbis].


Page: https://www.tensorflow.org/api_docs/python/tf/sparse
Public API for tf._api.v2.sparse namespace Classes class SparseTensor : Represents a sparse tensor. Functions add(...) : Adds two tensors, at least one of each is a SparseTensor . bincount(...) : Count the number of times an integer value appears in a tensor. concat(...) : Concatenates a list of SparseTensor along the specified dimension. (deprecated arguments) cross(...) : Generates sparse cross from a list of sparse and dense tensors. cross_hashed(...) : Generates hashed sparse cross from a list of sparse and dense tensors. expand_dims(...) : Returns a tensor with an length 1 axis inserted at index axis . eye(...) : Creates a two-dimensional sparse tensor with ones along the diagonal. fill_empty_rows(...) : Fills empty rows in the input 2-D SparseTensor with a default value. from_dense(...) : Converts a dense tensor into a sparse tensor. map_values(...) : Applies op to the .values tensor of one or more SparseTensor s. mask(...) : Masks elements of IndexedSlices . maximum(...) : Returns the element-wise max of two SparseTensors. minimum(...) : Returns the element-wise min of two SparseTensors. reduce_max(...) : Computes tf.sparse.maximum of elements across dimensions of a SparseTensor. reduce_sum(...) : Computes tf.sparse.add of elements across dimensions of a SparseTensor. reorder(...) : Reorders a SparseTensor into the canonical, row-major ordering. reset_shape(...) : Resets the shape of a SparseTensor with indices and values unchanged. reshape(...) : Reshapes a SparseTensor to represent values in a new dense shape. retain(...) : Retains specified non-empty values within a SparseTensor . segment_mean(...) : Computes the mean along sparse segments of a tensor. segment_sqrt_n(...) : Computes the sum along sparse segments of a tensor divided by the sqrt(N). segment_sum(...) : Computes the sum along sparse segments of a tensor. slice(...) : Slice a SparseTensor based on the start and size . softmax(...) : Applies softmax to a batched N-D SparseTensor . sparse_dense_matmul(...) : Multiply SparseTensor (or dense Matrix) (of rank 2) "A" by dense matrix split(...) : Split a SparseTensor into num_split tensors along axis . to_dense(...) : Converts a SparseTensor into a dense tensor. to_indicator(...) : Converts a SparseTensor of ids into a dense bool indicator tensor. transpose(...) : Transposes a SparseTensor .


Page: https://www.tensorflow.org/api_docs/python/tf/strings
Public API for tf._api.v2.strings namespace Functions as_string(...) : Converts each entry in the given tensor to strings. bytes_split(...) : Split string elements of input into bytes. format(...) : Formats a string template using a list of tensors. join(...) : Perform element-wise concatenation of a list of string tensors. length(...) : String lengths of input . lower(...) : Converts all uppercase characters into their respective lowercase replacements. ngrams(...) : Create a tensor of n-grams based on data . reduce_join(...) : Joins all strings into a single string, or joins along an axis. regex_full_match(...) : Check if the input matches the regex pattern. regex_replace(...) : Replace elements of input matching regex pattern with rewrite . split(...) : Split elements of input based on sep into a RaggedTensor . strip(...) : Strip leading and trailing whitespaces from the Tensor. substr(...) : Return substrings from Tensor of strings. to_hash_bucket(...) : Converts each string in the input Tensor to its hash mod by a number of buckets. to_hash_bucket_fast(...) : Converts each string in the input Tensor to its hash mod by a number of buckets. to_hash_bucket_strong(...) : Converts each string in the input Tensor to its hash mod by a number of buckets. to_number(...) : Converts each string in the input Tensor to the specified numeric type. unicode_decode(...) : Decodes each string in input into a sequence of Unicode code points. unicode_decode_with_offsets(...) : Decodes each string into a sequence of code points with start offsets. unicode_encode(...) : Encodes each sequence of Unicode code points in input into a string. unicode_script(...) : Determine the script codes of a given tensor of Unicode integer code points. unicode_split(...) : Splits each string in input into a sequence of Unicode code points. unicode_split_with_offsets(...) : Splits each string into a sequence of code points with start offsets. unicode_transcode(...) : Transcode the input text from a source encoding to a destination encoding. unsorted_segment_join(...) : Joins the elements of inputs based on segment_ids . upper(...) : Converts all lowercase characters into their respective uppercase replacements.


Page: https://www.tensorflow.org/api_docs/python/tf/summary
Public API for tf._api.v2.summary namespace Modules experimental module: Public API for tf._api.v2.summary.experimental namespace Classes class SummaryWriter : Interface representing a stateful summary writer object. Functions audio(...) : Write an audio summary. create_file_writer(...) : Creates a summary file writer for the given log directory. create_noop_writer(...) : Returns a summary writer that does nothing. flush(...) : Forces summary writer to send any buffered data to storage. graph(...) : Writes a TensorFlow graph summary. histogram(...) : Write a histogram summary. image(...) : Write an image summary. record_if(...) : Sets summary recording on or off per the provided boolean value. scalar(...) : Write a scalar summary. should_record_summaries(...) : Returns boolean Tensor which is True if summaries will be recorded. text(...) : Write a text summary. trace_export(...) : Stops and exports the active trace as a Summary and/or profile file. trace_off(...) : Stops the current trace and discards any collected information. trace_on(...) : Starts a trace to record computation graphs and profiling information. write(...) : Writes a generic summary to the default SummaryWriter if one exists.


Page: https://www.tensorflow.org/api_docs/python/tf/sysconfig
Public API for tf._api.v2.sysconfig namespace Functions get_build_info(...) : Get a dictionary describing TensorFlow's build environment. get_compile_flags(...) : Returns the compilation flags for compiling with TensorFlow. get_include(...) : Get the directory containing the TensorFlow C++ header files. get_lib(...) : Get the directory containing the TensorFlow framework library. get_link_flags(...) : Returns the linker flags for linking with TensorFlow. Other Members CXX11_ABI_FLAG 1 CXX_VERSION 201703 MONOLITHIC_BUILD 0


Page: https://www.tensorflow.org/api_docs/python/tf/test
Public API for tf._api.v2.test namespace Modules experimental module: Public API for tf._api.v2.test.experimental namespace Classes class Benchmark : Abstract class that provides helpers for TensorFlow benchmarks. class TestCase : Base class for tests that need to test TensorFlow. Functions assert_equal_graph_def(...) : Asserts that two GraphDef s are (mostly) the same. benchmark_config(...) : Returns a tf.compat.v1.ConfigProto for disabling the dependency optimizer. compute_gradient(...) : Computes the theoretical and numeric Jacobian of f . create_local_cluster(...) : Create and start local servers and return the associated Server objects. disable_with_predicate(...) : Disables the test if pred is true. gpu_device_name(...) : Returns the name of a GPU device if available or a empty string. is_built_with_cuda(...) : Returns whether TensorFlow was built with CUDA (GPU) support. is_built_with_gpu_support(...) : Returns whether TensorFlow was built with GPU (CUDA or ROCm) support. is_built_with_rocm(...) : Returns whether TensorFlow was built with ROCm (GPU) support. is_built_with_xla(...) : Returns whether TensorFlow was built with XLA support. is_gpu_available(...) : Returns whether TensorFlow can access a GPU. (deprecated) main(...) : Runs all unit tests. with_eager_op_as_function(...) : Returns the same class. This will be removed once all usages are removed.


Page: https://www.tensorflow.org/api_docs/python/tf/tpu
Public API for tf._api.v2.tpu namespace Modules experimental module: Public API for tf._api.v2.tpu.experimental namespace Classes class XLAOptions : XLA compilation options.


Page: https://www.tensorflow.org/api_docs/python/tf/train
Public API for tf._api.v2.train namespace Modules experimental module: Public API for tf._api.v2.train.experimental namespace Classes class BytesList : Used in tf.train.Example protos. Holds a list of byte-strings. class Checkpoint : Manages saving/restoring trackable values to disk. class CheckpointManager : Manages multiple checkpoints by keeping some and deleting unneeded ones. class CheckpointOptions : Options for constructing a Checkpoint. class CheckpointView : Gathers and serializes a checkpoint view. class ClusterDef : A ProtocolMessage class ClusterSpec : Represents a cluster as a set of "tasks", organized into "jobs". class Coordinator : A coordinator for threads. class Example : An Example is a standard proto storing data for training and inference. class ExponentialMovingAverage : Maintains moving averages of variables by employing an exponential decay. class Feature : Used in tf.train.Example protos. Contains a list of values. class FeatureList : Mainly used as part of a tf.train.SequenceExample . class FeatureLists : Mainly used as part of a tf.train.SequenceExample . class Features : Used in tf.train.Example protos. Contains the mapping from keys to Feature . class FloatList : Used in tf.train.Example protos. Holds a list of floats. class Int64List : Used in tf.train.Example protos. Holds a list of Int64s. class JobDef : A ProtocolMessage class SequenceExample : A SequenceExample represents a sequence of features and some context. class ServerDef : A ProtocolMessage class TrackableView : Gathers and serializes a trackable view. Functions checkpoints_iterator(...) : Continuously yield new checkpoint files as they appear. get_checkpoint_state(...) : Returns CheckpointState proto from the "checkpoint" file. latest_checkpoint(...) : Finds the filename of latest saved checkpoint file. list_variables(...) : Lists the checkpoint keys and shapes of variables in a checkpoint. load_checkpoint(...) : Returns CheckpointReader for checkpoint found in ckpt_dir_or_file . load_variable(...) : Returns the tensor value of the given variable in the checkpoint.


Page: https://www.tensorflow.org/api_docs/python/tf/types
Public API for tf._api.v2.types namespace Modules experimental module: Public API for tf._api.v2.types.experimental namespace


Page: https://www.tensorflow.org/api_docs/python/tf/version
Public API for tf._api.v2.version namespace Other Members COMPILER_VERSION 'Ubuntu Clang 17.0.6 (++20231208085846+6009708b4367-1~exp1~20231208085949.74)' GIT_VERSION 'v2.16.1-0-g5bc9d26649c' GRAPH_DEF_VERSION 1766 GRAPH_DEF_VERSION_MIN_CONSUMER 0 GRAPH_DEF_VERSION_MIN_PRODUCER 0 VERSION '2.16.1'


Page: https://www.tensorflow.org/api_docs/python/tf/xla
Public API for tf._api.v2.xla namespace Modules experimental module: Public API for tf._api.v2.xla.experimental namespace


Page: https://www.tensorflow.org/api_docs/python/tf/AggregationMethod
View source on GitHub A class listing aggregation methods used to combine gradients. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.AggregationMethod Computing partial derivatives can require aggregating gradient
contributions. This class lists the various methods that can
be used to combine gradients in the graph. The following aggregation methods are part of the stable API for
aggregating gradients: ADD_N : All of the gradient terms are summed as part of one
operation using the "AddN" op (see tf.add_n ). This
method has the property that all gradients must be ready and
buffered separately in memory before any aggregation is performed. DEFAULT : The system-chosen default aggregation method. The following aggregation methods are experimental and may not
be supported in future releases: EXPERIMENTAL_TREE : Gradient terms are summed in pairs using
the "AddN" op. This method of summing gradients may reduce
performance, but it can improve memory utilization because the
gradients can be released earlier. EXPERIMENTAL_ACCUMULATE_N : Same as EXPERIMENTAL_TREE . Example usage when computing gradient: @tf . function def example (): x = tf . constant ( 1.0 ) y = x * 2.0 z = y + y + y + y return tf . gradients ( z , [ x , y ], aggregation_method = tf . AggregationMethod . EXPERIMENTAL_ACCUMULATE_N ) example () [ < tf . Tensor : shape = (), dtype = float32 , numpy = 8.0 > , < tf . Tensor : shape = (), dtype = float32 , numpy = 4.0 > ] Class Variables ADD_N 0 DEFAULT 0 EXPERIMENTAL_ACCUMULATE_N 2 EXPERIMENTAL_TREE 1


Page: https://www.tensorflow.org/api_docs/python/tf/CriticalSection
View source on GitHub Critical section. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.CriticalSection tf . CriticalSection ( name = None , shared_name = None , critical_section_def = None , import_scope = None ) A CriticalSection object is a resource in the graph which executes subgraphs
in serial order.  A common example of a subgraph one may wish to run
exclusively is the one given by the following function: v = resource_variable_ops . ResourceVariable ( 0.0 , name = "v" ) def count (): value = v . read_value () with tf . control_dependencies ([ value ]): with tf . control_dependencies ([ v . assign_add ( 1 )]): return tf . identity ( value ) Here, a snapshot of v is captured in value ; and then v is updated.
The snapshot value is returned. If multiple workers or threads all execute count in parallel, there is no
guarantee that access to the variable v is atomic at any point within
any thread's calculation of count .  In fact, even implementing an atomic
counter that guarantees that the user will see each value 0, 1, ..., is
currently impossible. The solution is to ensure any access to the underlying resource v is
only processed through a critical section: cs = CriticalSection () f1 = cs . execute ( count ) f2 = cs . execute ( count ) output = f1 + f2 session . run ( output ) The functions f1 and f2 will be executed serially, and updates to v will be atomic. NOTES All resource objects, including the critical section and any captured
variables of functions executed on that critical section, will be
colocated to the same device (host and cpu/gpu). When using multiple critical sections on the same resources, there is no
guarantee of exclusive access to those resources.  This behavior is disallowed
by default (but see the kwarg exclusive_resource_access ). For example, running the same function in two separate critical sections
will not ensure serial execution: v = tf . compat . v1 . get_variable ( "v" , initializer = 0.0 , use_resource = True ) def accumulate ( up ): x = v . read_value () with tf . control_dependencies ([ x ]): with tf . control_dependencies ([ v . assign_add ( up )]): return tf . identity ( x ) ex1 = CriticalSection () . execute ( accumulate , 1.0 , exclusive_resource_access = False ) ex2 = CriticalSection () . execute ( accumulate , 1.0 , exclusive_resource_access = False ) bad_sum = ex1 + ex2 sess . run ( v . initializer ) sess . run ( bad_sum ) # May return 0.0 Attributes name Methods execute View source execute ( fn , exclusive_resource_access = True , name = None ) Execute function fn() inside the critical section. fn should not accept any arguments.  To add extra arguments to when
calling fn in the critical section, create a lambda: critical_section . execute ( lambda : fn ( * my_args , ** my_kwargs )) Args fn The function to execute.  Must return at least one tensor. exclusive_resource_access Whether the resources required by fn should be exclusive to this CriticalSection .  Default: True .
You may want to set this to False if you will be accessing a
resource in read-only mode in two different CriticalSections. name The name to use when creating the execute operation. Returns The tensors returned from fn() . Raises ValueError If fn attempts to lock this CriticalSection in any nested
or lazy way that may cause a deadlock. ValueError If exclusive_resource_access == True and
another CriticalSection has an execution requesting the same
resources as fn .  Note, even if exclusive_resource_access is True , if another execution in another CriticalSection was created
without exclusive_resource_access=True , a ValueError` will be raised.


Page: https://www.tensorflow.org/api_docs/python/tf/dtypes/DType
View source on GitHub Represents the type of the elements in a Tensor . Inherits From: TraceType View aliases Main aliases tf.DType Compat aliases for migration See Migration guide for
more details. tf.compat.v1.DType , tf.compat.v1.dtypes.DType tf . dtypes . DType ( type_enum , handle_data = None ) DType 's are used to specify the output data type for operations which
require it, or to inspect the data type of existing Tensor 's. Examples: tf . constant ( 1 , dtype = tf . int64 ) < tf . Tensor : shape = (), dtype = int64 , numpy = 1 > tf . constant ( 1.0 ) . dtype tf . float32 See tf.dtypes for a complete list of DType 's defined. Attributes as_datatype_enum Returns a types_pb2.DataType enum value based on this data type. as_numpy_dtype Returns a Python type object based on this DType . base_dtype Returns a non-reference DType based on this DType (for TF1). Programs written for TensorFlow 2.x do not need this attribute.
It exists only for compatibility with TensorFlow 1.x, which used
reference DType s in the implementation of tf.compat.v1.Variable .
In TensorFlow 2.x, tf.Variable is implemented without reference types. is_bool Returns whether this is a boolean data type. is_complex Returns whether this is a complex floating point type. is_floating Returns whether this is a (non-quantized, real) floating point type. is_integer Returns whether this is a (non-quantized) integer type. is_numeric Returns whether this is a numeric data type. is_numpy_compatible Returns whether this data type has a compatible NumPy data type. is_quantized Returns whether this is a quantized data type. is_unsigned Returns whether this type is unsigned. Non-numeric, unordered, and quantized types are not considered unsigned, and
this function returns False . limits Return intensity limits, i.e. (min, max) tuple, of the dtype.
Args:
  clip_negative : bool, optional If True, clip the negative range (i.e.
    return 0 for min intensity) even if the image dtype allows negative
    values. Returns
  min, max : tuple Lower and upper intensity limits. max Returns the maximum representable value in this data type. min Returns the minimum representable value in this data type. name real_dtype Returns the DType corresponding to this DType 's real part. size Methods experimental_as_proto View source experimental_as_proto () -> types_pb2 . SerializedDType Returns a proto representation of the Dtype instance. experimental_from_proto View source @classmethod experimental_from_proto ( proto : types_pb2 . SerializedDType ) -> 'DType' Returns a Dtype instance based on the serialized proto. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ types_pb2 . SerializedDType ] Returns the type of proto associated with DType serialization. is_compatible_with View source is_compatible_with ( other ) Returns True if the other DType will be converted to this DType (TF1). Programs written for TensorFlow 2.x do not need this function.
Instead, they can do equality comparison on DType objects directly: tf.as_dtype(this) == tf.as_dtype(other) . This function exists only for compatibility with TensorFlow 1.x, where it
additionally allows conversion from a reference type (used by tf.compat.v1.Variable ) to its base type. Args other A DType (or object that may be converted to a DType ). Returns True if a Tensor of the other DType will be implicitly converted to
this DType . is_subtype_of View source is_subtype_of ( other : tf . types . experimental . TraceType ) -> bool See tf.types.experimental.TraceType base class. most_specific_common_supertype View source most_specific_common_supertype ( types : Sequence [ tf . types . experimental . TraceType ] ) -> Optional [ 'DType' ] See tf.types.experimental.TraceType base class. __eq__ View source __eq__ ( other ) Returns True iff this DType refers to the same type as other . __ne__ View source __ne__ ( other ) Returns True iff self != other.


Page: https://www.tensorflow.org/api_docs/python/tf/DeviceSpec
View source on GitHub Represents a (possibly partial) specification for a TensorFlow device. tf . DeviceSpec ( job = None , replica = None , task = None , device_type = None , device_index = None ) DeviceSpec s are used throughout TensorFlow to describe where state is stored
and computations occur. Using DeviceSpec allows you to parse device spec
strings to verify their validity, merge them or compose them programmatically. Example: # Place the operations on device "GPU:0" in the "ps" job. device_spec = DeviceSpec ( job = "ps" , device_type = "GPU" , device_index = 0 ) with tf . device ( device_spec . to_string ()): # Both my_var and squared_var will be placed on /job:ps/device:GPU:0. my_var = tf . Variable ( ... , name = "my_variable" ) squared_var = tf . square ( my_var ) With eager execution disabled (by default in TensorFlow 1.x and by calling
disable_eager_execution() in TensorFlow 2.x), the following syntax
can be used: tf . compat . v1 . disable_eager_execution () # Same as previous device_spec = DeviceSpec ( job = "ps" , device_type = "GPU" , device_index = 0 ) # No need of .to_string() method. with tf . device ( device_spec ): my_var = tf . Variable ( ... , name = "my_variable" ) squared_var = tf . square ( my_var ) If a DeviceSpec is partially specified, it will be merged with other DeviceSpec s according to the scope in which it is defined. DeviceSpec components defined in inner scopes take precedence over those defined in
outer scopes. gpu0_spec = DeviceSpec ( job = "ps" , device_type = "GPU" , device_index = 0 ) with tf . device ( DeviceSpec ( job = "train" ) . to_string ()): with tf . device ( gpu0_spec . to_string ()): # Nodes created here will be assigned to /job:ps/device:GPU:0. with tf . device ( DeviceSpec ( device_type = "GPU" , device_index = 1 ) . to_string ()): # Nodes created here will be assigned to /job:train/device:GPU:1. A DeviceSpec consists of 5 components -- each of
which is optionally specified: Job: The job name. Replica: The replica index. Task: The task index. Device type: The device type string (e.g. "CPU" or "GPU"). Device index: The device index. Args job string.  Optional job name. replica int.  Optional replica index. task int.  Optional task index. device_type Optional device type string (e.g. "CPU" or "GPU") device_index int.  Optional device index.  If left unspecified, device
represents 'any' device_index. Attributes device_index device_type job replica task Methods from_string View source @classmethod from_string ( spec ) Construct a DeviceSpec from a string. Args spec a string of the form
/job: /replica: /task: /device:CPU: or
/job: /replica: /task: /device:GPU: as cpu and gpu are
  mutually exclusive. All entries are optional. Returns A DeviceSpec. make_merged_spec View source make_merged_spec ( dev ) Returns a new DeviceSpec which incorporates dev . When combining specs, dev will take precedence over the current spec.
So for instance: first_spec = tf . DeviceSpec ( job = 0 , device_type = "CPU" ) second_spec = tf . DeviceSpec ( device_type = "GPU" ) combined_spec = first_spec . make_merged_spec ( second_spec ) is equivalent to: combined_spec = tf . DeviceSpec ( job = 0 , device_type = "GPU" ) Args dev a DeviceSpec Returns A new DeviceSpec which combines self and dev parse_from_string View source parse_from_string ( spec ) Parse a DeviceSpec name into its components. 2.x behavior change : In TensorFlow 1.x, this function mutates its own state and returns itself.
In 2.x, DeviceSpecs are immutable, and this function will return a
  DeviceSpec which contains the spec. Recommended: # my_spec and my_updated_spec are unrelated. my_spec = tf . DeviceSpec . from_string ( "/CPU:0" ) my_updated_spec = tf . DeviceSpec . from_string ( "/GPU:0" ) with tf . device ( my_updated_spec ): ... Will work in 1.x and 2.x (though deprecated in 2.x): my_spec = tf . DeviceSpec . from_string ( "/CPU:0" ) my_updated_spec = my_spec . parse_from_string ( "/GPU:0" ) with tf . device ( my_updated_spec ): ... Will NOT work in 2.x: my_spec = tf . DeviceSpec . from_string ( "/CPU:0" ) my_spec . parse_from_string ( "/GPU:0" ) # <== Will not update my_spec with tf . device ( my_spec ): ... In general, DeviceSpec.from_string should completely replace DeviceSpec.parse_from_string , and DeviceSpec.replace should
completely replace setting attributes directly. Args spec an optional string of the form
/job: /replica: /task: /device:CPU: or
/job: /replica: /task: /device:GPU: as cpu and gpu are
  mutually exclusive. All entries are optional. Returns The DeviceSpec . Raises ValueError if the spec was not valid. replace View source replace ( ** kwargs ) Convenience method for making a new DeviceSpec by overriding fields. For instance: my_spec = DeviceSpec = ( job = "my_job" , device = "CPU" ) my_updated_spec = my_spec . replace ( device = "GPU" ) my_other_spec = my_spec . replace ( device = None ) Args **kwargs This method takes the same args as the DeviceSpec constructor Returns A DeviceSpec with the fields specified in kwargs overridden. to_string View source to_string () Return a string representation of this DeviceSpec . Returns a string of the form
/job: /replica: /task: /device: : . __eq__ View source __eq__ ( other ) Checks if the other DeviceSpec is same as the current instance, eg have same value for all the internal fields. Args other Another DeviceSpec Returns Return True if other is also a DeviceSpec instance and has same value
as the current instance.
Return False otherwise.


Page: https://www.tensorflow.org/api_docs/python/tf/GradientTape
View source on GitHub Record operations for automatic differentiation. View aliases Main aliases tf.autodiff.GradientTape Compat aliases for migration See Migration guide for
more details. tf.compat.v1.GradientTape tf . GradientTape ( persistent = False , watch_accessed_variables = True ) Used in the notebooks Used in the guide Used in the tutorials Introduction to gradients and automatic differentiation Advanced automatic differentiation Better performance with tf.function TensorFlow basics Effective Tensorflow 2 Deep Convolutional Generative Adversarial Network DeepDream pix2pix: Image-to-image translation with a conditional GAN Neural style transfer Custom training: walkthrough Operations are recorded if they are executed within this context manager and
at least one of their inputs is being "watched". Trainable variables (created by tf.Variable or tf.compat.v1.get_variable ,
where trainable=True is default in both cases) are automatically watched.
Tensors can be manually watched by invoking the watch method on this context
manager. For example, consider the function y = x * x . The gradient at x = 3.0 can
be computed as: x = tf . constant ( 3.0 ) with tf . GradientTape () as g : g . watch ( x ) y = x * x dy_dx = g . gradient ( y , x ) print ( dy_dx ) tf . Tensor ( 6.0 , shape = (), dtype = float32 ) GradientTapes can be nested to compute higher-order derivatives. For example, x = tf . constant ( 5.0 ) with tf . GradientTape () as g : g . watch ( x ) with tf . GradientTape () as gg : gg . watch ( x ) y = x * x dy_dx = gg . gradient ( y , x ) # dy_dx = 2 * x d2y_dx2 = g . gradient ( dy_dx , x ) # d2y_dx2 = 2 print ( dy_dx ) tf . Tensor ( 10.0 , shape = (), dtype = float32 ) print ( d2y_dx2 ) tf . Tensor ( 2.0 , shape = (), dtype = float32 ) By default, the resources held by a GradientTape are released as soon as
GradientTape.gradient() method is called. To compute multiple gradients over
the same computation, create a persistent gradient tape. This allows multiple
calls to the gradient() method as resources are released when the tape object
is garbage collected. For example: x = tf . constant ( 3.0 ) with tf . GradientTape ( persistent = True ) as g : g . watch ( x ) y = x * x z = y * y dz_dx = g . gradient ( z , x ) # (4*x^3 at x = 3) print ( dz_dx ) tf . Tensor ( 108.0 , shape = (), dtype = float32 ) dy_dx = g . gradient ( y , x ) print ( dy_dx ) tf . Tensor ( 6.0 , shape = (), dtype = float32 ) By default GradientTape will automatically watch any trainable variables that
are accessed inside the context. If you want fine grained control over which
variables are watched you can disable automatic tracking by passing watch_accessed_variables=False to the tape constructor: x = tf . Variable ( 2.0 ) w = tf . Variable ( 5.0 ) with tf . GradientTape ( watch_accessed_variables = False , persistent = True ) as tape : tape . watch ( x ) y = x ** 2 # Gradients will be available for `x`. z = w ** 3 # No gradients will be available as `w` isn't being watched. dy_dx = tape . gradient ( y , x ) print ( dy_dx ) tf . Tensor ( 4.0 , shape = (), dtype = float32 ) # No gradients will be available as `w` isn't being watched. dz_dw = tape . gradient ( z , w ) print ( dz_dw ) None Note that when using models you should ensure that your variables exist when
using watch_accessed_variables=False . Otherwise it's quite easy to make your
first iteration not have any gradients: a = tf . keras . layers . Dense ( 32 ) b = tf . keras . layers . Dense ( 32 ) with tf . GradientTape ( watch_accessed_variables = False ) as tape : tape . watch ( a . variables ) # Since `a.build` has not been called at this point # `a.variables` will return an empty list and the # tape will not be watching anything. result = b ( a ( inputs )) tape . gradient ( result , a . variables ) # The result of this computation will be # a list of `None`s since a's variables # are not being watched. Note that only tensors with real or complex dtypes are differentiable. Args persistent Boolean controlling whether a persistent gradient tape
is created. False by default, which means at most one call can
be made to the gradient() method on this object. watch_accessed_variables Boolean controlling whether the tape will
automatically watch any (trainable) variables accessed while the tape
is active. Defaults to True meaning gradients can be requested from any
result computed in the tape derived from reading a trainable Variable .
If False users must explicitly watch any Variable s they want to
request gradients from. Methods batch_jacobian View source batch_jacobian ( target , source , unconnected_gradients = tf . UnconnectedGradients . NONE , parallel_iterations = None , experimental_use_pfor = True ) Computes and stacks per-example jacobians. See wikipedia article for the definition of a Jacobian. This function is essentially an efficient
implementation of the following: tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])]) . Note that compared to GradientTape.jacobian which computes gradient of
each output value w.r.t each input value, this function is useful when target[i,...] is independent of source[j,...] for j != i . This
assumption allows more efficient computation as compared to GradientTape.jacobian . The output, as well as intermediate activations,
are lower dimensional and avoid a bunch of redundant zeros which would
result in the jacobian computation given the independence assumption. Note: Unless you set persistent=True a GradientTape can only be used to
compute one set of gradients (or jacobians). Note: By default the batch_jacobian implementation uses parallel for (pfor),
which creates a tf.function under the hood for each batch_jacobian call.
For better performance, and to avoid recompilation and vectorization
rewrites on each call, enclose GradientTape code in @tf.function. Example usage: with tf . GradientTape () as g : x = tf . constant ([[ 1. , 2. ], [ 3. , 4. ]], dtype = tf . float32 ) g . watch ( x ) y = x * x batch_jacobian = g . batch_jacobian ( y , x ) # batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]] Args target A tensor with rank 2 or higher and with shape [b, y1, ..., y_n]. target[i,...] should only depend on source[i,...] . source A tensor with rank 2 or higher and with shape [b, x1, ..., x_m]. unconnected_gradients a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
'UnconnectedGradients' and it defaults to 'none'. parallel_iterations A knob to control how many iterations are dispatched
in parallel. This knob can be used to control the total memory usage. experimental_use_pfor If true, uses pfor for computing the Jacobian. Else
uses a tf.while_loop. Returns A tensor t with shape [b, y_1, ..., y_n, x1, ..., x_m] where t[i, ...] is the jacobian of target[i, ...] w.r.t. source[i, ...] , i.e. stacked
per-example jacobians. Raises RuntimeError If called on a used, non-persistent tape. RuntimeError If called on a non-persistent tape with eager execution
enabled and without enabling experimental_use_pfor. ValueError If vectorization of jacobian computation fails or if first
dimension of target and source do not match. gradient View source gradient ( target , sources , output_gradients = None , unconnected_gradients = tf . UnconnectedGradients . NONE ) Computes the gradient using operations recorded in context of this tape. Note: Unless you set persistent=True a GradientTape can only be used to
compute one set of gradients (or jacobians). In addition to Tensors, gradient also supports RaggedTensors. For example, x = tf . ragged . constant ([[ 1.0 , 2.0 ], [ 3.0 ]]) with tf . GradientTape () as g : g . watch ( x ) y = x * x g . gradient ( y , x ) < tf . RaggedTensor [[ 2.0 , 4.0 ], [ 6.0 ]] > Args target a list or nested structure of Tensors or Variables or
CompositeTensors to be differentiated. sources a list or nested structure of Tensors or Variables or
CompositeTensors. target will be differentiated against elements in sources . output_gradients a list of gradients, one for each differentiable
element of target. Defaults to None. unconnected_gradients a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
'UnconnectedGradients' and it defaults to 'none'. Returns a list or nested structure of Tensors (or IndexedSlices, or None, or
CompositeTensor), one for each element in sources . Returned structure
is the same as the structure of sources . Raises RuntimeError If called on a used, non-persistent tape. RuntimeError If called inside the context of the tape. TypeError If the target is a None object. ValueError If the target is a variable or if unconnected gradients is
called with an unknown value. jacobian View source jacobian ( target , sources , unconnected_gradients = tf . UnconnectedGradients . NONE , parallel_iterations = None , experimental_use_pfor = True ) Computes the jacobian using operations recorded in context of this tape. Note: Unless you set persistent=True a GradientTape can only be used to
compute one set of gradients (or jacobians). Note: By default the jacobian implementation uses parallel for (pfor), which
creates a tf.function under the hood for each jacobian call. For better
performance, and to avoid recompilation and vectorization rewrites on each
call, enclose GradientTape code in @tf.function. See wikipedia
article for the definition of a Jacobian. Example usage: with tf . GradientTape () as g : x = tf . constant ([ 1.0 , 2.0 ]) g . watch ( x ) y = x * x jacobian = g . jacobian ( y , x ) # jacobian value is [[2., 0.], [0., 4.]] Args target Tensor to be differentiated. sources a list or nested structure of Tensors or Variables. target will be differentiated against elements in sources . unconnected_gradients a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
unconnected. The possible values and effects are detailed in
'UnconnectedGradients' and it defaults to 'none'. parallel_iterations A knob to control how many iterations are dispatched
in parallel. This knob can be used to control the total memory usage. experimental_use_pfor If true, vectorizes the jacobian computation. Else
falls back to a sequential while_loop. Vectorization can sometimes fail
or lead to excessive memory usage. This option can be used to disable
vectorization in such cases. Returns A list or nested structure of Tensors (or None), one for each element in sources . Returned structure is the same as the structure of sources .
Note if any gradient is sparse (IndexedSlices), jacobian function
currently makes it dense and returns a Tensor instead. This may change in
the future. Raises RuntimeError If called on a used, non-persistent tape. RuntimeError If called on a non-persistent tape with eager execution
enabled and without enabling experimental_use_pfor. ValueError If vectorization of jacobian computation fails. reset View source reset () Clears all information stored in this tape. Equivalent to exiting and reentering the tape context manager with a new
tape. For example, the two following code blocks are equivalent: with tf . GradientTape () as t : loss = loss_fn () with tf . GradientTape () as t : loss += other_loss_fn () t . gradient ( loss , ... ) # Only differentiates other_loss_fn, not loss_fn # The following is equivalent to the above with tf . GradientTape () as t : loss = loss_fn () t . reset () loss += other_loss_fn () t . gradient ( loss , ... ) # Only differentiates other_loss_fn, not loss_fn This is useful if you don't want to exit the context manager for the tape,
or can't because the desired reset point is inside a control flow construct: with tf . GradientTape () as t : loss = ... if loss > k : t . reset () stop_recording View source @tf_contextlib . contextmanager stop_recording () Temporarily stops recording operations on this tape. Operations executed while this context manager is active will not be
recorded on the tape. This is useful for reducing the memory used by tracing
all computations. For example: x = tf . constant ( 4.0 ) with tf . GradientTape () as tape : with tape . stop_recording (): y = x ** 2 dy_dx = tape . gradient ( y , x ) print ( dy_dx ) None Yields None Raises RuntimeError if the tape is not currently recording. watch View source watch ( tensor ) Ensures that tensor is being traced by this tape. Args tensor a Tensor/Variable or list of Tensors/Variables. Raises ValueError if it encounters something that is not a tensor. watched_variables View source watched_variables () Returns variables watched by this tape in order of construction. __enter__ View source __enter__ () Enters a context inside which operations are recorded on this tape. __exit__ View source __exit__ ( typ , value , traceback ) Exits the recording context, no further operations are traced.


Page: https://www.tensorflow.org/api_docs/python/tf/Graph
View source on GitHub A TensorFlow computation, represented as a dataflow graph. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.Graph tf . Graph () -> None Used in the notebooks Used in the guide Used in the tutorials Migrating model checkpoints Validating correctness & numerical equivalence Migrate the SavedModel workflow Debug a TensorFlow 2 migrated training pipeline Migrating your TFLite code to TF2 Generating Images with Little Data Using S3GAN Exploring the TF-Hub CORD-19 Swivel Embeddings Wiki40B Language Models Migrating tf.summary usage to TF 2.x Graphs are used by tf.function s to represent the function's computations.
Each graph contains a set of tf.Operation objects, which represent units of
computation; and tf.Tensor objects, which represent the units of data that
flow between operations. Using graphs directly (deprecated) A tf.Graph can be constructed and used directly without a tf.function , as
was required in TensorFlow 1, but this is deprecated and it is recommended to
use a tf.function instead. If a graph is directly used, other deprecated
TensorFlow 1 classes are also required to execute the graph, such as a tf.compat.v1.Session . A default graph can be registered with the tf.Graph.as_default context
manager. Then, operations will be added to the graph instead of being executed
eagerly. For example: g = tf . Graph () with g . as_default (): # Define operations and tensors in `g`. c = tf . constant ( 30.0 ) assert c . graph is g tf.compat.v1.get_default_graph() can be used to obtain the default graph. Important note: This class is not thread-safe for graph construction. All
operations should be created from a single thread, or external
synchronization must be provided. Unless otherwise specified, all methods
are not thread-safe. A Graph instance supports an arbitrary number of "collections"
that are identified by name. For convenience when building a large
graph, collections can store groups of related objects: for
example, the tf.Variable uses a collection (named tf.GraphKeys.GLOBAL_VARIABLES ) for
all variables that are created during the construction of a graph. The caller
may define additional collections by specifying a new name. Attributes building_function Returns True iff this graph represents a function. collections Returns the names of the collections known to this graph. finalized True if this graph has been finalized. graph_def_versions The GraphDef version information of this graph. For details on the meaning of each version, see GraphDef . operations seed The graph-level random seed of this graph. version Methods Dismantle Dismantle () (self: handle) -> None add_to_collection View source add_to_collection ( name , value ) -> None Stores value in the collection with the given name . Note that collections are not sets, so it is possible to add a value to
a collection several times. Args name The key for the collection. The GraphKeys class contains many
standard names for collections. value The value to add to the collection. add_to_collections View source add_to_collections ( names , value ) -> None Stores value in the collections given by names . Note that collections are not sets, so it is possible to add a value to
a collection several times. This function makes sure that duplicates in names are ignored, but it will not check for pre-existing membership of value in any of the collections in names . names can be any iterable, but if names is a string, it is treated as a
single collection name. Args names The keys for the collections to add to. The GraphKeys class
contains many standard names for collections. value The value to add to the collections. as_default View source as_default () -> ContextManager [ 'Graph' ] Returns a context manager that makes this Graph the default graph. This method should be used if you want to create multiple graphs
in the same process. For convenience, a global default graph is
provided, and all ops will be added to this graph if you do not
create a new graph explicitly. Use this method with the with keyword to specify that ops created within
the scope of a block should be added to this graph. In this case, once
the scope of the with is exited, the previous default graph is set again
as default. There is a stack, so it's ok to have multiple nested levels
of as_default calls. The default graph is a property of the current thread. If you
create a new thread, and wish to use the default graph in that
thread, you must explicitly add a with g.as_default(): in that
thread's function. The following code examples are equivalent: # 1. Using Graph.as_default(): g = tf . Graph () with g . as_default (): c = tf . constant ( 5.0 ) assert c . graph is g # 2. Constructing and making default: with tf . Graph () . as_default () as g : c = tf . constant ( 5.0 ) assert c . graph is g If eager execution is enabled ops created under this context manager will be
added to the graph instead of executed eagerly. Returns A context manager for using this graph as the default graph. as_graph_def View source as_graph_def ( from_version = None , add_shapes = False , use_pybind11_proto = False ) -> tf . compat . v1 . GraphDef Returns a serialized GraphDef representation of this graph. The serialized GraphDef can be imported into another Graph (using tf.import_graph_def ) or used with the C++ Session API . This method is thread-safe. Args from_version Optional.  If this is set, returns a GraphDef containing
only the nodes that were added to this graph since its version property had the given value. add_shapes If true, adds an "_output_shapes" list attr to each node with
the inferred shapes of each of its outputs. use_pybind11_proto If true, If true, uses the c++ pybind11_proto api to
get the GraphDef proto directly from c++, instead of through a TF
buffer. See https://github.com/pybind/pybind11_protobuf for reference. Returns A GraphDef protocol buffer. Raises ValueError If the graph_def would be too large. as_graph_element View source as_graph_element ( obj , allow_tensor = True , allow_operation = True ) -> Union [ tensor_lib . Tensor , 'Operation' ] Returns the object referred to by obj , as an Operation or Tensor . This function validates that obj represents an element of this
graph, and gives an informative error message if it is not. This function is the canonical way to get/validate an object of
one of the allowed types from an external argument reference in the
Session API. This method may be called concurrently from multiple threads. Args obj A Tensor , an Operation , or the name of a tensor or operation. Can
also be any object with an _as_graph_element() method that returns a
value of one of these types. Note: _as_graph_element will be called
inside the graph's lock and so may not modify the graph. allow_tensor If true, obj may refer to a Tensor . allow_operation If true, obj may refer to an Operation . Returns The Tensor or Operation in the Graph corresponding to obj . Raises TypeError If obj is not a type we support attempting to convert
to types. ValueError If obj is of an appropriate type but invalid. For
example, an invalid string. KeyError If obj is not an object in the graph. clear_collection View source clear_collection ( name ) -> None Clears all values in a collection. Args name The key for the collection. The GraphKeys class contains many
standard names for collections. colocate_with View source @tf_contextlib . contextmanager colocate_with ( op , ignore_existing = False ) -> Iterator [ None ] Returns a context manager that specifies an op to colocate with. Note: this function is not for public use, only for internal libraries. For example: a = tf . Variable ([ 1.0 ]) with g . colocate_with ( a ): b = tf . constant ( 1.0 ) c = tf . add ( a , b ) b and c will always be colocated with a , no matter where a is eventually placed. Note: Using a colocation scope resets any existing device constraints. If op is None then ignore_existing must be True and the new
scope resets all colocation and device constraints. Args op The op to colocate all created ops with, or None . ignore_existing If true, only applies colocation of this op within the
context, rather than applying all colocation properties on the stack.
If op is None , this value must be True . Raises ValueError if op is None but ignore_existing is False. Yields A context manager that specifies the op with which to colocate
newly created ops. container View source @tf_contextlib . contextmanager container ( container_name ) -> Iterator [ str ] Returns a context manager that specifies the resource container to use. Stateful operations, such as variables and queues, can maintain their
states on devices so that they can be shared by multiple processes.
A resource container is a string name under which these stateful
operations are tracked. These resources can be released or cleared
with tf.Session.reset() . For example: with g . container ( 'experiment0' ): # All stateful Operations constructed in this context will be placed # in resource container "experiment0". v1 = tf . Variable ([ 1.0 ]) v2 = tf . Variable ([ 2.0 ]) with g . container ( "experiment1" ): # All stateful Operations constructed in this context will be # placed in resource container "experiment1". v3 = tf . Variable ([ 3.0 ]) q1 = tf . queue . FIFOQueue ( 10 , tf . float32 ) # All stateful Operations constructed in this context will be # be created in the "experiment0". v4 = tf . Variable ([ 4.0 ]) q1 = tf . queue . FIFOQueue ( 20 , tf . float32 ) with g . container ( "" ): # All stateful Operations constructed in this context will be # be placed in the default resource container. v5 = tf . Variable ([ 5.0 ]) q3 = tf . queue . FIFOQueue ( 30 , tf . float32 ) # Resets container "experiment0", after which the state of v1, v2, v4, q1 # will become undefined (such as uninitialized). tf . Session . reset ( target , [ "experiment0" ]) Args container_name container name string. Returns A context manager for defining resource containers for stateful ops,
yields the container name. control_dependencies View source control_dependencies ( control_inputs ) -> _ControlDependenciesController Returns a context manager that specifies control dependencies. Use with the with keyword to specify that all operations constructed
within the context should have control dependencies on control_inputs . For example: with g . control_dependencies ([ a , b , c ]): # `d` and `e` will only run after `a`, `b`, and `c` have executed. d = ... e = ... Multiple calls to control_dependencies() can be nested, and in
that case a new Operation will have control dependencies on the union
of control_inputs from all active contexts. with g . control_dependencies ([ a , b ]): # Ops constructed here run after `a` and `b`. with g . control_dependencies ([ c , d ]): # Ops constructed here run after `a`, `b`, `c`, and `d`. You can pass None to clear the control dependencies: with g . control_dependencies ([ a , b ]): # Ops constructed here run after `a` and `b`. with g . control_dependencies ( None ): # Ops constructed here run normally, not waiting for either `a` or `b`. with g . control_dependencies ([ c , d ]): # Ops constructed here run after `c` and `d`, also not waiting # for either `a` or `b`. Note: The control dependencies context applies only to ops that
are constructed within the context. Merely using an op or tensor
in the context does not add a control dependency. The following
example illustrates this point: # WRONG def my_func ( pred , tensor ): t = tf . matmul ( tensor , tensor ) with tf . control_dependencies ([ pred ]): # The matmul op is created outside the context, so no control # dependency will be added. return t # RIGHT def my_func ( pred , tensor ): with tf . control_dependencies ([ pred ]): # The matmul op is created in the context, so a control dependency # will be added. return tf . matmul ( tensor , tensor ) Also note that though execution of ops created under this scope will trigger
execution of the dependencies, the ops created under this scope might still
be pruned from a normal tensorflow graph. For example, in the following
snippet of code the dependencies are never executed: loss = model . loss () with tf . control_dependencies ( dependencies ): loss = loss + tf . constant ( 1 ) # note: dependencies ignored in the # backward pass return tf . gradients ( loss , model . variables ) This is because evaluating the gradient graph does not require evaluating
the constant(1) op created in the forward pass. Args control_inputs A list of Operation or Tensor objects which must be
executed or computed before running the operations defined in the
context.  Can also be None to clear the control dependencies. Returns A context manager that specifies control dependencies for all
operations constructed within the context. Raises TypeError If control_inputs is not a list of Operation or Tensor objects. create_op View source create_op ( op_type , inputs , dtypes = None , input_types = None , name = None , attrs = None , op_def = None , compute_shapes = True , compute_device = True ) -> 'Operation' Creates an Operation in this graph. (deprecated arguments) Deprecated: SOME ARGUMENTS ARE DEPRECATED: (compute_shapes) . They will be removed in a future version.
Instructions for updating:
Shapes are always computed; don't use the compute_shapes as it has no effect. This is a low-level interface for creating an Operation . Most
programs will not call this method directly, and instead use the
Python op constructors, such as tf.constant() , which add ops to
the default graph. Args op_type The Operation type to create. This corresponds to the OpDef.name field for the proto that defines the operation. inputs A list of Tensor objects that will be inputs to the Operation . dtypes (Optional) A list of DType objects that will be the types of the
tensors that the operation produces. input_types (Optional.) A list of DType s that will be the types of the
tensors that the operation consumes. By default, uses the base DType of each input in inputs . Operations that expect reference-typed inputs
must specify input_types explicitly. name (Optional.) A string name for the operation. If not specified, a
name is generated based on op_type . attrs (Optional.) A dictionary where the key is the attribute name (a
string) and the value is the respective attr attribute of the NodeDef proto that will represent the operation (an AttrValue proto). op_def (Optional.) The OpDef proto that describes the op_type that
the operation will have. compute_shapes (Optional.) Deprecated. Has no effect (shapes are always
computed). compute_device (Optional.) If True, device functions will be executed to
compute the device property of the Operation. Raises TypeError if any of the inputs is not a Tensor . ValueError if colocation conflicts with existing device assignment. Returns An Operation object. device View source @tf_contextlib . contextmanager device ( device_name_or_function ) -> Iterator [ None ] Returns a context manager that specifies the default device to use. The device_name_or_function argument may either be a device name
string, a device function, or None: If it is a device name string, all operations constructed in
this context will be assigned to the device with that name, unless
overridden by a nested device() context. If it is a function, it will be treated as a function from
Operation objects to device name strings, and invoked each time
a new Operation is created. The Operation will be assigned to
the device with the returned name. If it is None, all device() invocations from the enclosing context
will be ignored. For information about the valid syntax of device name strings, see
the documentation in DeviceNameUtils . For example: with g . device ( '/device:GPU:0' ): # All operations constructed in this context will be placed # on GPU 0. with g . device ( None ): # All operations constructed in this context will have no # assigned device. # Defines a function from `Operation` to device string. def matmul_on_gpu ( n ): if n . type == "MatMul" : return "/device:GPU:0" else : return "/cpu:0" with g . device ( matmul_on_gpu ): # All operations of type "MatMul" constructed in this context # will be placed on GPU 0; all other operations will be placed # on CPU 0. Note: The device scope may be overridden by op wrappers or
other library code. For example, a variable assignment op v.assign() must be colocated with the tf.Variable v , and
incompatible device scopes will be ignored. Args device_name_or_function The device name or function to use in the
context. Yields A context manager that specifies the default device to use for newly
created ops. Raises RuntimeError If device scopes are not properly nested. finalize View source finalize () -> None Finalizes this graph, making it read-only. After calling g.finalize() , no new operations can be added to g .  This method is used to ensure that no operations are added
to a graph when it is shared between multiple threads, for example
when using a tf.compat.v1.train.QueueRunner . get View source get () -> GraphType get_all_collection_keys View source get_all_collection_keys () -> list [ str ] Returns a list of collections used in this graph. get_collection View source get_collection ( name , scope = None ) -> list [ Any ] Returns a list of values in the collection with the given name . This is different from get_collection_ref() which always returns the
actual collection list if it exists in that it returns a new list each time
it is called. Args name The key for the collection. For example, the GraphKeys class
contains many standard names for collections. scope (Optional.) A string. If supplied, the resulting list is filtered
to include only items whose name attribute matches scope using re.match . Items without a name attribute are never returned if a
scope is supplied. The choice of re.match means that a scope without
special tokens filters by prefix. Returns The list of values in the collection with the given name , or
an empty list if no value has been added to that collection. The
list contains the values in the order under which they were
collected. get_collection_ref View source get_collection_ref ( name ) -> list [ Any ] Returns a list of values in the collection with the given name . If the collection exists, this returns the list itself, which can
be modified in place to change the collection.  If the collection does
not exist, it is created as an empty list and the list is returned. This is different from get_collection() which always returns a copy of
the collection list if it exists and never creates an empty collection. Args name The key for the collection. For example, the GraphKeys class
contains many standard names for collections. Returns The list of values in the collection with the given name , or an empty
list if no value has been added to that collection. get_name_scope View source get_name_scope () -> str Returns the current name scope. For example: with tf . name_scope ( 'scope1' ): with tf . name_scope ( 'scope2' ): print ( tf . compat . v1 . get_default_graph () . get_name_scope ()) would print the string scope1/scope2 . Returns A string representing the current name scope. get_operation_by_name View source get_operation_by_name ( name ) -> 'Operation' Returns the Operation with the given name . This method may be called concurrently from multiple threads. Args name The name of the Operation to return. Returns The Operation with the given name . Raises TypeError If name is not a string. KeyError If name does not correspond to an operation in this graph. get_operations get_operations () (self: handle) -> list get_tensor_by_name View source get_tensor_by_name ( name ) -> tf . Tensor Returns the Tensor with the given name . This method may be called concurrently from multiple threads. Args name The name of the Tensor to return. Returns The Tensor with the given name . Raises TypeError If name is not a string. KeyError If name does not correspond to a tensor in this graph. gradient_override_map View source @tf_contextlib . contextmanager gradient_override_map ( op_type_map ) -> Iterator [ None ] EXPERIMENTAL: A context manager for overriding gradient functions. This context manager can be used to override the gradient function
that will be used for ops within the scope of the context. For example: @tf . RegisterGradient ( "CustomSquare" ) def _custom_square_grad ( op , grad ): # ... with tf . Graph () . as_default () as g : c = tf . constant ( 5.0 ) s_1 = tf . square ( c ) # Uses the default gradient for tf.square. with g . gradient_override_map ({ "Square" : "CustomSquare" }): s_2 = tf . square ( s_2 ) # Uses _custom_square_grad to compute the # gradient of s_2. Args op_type_map A dictionary mapping op type strings to alternative op type
strings. Returns A context manager that sets the alternative op type to be used for one
or more ops created in that context. Raises TypeError If op_type_map is not a dictionary mapping strings to
strings. is_feedable View source is_feedable ( tensor ) -> bool Returns True if and only if tensor is feedable. is_fetchable View source is_fetchable ( tensor_or_op ) -> bool Returns True if and only if tensor_or_op is fetchable. name_scope View source @tf_contextlib . contextmanager name_scope ( name ) -> Iterator [ str ] Returns a context manager that creates hierarchical names for operations. A graph maintains a stack of name scopes. A with name_scope(...): statement pushes a new name onto the stack for the lifetime of the context. The name argument will be interpreted as follows: A string (not ending with '/') will create a new name scope, in which name is appended to the prefix of all operations created in the
context. If name has been used before, it will be made unique by
calling self.unique_name(name) . A scope previously captured from a with g.name_scope(...) as
scope: statement will be treated as an "absolute" name scope, which
makes it possible to re-enter existing scopes. A value of None or the empty string will reset the current name scope
to the top-level (empty) name scope. For example: with tf . Graph () . as_default () as g : c = tf . constant ( 5.0 , name = "c" ) assert c . op . name == "c" c_1 = tf . constant ( 6.0 , name = "c" ) assert c_1 . op . name == "c_1" # Creates a scope called "nested" with g . name_scope ( "nested" ) as scope : nested_c = tf . constant ( 10.0 , name = "c" ) assert nested_c . op . name == "nested/c" # Creates a nested scope called "inner". with g . name_scope ( "inner" ): nested_inner_c = tf . constant ( 20.0 , name = "c" ) assert nested_inner_c . op . name == "nested/inner/c" # Create a nested scope called "inner_1". with g . name_scope ( "inner" ): nested_inner_1_c = tf . constant ( 30.0 , name = "c" ) assert nested_inner_1_c . op . name == "nested/inner_1/c" # Treats `scope` as an absolute name scope, and # switches to the "nested/" scope. with g . name_scope ( scope ): nested_d = tf . constant ( 40.0 , name = "d" ) assert nested_d . op . name == "nested/d" with g . name_scope ( "" ): e = tf . constant ( 50.0 , name = "e" ) assert e . op . name == "e" The name of the scope itself can be captured by with
g.name_scope(...) as scope: , which stores the name of the scope
in the variable scope . This value can be used to name an
operation that represents the overall result of executing the ops
in a scope. For example: inputs = tf . constant ( ... ) with g . name_scope ( 'my_layer' ) as scope : weights = tf . Variable ( ... , name = "weights" ) biases = tf . Variable ( ... , name = "biases" ) affine = tf . matmul ( inputs , weights ) + biases output = tf . nn . relu ( affine , name = scope ) Note: This constructor validates the given name . Valid scope
names match one of the following regular expressions: [ A - Za - z0 - 9. ][ A - Za - z0 - 9 _ . \ -/ ] * ( for scopes at the root ) [ A - Za - z0 - 9 _ . \ -/ ] * ( for other scopes ) Args name A name for the scope. Returns A context manager that installs name as a new name scope. Raises ValueError If name is not a valid scope name, according to the rules
above. new_operations new_operations () (self: handle) -> List[TF_Operation] num_operations num_operations () (self: handle) -> int op_def_for_type View source op_def_for_type ( type ) -> op_def_pb2 . OpDef Returns the OpDef proto for type . type is a string. prevent_feeding View source prevent_feeding ( tensor ) -> None Marks the given tensor as unfeedable in this graph. prevent_fetching View source prevent_fetching ( op ) -> None Marks the given op as unfetchable in this graph. switch_to_thread_local View source switch_to_thread_local () -> None Make device, colocation and dependencies stacks thread-local. Device, colocation and dependencies stacks are not thread-local be default.
If multiple threads access them, then the state is shared.  This means that
one thread may affect the behavior of another thread. After this method is called, the stacks become thread-local.  If multiple
threads access them, then the state is not shared.  Each thread uses its own
value; a thread doesn't affect other threads by mutating such a stack. The initial value for every thread's stack is set to the current value
of the stack when switch_to_thread_local() was first called. unique_name View source unique_name ( name , mark_as_used = True ) -> str Return a unique operation name for name . Note: You rarely need to call unique_name() directly.  Most of
the time you just need to create with g.name_scope() blocks to
generate structured names. unique_name is used to generate structured names, separated by "/" , to help identify operations when debugging a graph.
Operation names are displayed in error messages reported by the
TensorFlow runtime, and in various visualization tools such as
TensorBoard. If mark_as_used is set to True , which is the default, a new
unique name is created and marked as in use. If it's set to False ,
the unique name is returned without actually being marked as used.
This is useful when the caller simply wants to know what the name
to be created will be. Args name The name for an operation. mark_as_used Whether to mark this name as being used. Returns A string to be passed to create_op() that will be used
to name the operation being created. __enter__ View source __enter__ () -> GraphType __exit__ View source __exit__ ( * args ) -> None


Page: https://www.tensorflow.org/api_docs/python/tf/IndexedSlices
View source on GitHub A sparse representation of a set of tensor slices at given indices. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.IndexedSlices tf . IndexedSlices ( values , indices , dense_shape = None ) Used in the notebooks Used in the tutorials Client-efficient large-model federated learning via `federated_select` and sparse aggregation This class is a simple wrapper for a pair of Tensor objects: values : A Tensor of any dtype with shape [D0, D1, ..., Dn] . indices : A 1-D integer Tensor with shape [D0] . An IndexedSlices is typically used to represent a subset of a larger
tensor dense of shape [LARGE0, D1, .. , DN] where LARGE0 >> D0 .
The values in indices are the indices in the first dimension of
the slices that have been extracted from the larger tensor. The dense tensor dense represented by an IndexedSlices slices has dense [ slices . indices [ i ], :, :, :, ... ] = slices . values [ i , :, :, :, ... ] The IndexedSlices class is used principally in the definition of
gradients for operations that have sparse gradients
(e.g. tf.gather ). v = tf . Variable ([[ 0. , 1 , 2 ], [ 2 , 3 , 4 ], [ 4 , 5 , 6 ], [ 6 , 7 , 8 ]]) with tf . GradientTape () as tape : r = tf . gather ( v , [ 1 , 3 ]) index_slices = tape . gradient ( r , v ) index_slices < ... IndexedSlices object ... > index_slices . indices . numpy () array ([ 1 , 3 ], dtype = int32 ) index_slices . values . numpy () array ([[ 1. , 1. , 1. ], [ 1. , 1. , 1. ]], dtype = float32 ) Contrast this representation with tf.sparse.SparseTensor ,
which uses multi-dimensional indices and scalar values. Attributes dense_shape A 1-D Tensor containing the shape of the corresponding dense tensor. device The name of the device on which values will be produced, or None . dtype The DType of elements in this tensor. graph The Graph that contains the values, indices, and shape tensors. indices A 1-D Tensor containing the indices of the slices. name The name of this IndexedSlices . op The Operation that produces values as an output. shape Gets the tf.TensorShape representing the shape of the dense tensor. values A Tensor containing the values of the slices. Methods consumers View source consumers () __neg__ View source __neg__ ()


Page: https://www.tensorflow.org/api_docs/python/tf/IndexedSlicesSpec
View source on GitHub Type specification for a tf.IndexedSlices . Inherits From: TypeSpec , TraceType View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.IndexedSlicesSpec tf . IndexedSlicesSpec ( shape = None , dtype = tf . dtypes . float32 , indices_dtype = tf . dtypes . int64 , dense_shape_dtype = None , indices_shape = None ) Args shape The dense shape of the IndexedSlices , or None to allow any
dense shape. dtype tf.DType of values in the IndexedSlices . indices_dtype tf.DType of the indices in the IndexedSlices .  One
of tf.int32 or tf.int64 . dense_shape_dtype tf.DType of the dense_shape in the IndexedSlices .
One of tf.int32 , tf.int64 , or None (if the IndexedSlices has
no dense_shape tensor). indices_shape The shape of the indices component, which indicates
how many slices are in the IndexedSlices . Attributes value_type Methods experimental_as_proto View source experimental_as_proto () -> struct_pb2 . TypeSpecProto Returns a proto representation of the TypeSpec instance. Do NOT override for custom non-TF types. experimental_from_proto View source @classmethod experimental_from_proto ( proto : struct_pb2 . TypeSpecProto ) -> 'TypeSpec' Returns a TypeSpec instance based on the serialized proto. Do NOT override for custom non-TF types. Args proto Proto generated using 'experimental_as_proto'. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ struct_pb2 . TypeSpecProto ] Returns the type of proto associated with TypeSpec serialization. Do NOT override for custom non-TF types. is_compatible_with View source is_compatible_with ( spec_or_value ) Returns true if spec_or_value is compatible with this TypeSpec. Prefer using "is_subtype_of" and "most_specific_common_supertype" wherever
possible. Args spec_or_value A TypeSpec or TypeSpec associated value to compare against. is_subtype_of View source is_subtype_of ( other : tf . types . experimental . TraceType ) -> bool Returns True if self is a subtype of other . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args other A TraceType object. most_specific_common_supertype View source most_specific_common_supertype ( others : Sequence [ tf . types . experimental . TraceType ] ) -> Optional [ 'TypeSpec' ] Returns the most specific supertype TypeSpec  of self and others . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args others A sequence of TraceTypes. most_specific_compatible_type View source most_specific_compatible_type ( other : 'TypeSpec' ) -> 'TypeSpec' Returns the most specific TypeSpec compatible with self and other . (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use most_specific_common_supertype instead. Deprecated. Please use most_specific_common_supertype instead.
Do not override this function. Args other A TypeSpec . Raises ValueError If there is no TypeSpec that is compatible with both self and other . __eq__ View source __eq__ ( other ) -> bool Return self==value. __ne__ View source __ne__ ( other ) -> bool Return self!=value.


Page: https://www.tensorflow.org/api_docs/python/tf/Module
View source on GitHub Base neural network module class. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.Module tf . Module ( name = None ) Used in the notebooks Used in the guide Used in the tutorials Introduction to modules, layers, and models Better performance with tf.function Distributed training with Core APIs and DTensor Logistic regression for binary classification with Core APIs Multilayer perceptrons for digit recognition with Core APIs Distributed training with DTensors DeepDream Simple audio recognition: Recognizing keywords Neural machine translation with attention Neural machine translation with a Transformer and Keras A module is a named container for tf.Variable s, other tf.Module s and
functions which apply to user input. For example a dense layer in a neural
network might be implemented as a tf.Module : class Dense ( tf . Module ): def __init__ ( self , input_dim , output_size , name = None ): super () . __init__ ( name = name ) self . w = tf . Variable ( tf . random . normal ([ input_dim , output_size ]), name = 'w' ) self . b = tf . Variable ( tf . zeros ([ output_size ]), name = 'b' ) def __call__ ( self , x ): y = tf . matmul ( x , self . w ) + self . b return tf . nn . relu ( y ) You can use the Dense layer as you would expect: d = Dense ( input_dim = 3 , output_size = 2 ) d ( tf . ones ([ 1 , 3 ])) < tf . Tensor : shape = ( 1 , 2 ), dtype = float32 , numpy =... , dtype = float32 ) > By subclassing tf.Module instead of object any tf.Variable or tf.Module instances assigned to object properties can be collected using
the variables , trainable_variables or submodules property: d . variables ( < tf . Variable 'b:0' shape = ( 2 ,) dtype = float32 , numpy =... , dtype = float32 )>, < tf . Variable 'w:0' shape = ( 3 , 2 ) dtype = float32 , numpy =... , dtype = float32 )>) Subclasses of tf.Module can also take advantage of the _flatten method
which can be used to implement tracking of any other types. All tf.Module classes have an associated tf.name_scope which can be used
to group operations in TensorBoard and create hierarchies for variable names
which can help with debugging. We suggest using the name scope when creating
nested submodules/parameters or for forward methods whose graph you might want
to inspect in TensorBoard. You can enter the name scope explicitly using with self.name_scope: or you can annotate methods (apart from __init__ )
with @tf.Module.with_name_scope . class MLP ( tf . Module ): def __init__ ( self , input_size , sizes , name = None ): super () . __init__ ( name = name ) self . layers = [] with self . name_scope : for size in sizes : self . layers . append ( Dense ( input_dim = input_size , output_size = size )) input_size = size @tf . Module . with_name_scope def __call__ ( self , x ): for layer in self . layers : x = layer ( x ) return x module = MLP ( input_size = 5 , sizes = [ 5 , 5 ]) module . variables ( < tf . Variable 'mlp/b:0' shape = ( 5 ,) dtype = float32 , numpy =... , dtype = float32 )>, < tf . Variable 'mlp/w:0' shape = ( 5 , 5 ) dtype = float32 , numpy =... , dtype = float32 )>, < tf . Variable 'mlp/b:0' shape = ( 5 ,) dtype = float32 , numpy =... , dtype = float32 )>, < tf . Variable 'mlp/w:0' shape = ( 5 , 5 ) dtype = float32 , numpy =... , dtype = float32 )>) Attributes name Returns the name of this module as passed or determined in the ctor. Note: This is not the same as the self.name_scope.name which includes
parent module names. name_scope Returns a tf.name_scope instance for this class. non_trainable_variables Sequence of non-trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change. submodules Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as
properties of modules which are properties of this module (and so on). a = tf . Module () b = tf . Module () c = tf . Module () a . b = b b . c = c list ( a . submodules ) == [ b , c ] True list ( b . submodules ) == [ c ] True list ( c . submodules ) == [] True trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change. variables Sequence of variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance
and submodules. For performance reasons you may wish to cache the result
of calling this method if you don't expect the return value to change. Methods with_name_scope View source @classmethod with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule ( tf . Module ): @tf . Module . with_name_scope def __call__ ( self , x ): if not hasattr ( self , 'w' ): self . w = tf . Variable ( tf . random . normal ([ x . shape [ 1 ], 3 ])) return tf . matmul ( x , self . w ) Using the above module would produce tf.Variable s and tf.Tensor s whose
names included the module name: mod = MyModule () mod ( tf . ones ([ 1 , 2 ])) < tf . Tensor : shape = ( 1 , 3 ), dtype = float32 , numpy =... , dtype = float32 ) > mod . w < tf . Variable 'my_module/Variable:0' shape = ( 2 , 3 ) dtype = float32 , numpy =... , dtype = float32 ) > Args method The method to wrap. Returns The original method wrapped such that it enters the module's name scope.


Page: https://www.tensorflow.org/api_docs/python/tf/Operation
View source on GitHub Represents a graph node that performs computation on tensors. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.Operation tf . Operation ( * args , ** kwargs ) An Operation is a node in a tf.Graph that takes zero or more Tensor objects as input, and produces zero or more Tensor objects as output.
Objects of type Operation are created by calling a Python op constructor
(such as tf.matmul ) within a tf.function or under a tf.Graph.as_default context manager. For example, within a tf.function , c = tf.matmul(a, b) creates an Operation of type "MatMul" that takes tensors a and b as input, and
produces c as output. If a tf.compat.v1.Session is used, an Operation of a tf.Graph can be
executed by passing it to tf.Session.run . op.run() is a shortcut for
calling tf.compat.v1.get_default_session().run(op) . Attributes control_inputs The Operation objects on which this op has a control dependency. Before this op is executed, TensorFlow will ensure that the
operations in self.control_inputs have finished executing. This
mechanism can be used to run ops sequentially for performance
reasons, or to ensure that the side effects of an op are observed
in the correct order. device The name of the device to which this op has been assigned, if any. graph inputs The sequence of Tensor objects representing the data inputs of this op. name node_def op_def outputs traceback Returns the call stack from when this operation was constructed. type Methods colocation_groups View source colocation_groups () -> list [ bytes ] Returns the list of colocation groups of the op. experimental_set_type View source experimental_set_type ( type_proto ) -> None Sets the corresponding node's experimental_type field. See the description of NodeDef.experimental_type for more info. Args type_proto A FullTypeDef proto message. The root type_if of this object
must be TFT_PRODUCT , even for ops which only have a singlre return
value. from_node_def View source @classmethod from_node_def ( node_def , g , inputs = None , output_types = None , control_inputs = None , input_types = None , original_op = None , op_def = None ) -> OperationType Creates an Operation . Note: This constructor validates the name of the Operation (passed
as node_def.name ). Valid Operation names match the following
regular expression: [ A - Za - z0 - 9. ][ A - Za - z0 - 9 _ . \\ -/ ] * Args node_def node_def_pb2.NodeDef . NodeDef for the Operation . Used for
attributes of node_def_pb2.NodeDef , typically name , op , and device .  The input attribute is irrelevant here as it will be
computed when generating the model. g Graph . The parent graph. inputs list of Tensor objects. The inputs to this Operation . output_types list of DType objects.  List of the types of the Tensors computed by this operation.  The length of this list indicates the
number of output endpoints of the Operation . control_inputs list of operations or tensors from which to have a control
dependency. input_types List of DType objects representing the types of the tensors
accepted by the Operation .  By default uses [x.dtype.base_dtype for x
in inputs] .  Operations that expect reference-typed inputs must specify
these explicitly. original_op Optional. Used to associate the new Operation with an
existing Operation (for example, a replica with the op that was
replicated). op_def Optional. The op_def_pb2.OpDef proto that describes the op type
that this Operation represents. Raises TypeError if control inputs are not Operations or Tensors,
or if node_def is not a NodeDef ,
or if g is not a Graph ,
or if inputs are not tensors,
or if inputs and input_types are incompatible. ValueError if the node_def name is not valid. Returns Operation object. get_attr View source get_attr ( name ) Returns the value of the attr of this op with the given name . Args name The name of the attr to fetch. Returns The value of the attr, as a Python object. Raises ValueError If this op does not have an attr with the given name . run View source run ( feed_dict = None , session = None ) -> None Runs this operation in a Session . Calling this method will execute all preceding operations that
produce the inputs needed for this operation. Note: Before invoking Operation.run() , its graph must have been
launched in a session, and either a default session must be
available, or session must be specified explicitly. Args feed_dict A dictionary that maps Tensor objects to feed values. See tf.Session.run for a description of the valid feed values. session (Optional.) The Session to be used to run to this operation. If
none, the default session will be used. values View source values () -> tuple [ Any , ... ] Deprecated: Use outputs.


Page: https://www.tensorflow.org/api_docs/python/tf/OptionalSpec
View source on GitHub Type specification for tf.experimental.Optional . Inherits From: TypeSpec , TraceType View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.OptionalSpec , tf.compat.v1.data.experimental.OptionalStructure tf . OptionalSpec ( element_spec ) For instance, tf.OptionalSpec can be used to define a tf.function that takes tf.experimental.Optional as an input argument: @tf . function ( input_signature = [ tf . OptionalSpec ( tf . TensorSpec ( shape = (), dtype = tf . int32 , name = None ))]) def maybe_square ( optional ): if optional . has_value (): x = optional . get_value () return x * x return - 1 optional = tf . experimental . Optional . from_value ( 5 ) print ( maybe_square ( optional )) tf . Tensor ( 25 , shape = (), dtype = int32 ) Attributes element_spec A (nested) structure of TypeSpec objects that represents the
type specification of the optional element. value_type The Python type for values that are compatible with this TypeSpec. In particular, all values that are compatible with this TypeSpec must be an
instance of this type. Methods experimental_as_proto View source experimental_as_proto () -> struct_pb2 . TypeSpecProto Returns a proto representation of the TypeSpec instance. Do NOT override for custom non-TF types. experimental_from_proto View source @classmethod experimental_from_proto ( proto : struct_pb2 . TypeSpecProto ) -> 'TypeSpec' Returns a TypeSpec instance based on the serialized proto. Do NOT override for custom non-TF types. Args proto Proto generated using 'experimental_as_proto'. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ struct_pb2 . TypeSpecProto ] Returns the type of proto associated with TypeSpec serialization. Do NOT override for custom non-TF types. from_value View source @staticmethod from_value ( value ) is_compatible_with View source is_compatible_with ( spec_or_value ) Returns true if spec_or_value is compatible with this TypeSpec. Prefer using "is_subtype_of" and "most_specific_common_supertype" wherever
possible. Args spec_or_value A TypeSpec or TypeSpec associated value to compare against. is_subtype_of View source is_subtype_of ( other : tf . types . experimental . TraceType ) -> bool Returns True if self is a subtype of other . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args other A TraceType object. most_specific_common_supertype View source most_specific_common_supertype ( others : Sequence [ tf . types . experimental . TraceType ] ) -> Optional [ 'TypeSpec' ] Returns the most specific supertype TypeSpec  of self and others . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args others A sequence of TraceTypes. most_specific_compatible_type View source most_specific_compatible_type ( other : 'TypeSpec' ) -> 'TypeSpec' Returns the most specific TypeSpec compatible with self and other . (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use most_specific_common_supertype instead. Deprecated. Please use most_specific_common_supertype instead.
Do not override this function. Args other A TypeSpec . Raises ValueError If there is no TypeSpec that is compatible with both self and other . __eq__ View source __eq__ ( other ) -> bool Return self==value. __ne__ View source __ne__ ( other ) -> bool Return self!=value.


Page: https://www.tensorflow.org/api_docs/python/tf/experimental/Optional
View source on GitHub Represents a value that may or may not be present. View aliases Main aliases tf.data.experimental.Optional Compat aliases for migration See Migration guide for
more details. tf.compat.v1.data.experimental.Optional , tf.compat.v1.experimental.Optional A tf.experimental.Optional can represent the result of an operation that may
fail as a value, rather than raising an exception and halting execution. For
example, tf.data.Iterator.get_next_as_optional() returns a tf.experimental.Optional that either contains the next element of an
iterator if one exists, or an "empty" value that indicates the end of the
sequence has been reached. tf.experimental.Optional can only be used with values that are convertible
to tf.Tensor or tf.CompositeTensor . One can create a tf.experimental.Optional from a value using the from_value() method: optional = tf . experimental . Optional . from_value ( 42 ) print ( optional . has_value ()) tf . Tensor ( True , shape = (), dtype = bool ) print ( optional . get_value ()) tf . Tensor ( 42 , shape = (), dtype = int32 ) or without a value using the empty() method: optional = tf . experimental . Optional . empty ( tf . TensorSpec ( shape = (), dtype = tf . int32 , name = None )) print ( optional . has_value ()) tf . Tensor ( False , shape = (), dtype = bool ) Attributes element_spec The type specification of an element of this optional. optional = tf . experimental . Optional . from_value ( 42 ) print ( optional . element_spec ) tf . TensorSpec ( shape = (), dtype = tf . int32 , name = None ) Methods empty View source @staticmethod empty ( element_spec ) Returns an Optional that has no value. Note: This method takes an argument that defines the structure of the value
that would be contained in the returned Optional if it had a value. optional = tf . experimental . Optional . empty ( tf . TensorSpec ( shape = (), dtype = tf . int32 , name = None )) print ( optional . has_value ()) tf . Tensor ( False , shape = (), dtype = bool ) Args element_spec A (nested) structure of tf.TypeSpec objects matching the
structure of an element of this optional. Returns A tf.experimental.Optional with no value. from_value View source @staticmethod from_value ( value ) Returns a tf.experimental.Optional that wraps the given value. optional = tf . experimental . Optional . from_value ( 42 ) print ( optional . has_value ()) tf . Tensor ( True , shape = (), dtype = bool ) print ( optional . get_value ()) tf . Tensor ( 42 , shape = (), dtype = int32 ) Args value A value to wrap. The value must be convertible to tf.Tensor or tf.CompositeTensor . Returns A tf.experimental.Optional that wraps value . get_value View source @abc . abstractmethod get_value ( name = None ) Returns the value wrapped by this optional. If this optional does not have a value (i.e. self.has_value() evaluates to False ), this operation will raise tf.errors.InvalidArgumentError at
runtime. optional = tf . experimental . Optional . from_value ( 42 ) print ( optional . get_value ()) tf . Tensor ( 42 , shape = (), dtype = int32 ) Args name (Optional.) A name for the created operation. Returns The wrapped value. has_value View source @abc . abstractmethod has_value ( name = None ) Returns a tensor that evaluates to True if this optional has a value. optional = tf . experimental . Optional . from_value ( 42 ) print ( optional . has_value ()) tf . Tensor ( True , shape = (), dtype = bool ) Args name (Optional.) A name for the created operation. Returns A scalar tf.Tensor of type tf.bool .


Page: https://www.tensorflow.org/api_docs/python/tf/RaggedTensor
View source on GitHub Represents a ragged tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.RaggedTensor Used in the notebooks Used in the guide Ragged tensors Unicode strings A RaggedTensor is a tensor with one or more ragged dimensions , which are
dimensions whose slices may have different lengths.  For example, the inner
(column) dimension of rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []] is ragged,
since the column slices ( rt[0, :] , ..., rt[4, :] ) have different lengths.
Dimensions whose slices all have the same length are called uniform
dimensions .  The outermost dimension of a RaggedTensor is always uniform,
since it consists of a single slice (and so there is no possibility for
differing slice lengths). The total number of dimensions in a RaggedTensor is called its rank ,
and the number of ragged dimensions in a RaggedTensor is called its ragged-rank .  A RaggedTensor 's ragged-rank is fixed at graph creation
time: it can't depend on the runtime values of Tensor s, and can't vary
dynamically for different session runs. Note that the __init__ constructor is private. Please use one of the
following methods to construct a RaggedTensor : tf.RaggedTensor.from_row_lengths tf.RaggedTensor.from_value_rowids tf.RaggedTensor.from_row_splits tf.RaggedTensor.from_row_starts tf.RaggedTensor.from_row_limits tf.RaggedTensor.from_nested_row_splits tf.RaggedTensor.from_nested_row_lengths tf.RaggedTensor.from_nested_value_rowids Potentially Ragged Tensors Many ops support both Tensor s and RaggedTensor s
(see tf.ragged for a
full listing). The term "potentially ragged tensor" may be used to refer to a
tensor that might be either a Tensor or a RaggedTensor .  The ragged-rank
of a Tensor is zero. Documenting RaggedTensor Shapes When documenting the shape of a RaggedTensor, ragged dimensions can be
indicated by enclosing them in parentheses.  For example, the shape of
a 3-D RaggedTensor that stores the fixed-size word embedding for each
word in a sentence, for each sentence in a batch, could be written as [num_sentences, (num_words), embedding_size] .  The parentheses around (num_words) indicate that dimension is ragged, and that the length
of each element list in that dimension may vary for each item. Component Tensors Internally, a RaggedTensor consists of a concatenated list of values that
are partitioned into variable-length rows.  In particular, each RaggedTensor consists of: A values tensor, which concatenates the variable-length rows into a
flattened list.  For example, the values tensor for [[3, 1, 4, 1], [], [5, 9, 2], [6], []] is [3, 1, 4, 1, 5, 9, 2, 6] . A row_splits vector, which indicates how those flattened values are
divided into rows.  In particular, the values for row rt[i] are stored
in the slice rt.values[rt.row_splits[i]:rt.row_splits[i+1]] . Example: print ( tf . RaggedTensor . from_row_splits ( values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ], row_splits = [ 0 , 4 , 4 , 7 , 8 , 8 ])) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > Alternative Row-Partitioning Schemes In addition to row_splits , ragged tensors provide support for five other
row-partitioning schemes: row_lengths : a vector with shape [nrows] , which specifies the length
of each row. value_rowids and nrows : value_rowids is a vector with shape [nvals] , corresponding one-to-one with values , which specifies
each value's row index.  In particular, the row rt[row] consists of the
values rt.values[j] where value_rowids[j]==row . nrows is an
integer scalar that specifies the number of rows in the RaggedTensor . ( nrows is used to indicate trailing empty rows.) row_starts : a vector with shape [nrows] , which specifies the start
offset of each row.  Equivalent to row_splits[:-1] . row_limits : a vector with shape [nrows] , which specifies the stop
offset of each row.  Equivalent to row_splits[1:] . uniform_row_length : A scalar tensor, specifying the length of every
row.  This row-partitioning scheme may only be used if all rows have
the same length. Example: The following ragged tensors are equivalent, and all represent the
nested list [[3, 1, 4, 1], [], [5, 9, 2], [6], []] . values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ] RaggedTensor . from_row_splits ( values , row_splits = [ 0 , 4 , 4 , 7 , 8 , 8 ]) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > RaggedTensor . from_row_lengths ( values , row_lengths = [ 4 , 0 , 3 , 1 , 0 ]) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > RaggedTensor . from_value_rowids ( values , value_rowids = [ 0 , 0 , 0 , 0 , 2 , 2 , 2 , 3 ], nrows = 5 ) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > RaggedTensor . from_row_starts ( values , row_starts = [ 0 , 4 , 4 , 7 , 8 ]) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > RaggedTensor . from_row_limits ( values , row_limits = [ 4 , 4 , 7 , 8 , 8 ]) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > RaggedTensor . from_uniform_row_length ( values , uniform_row_length = 2 ) < tf . RaggedTensor [[ 3 , 1 ], [ 4 , 1 ], [ 5 , 9 ], [ 2 , 6 ]] > Multiple Ragged Dimensions RaggedTensor s with multiple ragged dimensions can be defined by using
a nested RaggedTensor for the values tensor.  Each nested RaggedTensor adds a single ragged dimension. inner_rt = RaggedTensor . from_row_splits ( # =rt1 from above values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ], row_splits = [ 0 , 4 , 4 , 7 , 8 , 8 ]) outer_rt = RaggedTensor . from_row_splits ( values = inner_rt , row_splits = [ 0 , 3 , 3 , 5 ]) print ( outer_rt . to_list ()) [[[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ]], [], [[ 6 ], []]] print ( outer_rt . ragged_rank ) 2 The factory function RaggedTensor.from_nested_row_splits may be used to
construct a RaggedTensor with multiple ragged dimensions directly, by
providing a list of row_splits tensors: RaggedTensor . from_nested_row_splits ( flat_values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ], nested_row_splits = ([ 0 , 3 , 3 , 5 ], [ 0 , 4 , 4 , 7 , 8 , 8 ])) . to_list () [[[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ]], [], [[ 6 ], []]] Uniform Inner Dimensions RaggedTensor s with uniform inner dimensions can be defined
by using a multidimensional Tensor for values . rt = RaggedTensor . from_row_splits ( values = tf . ones ([ 5 , 3 ], tf . int32 ), row_splits = [ 0 , 2 , 5 ]) print ( rt . to_list ()) [[[ 1 , 1 , 1 ], [ 1 , 1 , 1 ]], [[ 1 , 1 , 1 ], [ 1 , 1 , 1 ], [ 1 , 1 , 1 ]]] print ( rt . shape ) ( 2 , None , 3 ) Uniform Outer Dimensions RaggedTensor s with uniform outer dimensions can be defined by using
one or more RaggedTensor with a uniform_row_length row-partitioning
tensor.  For example, a RaggedTensor with shape [2, 2, None] can be
constructed with this method from a RaggedTensor values with shape [4, None] : values = tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 ], [ 5 , 6 ], [ 7 , 8 , 9 , 10 ]]) print ( values . shape ) ( 4 , None ) rt6 = tf . RaggedTensor . from_uniform_row_length ( values , 2 ) print ( rt6 ) < tf . RaggedTensor [[[ 1 , 2 , 3 ], [ 4 ]], [[ 5 , 6 ], [ 7 , 8 , 9 , 10 ]]] > print ( rt6 . shape ) ( 2 , 2 , None ) Note that rt6 only contains one ragged dimension (the innermost
dimension). In contrast, if from_row_splits is used to construct a similar RaggedTensor , then that RaggedTensor will have two ragged dimensions: rt7 = tf . RaggedTensor . from_row_splits ( values , [ 0 , 2 , 4 ]) print ( rt7 . shape ) ( 2 , None , None ) Uniform and ragged outer dimensions may be interleaved, meaning that a
tensor with any combination of ragged and uniform dimensions may be created.
For example, a RaggedTensor t4 with shape [3, None, 4, 8, None, 2] could
be constructed as follows: t0 = tf . zeros ([ 1000 , 2 ]) # Shape:         [1000, 2] t1 = RaggedTensor . from_row_lengths ( t0 , [ ... ]) #           [160, None, 2] t2 = RaggedTensor . from_uniform_row_length ( t1 , 8 ) #         [20, 8, None, 2] t3 = RaggedTensor . from_uniform_row_length ( t2 , 4 ) #       [5, 4, 8, None, 2] t4 = RaggedTensor . from_row_lengths ( t3 , [ ... ]) # [3, None, 4, 8, None, 2] Attributes dtype The DType of values in this tensor. flat_values The innermost values tensor for this ragged tensor. Concretely, if rt.values is a Tensor , then rt.flat_values is rt.values ; otherwise, rt.flat_values is rt.values.flat_values . Conceptually, flat_values is the tensor formed by flattening the
outermost dimension and all of the ragged dimensions into a single
dimension. rt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:] (where nvals is the number of items in the flattened dimensions). Example: rt = tf . ragged . constant ([[[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ]], [], [[ 6 ], []]]) print ( rt . flat_values ) tf . Tensor ([ 3 1 4 1 5 9 2 6 ], shape = ( 8 ,), dtype = int32 ) nested_row_splits A tuple containing the row_splits for all ragged dimensions. rt.nested_row_splits is a tuple containing the row_splits tensors for
all ragged dimensions in rt , ordered from outermost to innermost.  In
particular, rt.nested_row_splits = (rt.row_splits,) + value_splits where: * ` value_splits = () ` if ` rt . values ` is a ` Tensor ` . * ` value_splits = rt . values . nested_row_splits ` otherwise . Example: rt = tf . ragged . constant ( [[[[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ]], [], [[ 6 ], []]]]) for i , splits in enumerate ( rt . nested_row_splits ): print ( 'Splits for dimension %d : %s ' % ( i + 1 , splits . numpy ())) Splits for dimension 1 : [ 0 3 ] Splits for dimension 2 : [ 0 3 3 5 ] Splits for dimension 3 : [ 0 4 4 7 8 8 ] ragged_rank The number of times the RaggedTensor's flat_values is partitioned. values = tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 ], [ 5 , 6 ], [ 7 , 8 , 9 , 10 ]]) values . ragged_rank 1 rt = tf . RaggedTensor . from_uniform_row_length ( values , 2 ) rt . ragged_rank 2 row_splits The row-split indices for this ragged tensor's values . rt.row_splits specifies where the values for each row begin and end in rt.values .  In particular, the values for row rt[i] are stored in
the slice rt.values[rt.row_splits[i]:rt.row_splits[i+1]] . Example: rt = tf . ragged . constant ([[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []]) print ( rt . row_splits ) # indices of row splits in rt.values tf . Tensor ([ 0 4 4 7 8 8 ], shape = ( 6 ,), dtype = int64 ) shape The statically known shape of this ragged tensor. tf . ragged . constant ([[ 0 ], [ 1 , 2 ]]) . shape TensorShape ([ 2 , None ]) tf . ragged . constant ([[[ 0 , 1 ]], [[ 1 , 2 ], [ 3 , 4 ]]], ragged_rank = 1 ) . shape TensorShape ([ 2 , None , 2 ]) uniform_row_length The length of each row in this ragged tensor, or None if rows are ragged. rt1 = tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 ], [ 5 , 6 ], [ 7 , 8 , 9 , 10 ]]) print ( rt1 . uniform_row_length ) # rows are ragged. None rt2 = tf . RaggedTensor . from_uniform_row_length ( values = rt1 , uniform_row_length = 2 ) print ( rt2 ) < tf . RaggedTensor [[[ 1 , 2 , 3 ], [ 4 ]], [[ 5 , 6 ], [ 7 , 8 , 9 , 10 ]]] > print ( rt2 . uniform_row_length ) # rows are not ragged (all have size 2). tf . Tensor ( 2 , shape = (), dtype = int64 ) A RaggedTensor's rows are only considered to be uniform (i.e. non-ragged)
if it can be determined statically (at graph construction time) that the
rows all have the same length. values The concatenated rows for this ragged tensor. rt.values is a potentially ragged tensor formed by flattening the two
outermost dimensions of rt into a single dimension. rt.values.shape = [nvals] + rt.shape[2:] (where nvals is the
number of items in the outer two dimensions of rt ). rt.ragged_rank = self.ragged_rank - 1 Example: rt = tf . ragged . constant ([[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []]) print ( rt . values ) tf . Tensor ([ 3 1 4 1 5 9 2 6 ], shape = ( 8 ,), dtype = int32 ) Methods bounding_shape View source bounding_shape ( axis = None , name = None , out_type = None ) Returns the tight bounding box shape for this RaggedTensor . Args axis An integer scalar or vector indicating which axes to return the
bounding box for.  If not specified, then the full bounding box is
returned. name A name prefix for the returned tensor (optional). out_type dtype for the returned tensor.  Defaults to self.row_splits.dtype . Returns An integer Tensor ( dtype=self.row_splits.dtype ).  If axis is not
specified, then output is a vector with output.shape=[self.shape.ndims] .  If axis is a scalar, then the output is a scalar.  If axis is a vector, then output is a vector,
where output[i] is the bounding size for dimension axis[i] . Example: rt = tf . ragged . constant ([[ 1 , 2 , 3 , 4 ], [ 5 ], [], [ 6 , 7 , 8 , 9 ], [ 10 ]]) rt . bounding_shape () . numpy () array ([ 5 , 4 ]) consumers View source consumers () from_nested_row_lengths View source @classmethod from_nested_row_lengths ( flat_values , nested_row_lengths , name = None , validate = True ) Creates a RaggedTensor from a nested list of row_lengths tensors. Equivalent to: result = flat_values for row_lengths in reversed ( nested_row_lengths ): result = from_row_lengths ( result , row_lengths ) Args flat_values A potentially ragged tensor. nested_row_lengths A list of 1-D integer tensors.  The i th tensor is
used as the row_lengths for the i th ragged dimension. name A name prefix for the RaggedTensor (optional). validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. Returns A RaggedTensor (or flat_values if nested_row_lengths is empty). from_nested_row_splits View source @classmethod from_nested_row_splits ( flat_values , nested_row_splits , name = None , validate = True ) Creates a RaggedTensor from a nested list of row_splits tensors. Equivalent to: result = flat_values for row_splits in reversed ( nested_row_splits ): result = from_row_splits ( result , row_splits ) Args flat_values A potentially ragged tensor. nested_row_splits A list of 1-D integer tensors.  The i th tensor is
used as the row_splits for the i th ragged dimension. name A name prefix for the RaggedTensor (optional). validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. Returns A RaggedTensor (or flat_values if nested_row_splits is empty). from_nested_value_rowids View source @classmethod from_nested_value_rowids ( flat_values , nested_value_rowids , nested_nrows = None , name = None , validate = True ) Creates a RaggedTensor from a nested list of value_rowids tensors. Equivalent to: result = flat_values for ( rowids , nrows ) in reversed ( zip ( nested_value_rowids , nested_nrows )): result = from_value_rowids ( result , rowids , nrows ) Args flat_values A potentially ragged tensor. nested_value_rowids A list of 1-D integer tensors.  The i th tensor is
used as the value_rowids for the i th ragged dimension. nested_nrows A list of integer scalars.  The i th scalar is used as the nrows for the i th ragged dimension. name A name prefix for the RaggedTensor (optional). validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. Returns A RaggedTensor (or flat_values if nested_value_rowids is empty). Raises ValueError If len(nested_values_rowids) != len(nested_nrows) . from_row_lengths View source @classmethod from_row_lengths ( values , row_lengths , name = None , validate = True ) Creates a RaggedTensor with rows partitioned by row_lengths . The returned RaggedTensor corresponds with the python list defined by: result = [[ values . pop ( 0 ) for i in range ( length )] for length in row_lengths ] Args values A potentially ragged tensor with shape [nvals, ...] . row_lengths A 1-D integer tensor with shape [nrows] .  Must be
nonnegative. sum(row_lengths) must be nvals . name A name prefix for the RaggedTensor (optional). validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. Returns A RaggedTensor . result.rank = values.rank + 1 . result.ragged_rank = values.ragged_rank + 1 . Example: print ( tf . RaggedTensor . from_row_lengths ( values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ], row_lengths = [ 4 , 0 , 3 , 1 , 0 ])) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > from_row_limits View source @classmethod from_row_limits ( values , row_limits , name = None , validate = True ) Creates a RaggedTensor with rows partitioned by row_limits . Equivalent to: from_row_splits(values, concat([0, row_limits])) . Args values A potentially ragged tensor with shape [nvals, ...] . row_limits A 1-D integer tensor with shape [nrows] .  Must be sorted in
ascending order.  If nrows>0 , then row_limits[-1] must be nvals . name A name prefix for the RaggedTensor (optional). validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. Returns A RaggedTensor . result.rank = values.rank + 1 . result.ragged_rank = values.ragged_rank + 1 . Example: print ( tf . RaggedTensor . from_row_limits ( values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ], row_limits = [ 4 , 4 , 7 , 8 , 8 ])) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > from_row_splits View source @classmethod from_row_splits ( values , row_splits , name = None , validate = True ) Creates a RaggedTensor with rows partitioned by row_splits . The returned RaggedTensor corresponds with the python list defined by: result = [ values [ row_splits [ i ]: row_splits [ i + 1 ]] for i in range ( len ( row_splits ) - 1 )] Args values A potentially ragged tensor with shape [nvals, ...] . row_splits A 1-D integer tensor with shape [nrows+1] .  Must not be
empty, and must be sorted in ascending order. row_splits[0] must be
zero and row_splits[-1] must be nvals . name A name prefix for the RaggedTensor (optional). validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. Returns A RaggedTensor . result.rank = values.rank + 1 . result.ragged_rank = values.ragged_rank + 1 . Raises ValueError If row_splits is an empty list. Example: print ( tf . RaggedTensor . from_row_splits ( values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ], row_splits = [ 0 , 4 , 4 , 7 , 8 , 8 ])) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > from_row_starts View source @classmethod from_row_starts ( values , row_starts , name = None , validate = True ) Creates a RaggedTensor with rows partitioned by row_starts . Equivalent to: from_row_splits(values, concat([row_starts, nvals])) . Args values A potentially ragged tensor with shape [nvals, ...] . row_starts A 1-D integer tensor with shape [nrows] .  Must be
nonnegative and sorted in ascending order.  If nrows>0 , then row_starts[0] must be zero. name A name prefix for the RaggedTensor (optional). validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. Returns A RaggedTensor . result.rank = values.rank + 1 . result.ragged_rank = values.ragged_rank + 1 . Example: print ( tf . RaggedTensor . from_row_starts ( values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ], row_starts = [ 0 , 4 , 4 , 7 , 8 ])) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > from_sparse View source @classmethod from_sparse ( st_input , name = None , row_splits_dtype = tf . dtypes . int64 ) Converts a 2D tf.sparse.SparseTensor to a RaggedTensor . Each row of the output RaggedTensor will contain the explicit values
from the same row in st_input . st_input must be ragged-right.  If not
it is not ragged-right, then an error will be generated. Example: indices = [[ 0 , 0 ], [ 0 , 1 ], [ 0 , 2 ], [ 1 , 0 ], [ 3 , 0 ]] st = tf . sparse . SparseTensor ( indices = indices , values = [ 1 , 2 , 3 , 4 , 5 ], dense_shape = [ 4 , 3 ]) tf . RaggedTensor . from_sparse ( st ) . to_list () [[ 1 , 2 , 3 ], [ 4 ], [], [ 5 ]] Currently, only two-dimensional SparseTensors are supported. Args st_input The sparse tensor to convert.  Must have rank 2. name A name prefix for the returned tensors (optional). row_splits_dtype dtype for the returned RaggedTensor 's row_splits tensor.  One of tf.int32 or tf.int64 . Returns A RaggedTensor with the same values as st_input . output.ragged_rank = rank(st_input) - 1 . output.shape = [st_input.dense_shape[0], None] . Raises ValueError If the number of dimensions in st_input is not known
statically, or is not two. from_tensor View source @classmethod from_tensor ( tensor , lengths = None , padding = None , ragged_rank = 1 , name = None , row_splits_dtype = tf . dtypes . int64 ) Converts a tf.Tensor into a RaggedTensor . The set of absent/default values may be specified using a vector of lengths
or a padding value (but not both).  If lengths is specified, then the
output tensor will satisfy output[row] = tensor[row][:lengths[row]] . If
'lengths' is a list of lists or tuple of lists, those lists will be used
as nested row lengths. If padding is specified, then any row suffix consisting entirely of padding will be excluded from the returned RaggedTensor .  If neither lengths nor padding is specified, then the
returned RaggedTensor will have no absent/default values. Examples: dt = tf . constant ([[ 5 , 7 , 0 ], [ 0 , 3 , 0 ], [ 6 , 0 , 0 ]]) tf . RaggedTensor . from_tensor ( dt ) < tf . RaggedTensor [[ 5 , 7 , 0 ], [ 0 , 3 , 0 ], [ 6 , 0 , 0 ]] > tf . RaggedTensor . from_tensor ( dt , lengths = [ 1 , 0 , 3 ]) < tf . RaggedTensor [[ 5 ], [], [ 6 , 0 , 0 ]] > tf . RaggedTensor . from_tensor ( dt , padding = 0 ) < tf . RaggedTensor [[ 5 , 7 ], [ 0 , 3 ], [ 6 ]] > dt = tf . constant ([[[ 5 , 0 ], [ 7 , 0 ], [ 0 , 0 ]], [[ 0 , 0 ], [ 3 , 0 ], [ 0 , 0 ]], [[ 6 , 0 ], [ 0 , 0 ], [ 0 , 0 ]]]) tf . RaggedTensor . from_tensor ( dt , lengths = ([ 2 , 0 , 3 ], [ 1 , 1 , 2 , 0 , 1 ])) < tf . RaggedTensor [[[ 5 ], [ 7 ]], [], [[ 6 , 0 ], [], [ 0 ]]] > Args tensor The Tensor to convert.  Must have rank ragged_rank + 1 or
higher. lengths An optional set of row lengths, specified using a 1-D integer Tensor whose length is equal to tensor.shape[0] (the number of rows
in tensor ).  If specified, then output[row] will contain tensor[row][:lengths[row]] .  Negative lengths are treated as zero. You
  may optionally pass a list or tuple of lengths to this argument, which
  will be used as nested row lengths to construct a ragged tensor with
  multiple ragged dimensions. padding An optional padding value.  If specified, then any row suffix
consisting entirely of padding will be excluded from the returned
RaggedTensor. padding is a Tensor with the same dtype as tensor and with shape=tensor.shape[ragged_rank + 1:] . ragged_rank Integer specifying the ragged rank for the returned RaggedTensor .  Must be greater than zero. name A name prefix for the returned tensors (optional). row_splits_dtype dtype for the returned RaggedTensor 's row_splits tensor.  One of tf.int32 or tf.int64 . Returns A RaggedTensor with the specified ragged_rank .  The shape of the
returned ragged tensor is compatible with the shape of tensor . Raises ValueError If both lengths and padding are specified. ValueError If the rank of tensor is 0 or 1. from_uniform_row_length View source @classmethod from_uniform_row_length ( values , uniform_row_length , nrows = None , validate = True , name = None ) Creates a RaggedTensor with rows partitioned by uniform_row_length . This method can be used to create RaggedTensor s with multiple uniform
outer dimensions.  For example, a RaggedTensor with shape [2, 2, None] can be constructed with this method from a RaggedTensor values with shape [4, None] : values = tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 ], [ 5 , 6 ], [ 7 , 8 , 9 , 10 ]]) print ( values . shape ) ( 4 , None ) rt1 = tf . RaggedTensor . from_uniform_row_length ( values , 2 ) print ( rt1 ) < tf . RaggedTensor [[[ 1 , 2 , 3 ], [ 4 ]], [[ 5 , 6 ], [ 7 , 8 , 9 , 10 ]]] > print ( rt1 . shape ) ( 2 , 2 , None ) Note that rt1 only contains one ragged dimension (the innermost
dimension). In contrast, if from_row_splits is used to construct a similar RaggedTensor , then that RaggedTensor will have two ragged dimensions: rt2 = tf . RaggedTensor . from_row_splits ( values , [ 0 , 2 , 4 ]) print ( rt2 . shape ) ( 2 , None , None ) Args values A potentially ragged tensor with shape [nvals, ...] . uniform_row_length A scalar integer tensor.  Must be nonnegative. The
size of the outer axis of values must be evenly divisible by uniform_row_length . nrows The number of rows in the constructed RaggedTensor.  If not
specified, then it defaults to nvals/uniform_row_length (or 0 if uniform_row_length==0 ). nrows only needs to be specified if uniform_row_length might be zero. uniform_row_length*nrows must be nvals . validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. name A name prefix for the RaggedTensor (optional). Returns A RaggedTensor that corresponds with the python list defined by: result = [[ values . pop ( 0 ) for i in range ( uniform_row_length )] for _ in range ( nrows )] result.rank = values.rank + 1 . result.ragged_rank = values.ragged_rank + 1 . from_value_rowids View source @classmethod from_value_rowids ( values , value_rowids , nrows = None , name = None , validate = True ) Creates a RaggedTensor with rows partitioned by value_rowids . The returned RaggedTensor corresponds with the python list defined by: result = [[ values [ i ] for i in range ( len ( values )) if value_rowids [ i ] == row ] for row in range ( nrows )] Args values A potentially ragged tensor with shape [nvals, ...] . value_rowids A 1-D integer tensor with shape [nvals] , which corresponds
one-to-one with values , and specifies each value's row index.  Must be
nonnegative, and must be sorted in ascending order. nrows An integer scalar specifying the number of rows.  This should be
specified if the RaggedTensor may containing empty training rows. Must
be greater than value_rowids[-1] (or zero if value_rowids is empty).
Defaults to value_rowids[-1] + 1 (or zero if value_rowids is empty). name A name prefix for the RaggedTensor (optional). validate If true, then use assertions to check that the arguments form
a valid RaggedTensor .  Note: these assertions incur a runtime cost,
  since they must be checked for each tensor value. Returns A RaggedTensor . result.rank = values.rank + 1 . result.ragged_rank = values.ragged_rank + 1 . Raises ValueError If nrows is incompatible with value_rowids . Example: print ( tf . RaggedTensor . from_value_rowids ( values = [ 3 , 1 , 4 , 1 , 5 , 9 , 2 , 6 ], value_rowids = [ 0 , 0 , 0 , 0 , 2 , 2 , 2 , 3 ], nrows = 5 )) < tf . RaggedTensor [[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []] > get_shape View source get_shape () -> tf . TensorShape The statically known shape of this ragged tensor. Returns A TensorShape containing the statically known shape of this ragged
tensor.  Ragged dimensions have a size of None . Alias for shape property. Examples: tf . ragged . constant ([[ 0 ], [ 1 , 2 ]]) . get_shape () TensorShape ([ 2 , None ]) tf . ragged . constant ( [[[ 0 , 1 ]], [[ 1 , 2 ], [ 3 , 4 ]]], ragged_rank = 1 ) . get_shape () TensorShape ([ 2 , None , 2 ]) merge_dims View source merge_dims ( outer_axis , inner_axis ) Merges outer_axis...inner_axis into a single dimension. Returns a copy of this RaggedTensor with the specified range of dimensions
flattened into a single dimension, with elements in row-major order. Examples: rt = tf . ragged . constant ([[[ 1 , 2 ], [ 3 ]], [[ 4 , 5 , 6 ]]]) print ( rt . merge_dims ( 0 , 1 )) < tf . RaggedTensor [[ 1 , 2 ], [ 3 ], [ 4 , 5 , 6 ]] > print ( rt . merge_dims ( 1 , 2 )) < tf . RaggedTensor [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]] > print ( rt . merge_dims ( 0 , 2 )) tf . Tensor ([ 1 2 3 4 5 6 ], shape = ( 6 ,), dtype = int32 ) To mimic the behavior of np.flatten (which flattens all dimensions), use rt.merge_dims(0, -1).  To mimic the behavior of tf.layers.Flatten (which
flattens all dimensions except the outermost batch dimension), use rt.merge_dims(1, -1)`. Args outer_axis int : The first dimension in the range of dimensions to
merge. May be negative if self.shape.rank is statically known. inner_axis int : The last dimension in the range of dimensions to merge.
May be negative if self.shape.rank is statically known. Returns A copy of this tensor, with the specified dimensions merged into a
single dimension.  The shape of the returned tensor will be self.shape[:outer_axis] + [N] + self.shape[inner_axis + 1:] , where N is the total number of slices in the merged dimensions. nested_row_lengths View source nested_row_lengths ( name = None ) Returns a tuple containing the row_lengths for all ragged dimensions. rt.nested_row_lengths() is a tuple containing the row_lengths tensors
for all ragged dimensions in rt , ordered from outermost to innermost. Args name A name prefix for the returned tensors (optional). Returns A tuple of 1-D integer Tensors .  The length of the tuple is equal to self.ragged_rank . nested_value_rowids View source nested_value_rowids ( name = None ) Returns a tuple containing the value_rowids for all ragged dimensions. rt.nested_value_rowids is a tuple containing the value_rowids tensors
for
all ragged dimensions in rt , ordered from outermost to innermost.  In
particular, rt.nested_value_rowids = (rt.value_rowids(),) + value_ids where: value_ids = () if rt.values is a Tensor . value_ids = rt.values.nested_value_rowids otherwise. Args name A name prefix for the returned tensors (optional). Returns A tuple of 1-D integer Tensor s. Example: rt = tf . ragged . constant ( [[[[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ]], [], [[ 6 ], []]]]) for i , ids in enumerate ( rt . nested_value_rowids ()): print ( 'row ids for dimension %d : %s ' % ( i + 1 , ids . numpy ())) row ids for dimension 1 : [ 0 0 0 ] row ids for dimension 2 : [ 0 0 0 2 2 ] row ids for dimension 3 : [ 0 0 0 0 2 2 2 3 ] nrows View source nrows ( out_type = None , name = None ) Returns the number of rows in this ragged tensor. I.e., the size of the outermost dimension of the tensor. Args out_type dtype for the returned tensor.  Defaults to self.row_splits.dtype . name A name prefix for the returned tensor (optional). Returns A scalar Tensor with dtype out_type . Example: rt = tf . ragged . constant ([[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []]) print ( rt . nrows ()) # rt has 5 rows. tf . Tensor ( 5 , shape = (), dtype = int64 ) numpy View source numpy () Returns a numpy array with the values for this RaggedTensor . Requires that this RaggedTensor was constructed in eager execution mode. Ragged dimensions are encoded using numpy arrays with dtype=object and rank=1 , where each element is a single row. Examples In the following example, the value returned by RaggedTensor.numpy() contains three numpy array objects: one for each row (with rank=1 and dtype=int64 ), and one to combine them (with rank=1 and dtype=object ): tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 , 5 ]], dtype = tf . int64 ) . numpy () array ([ array ([ 1 , 2 , 3 ]), array ([ 4 , 5 ])], dtype = object ) Uniform dimensions are encoded using multidimensional numpy array s.  In
the following example, the value returned by RaggedTensor.numpy() contains
a single numpy array object, with rank=2 and dtype=int64 : tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], dtype = tf . int64 ) . numpy () array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) Returns A numpy array . row_lengths View source row_lengths ( axis = 1 , name = None ) Returns the lengths of the rows in this ragged tensor. rt.row_lengths()[i] indicates the number of values in the i th row of rt . Args axis An integer constant indicating the axis whose row lengths should be
returned. name A name prefix for the returned tensor (optional). Returns A potentially ragged integer Tensor with shape self.shape[:axis] . Raises ValueError If axis is out of bounds. Example: rt = tf . ragged . constant ( [[[ 3 , 1 , 4 ], [ 1 ]], [], [[ 5 , 9 ], [ 2 ]], [[ 6 ]], []]) print ( rt . row_lengths ()) # lengths of rows in rt tf . Tensor ([ 2 0 2 1 0 ], shape = ( 5 ,), dtype = int64 ) print ( rt . row_lengths ( axis = 2 )) # lengths of axis=2 rows. < tf . RaggedTensor [[ 3 , 1 ], [], [ 2 , 1 ], [ 1 ], []] > row_limits View source row_limits ( name = None ) Returns the limit indices for rows in this ragged tensor. These indices specify where the values for each row end in self.values . rt.row_limits(self) is equal to rt.row_splits[:-1] . Args name A name prefix for the returned tensor (optional). Returns A 1-D integer Tensor with shape [nrows] .
The returned tensor is nonnegative, and is sorted in ascending order. Example: rt = tf . ragged . constant ([[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []]) print ( rt . values ) tf . Tensor ([ 3 1 4 1 5 9 2 6 ], shape = ( 8 ,), dtype = int32 ) print ( rt . row_limits ()) # indices of row limits in rt.values tf . Tensor ([ 4 4 7 8 8 ], shape = ( 5 ,), dtype = int64 ) row_starts View source row_starts ( name = None ) Returns the start indices for rows in this ragged tensor. These indices specify where the values for each row begin in self.values . rt.row_starts() is equal to rt.row_splits[:-1] . Args name A name prefix for the returned tensor (optional). Returns A 1-D integer Tensor with shape [nrows] .
The returned tensor is nonnegative, and is sorted in ascending order. Example: rt = tf . ragged . constant ([[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []]) print ( rt . values ) tf . Tensor ([ 3 1 4 1 5 9 2 6 ], shape = ( 8 ,), dtype = int32 ) print ( rt . row_starts ()) # indices of row starts in rt.values tf . Tensor ([ 0 4 4 7 8 ], shape = ( 5 ,), dtype = int64 ) to_list View source to_list () Returns a nested Python list with the values for this RaggedTensor . Requires that rt was constructed in eager execution mode. Returns A nested Python list . to_sparse View source to_sparse ( name = None ) Converts this RaggedTensor into a tf.sparse.SparseTensor . Example: rt = tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 ], [], [ 5 , 6 ]]) print ( rt . to_sparse ()) SparseTensor ( indices = tf . Tensor ( [[ 0 0 ] [ 0 1 ] [ 0 2 ] [ 1 0 ] [ 3 0 ] [ 3 1 ]], shape = ( 6 , 2 ), dtype = int64 ), values = tf . Tensor ([ 1 2 3 4 5 6 ], shape = ( 6 ,), dtype = int32 ), dense_shape = tf . Tensor ([ 4 3 ], shape = ( 2 ,), dtype = int64 )) Args name A name prefix for the returned tensors (optional). Returns A SparseTensor with the same values as self . to_tensor View source to_tensor ( default_value = None , name = None , shape = None ) Converts this RaggedTensor into a tf.Tensor . If shape is specified, then the result is padded and/or truncated to
the specified shape. Examples: rt = tf . ragged . constant ([[ 9 , 8 , 7 ], [], [ 6 , 5 ], [ 4 ]]) print ( rt . to_tensor ()) tf . Tensor ( [[ 9 8 7 ] [ 0 0 0 ] [ 6 5 0 ] [ 4 0 0 ]], shape = ( 4 , 3 ), dtype = int32 ) print ( rt . to_tensor ( shape = [ 5 , 2 ])) tf . Tensor ( [[ 9 8 ] [ 0 0 ] [ 6 5 ] [ 4 0 ] [ 0 0 ]], shape = ( 5 , 2 ), dtype = int32 ) Args default_value Value to set for indices not specified in self . Defaults
to zero. default_value must be broadcastable to self.shape[self.ragged_rank + 1:] . name A name prefix for the returned tensors (optional). shape The shape of the resulting dense tensor.  In particular, result.shape[i] is shape[i] (if shape[i] is not None), or self.bounding_shape(i) (otherwise). shape.rank must be None or
equal to self.rank . Returns A Tensor with shape ragged.bounding_shape(self) and the
values specified by the non-empty values in self .  Empty values are
assigned default_value . value_rowids View source value_rowids ( name = None ) Returns the row indices for the values in this ragged tensor. rt.value_rowids() corresponds one-to-one with the outermost dimension of rt.values , and specifies the row containing each value.  In particular,
the row rt[row] consists of the values rt.values[j] where rt.value_rowids()[j] == row . Args name A name prefix for the returned tensor (optional). Returns A 1-D integer Tensor with shape self.values.shape[:1] .
The returned tensor is nonnegative, and is sorted in ascending order. Example: rt = tf . ragged . constant ([[ 3 , 1 , 4 , 1 ], [], [ 5 , 9 , 2 ], [ 6 ], []]) print ( rt . values ) tf . Tensor ([ 3 1 4 1 5 9 2 6 ], shape = ( 8 ,), dtype = int32 ) print ( rt . value_rowids ()) # corresponds 1:1 with rt.values tf . Tensor ([ 0 0 0 0 2 2 2 3 ], shape = ( 8 ,), dtype = int64 ) with_flat_values View source with_flat_values ( new_values ) Returns a copy of self with flat_values replaced by new_value . Preserves cached row-partitioning tensors such as self.cached_nrows and self.cached_value_rowids if they have values. Args new_values Potentially ragged tensor that should replace self.flat_values .  Must have rank > 0 , and must have the same number
of rows as self.flat_values . Returns A RaggedTensor . result.rank = self.ragged_rank + new_values.rank . result.ragged_rank = self.ragged_rank + new_values.ragged_rank . with_row_splits_dtype View source with_row_splits_dtype ( dtype ) Returns a copy of this RaggedTensor with the given row_splits dtype. For RaggedTensors with multiple ragged dimensions, the row_splits for all
nested RaggedTensor objects are cast to the given dtype. Args dtype The dtype for row_splits .  One of tf.int32 or tf.int64 . Returns A copy of this RaggedTensor, with the row_splits cast to the given
type. with_values View source with_values ( new_values ) Returns a copy of self with values replaced by new_value . Preserves cached row-partitioning tensors such as self.cached_nrows and self.cached_value_rowids if they have values. Args new_values Potentially ragged tensor to use as the values for the
returned RaggedTensor .  Must have rank > 0 , and must have the same
number of rows as self.values . Returns A RaggedTensor . result.rank = 1 + new_values.rank . result.ragged_rank = 1 + new_values.ragged_rank __abs__ View source __abs__ ( name = None ) Computes the absolute value of a ragged tensor. Given a ragged tensor of integer or floating-point values, this operation
returns a ragged tensor of the same type, where each element contains the
absolute value of the corresponding element in the input. Given a ragged tensor x of complex numbers, this operation returns a tensor
of type float32 or float64 that is the absolute value of each element in x . For a complex number \(a + bj\), its absolute value is computed as
\(\sqrt{a^2 + b^2}\). For example: # real number x = tf . ragged . constant ([[ - 2.2 , 3.2 ], [ - 4.2 ]]) tf . abs ( x ) < tf . RaggedTensor [[ 2.2 , 3.2 ], [ 4.2 ]] > # complex number x = tf . ragged . constant ([[ - 2.2 + 4.7 j ], [ - 3.2 + 5.7 j ], [ - 4.2 + 6.7 j ]]) tf . abs ( x ) < tf . RaggedTensor [[ 5.189412298131649 ], [ 6.536818798161687 ], [ 7.907591289387685 ]] > Args name A name for the operation (optional). Returns A RaggedTensor of the same size and type as x , with absolute values.
Note, for complex64 or complex128 input, the returned RaggedTensor will be of type float32 or float64 , respectively. __add__ View source __add__ ( y , name = None ) Returns x + y element-wise. Example usages below. Add a scalar and a list: x = [ 1 , 2 , 3 , 4 , 5 ] y = 1 tf . add ( x , y ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 2 , 3 , 4 , 5 , 6 ], dtype = int32 ) > Note that binary + operator can be used instead: x = tf . convert_to_tensor ([ 1 , 2 , 3 , 4 , 5 ]) y = tf . convert_to_tensor ( 1 ) x + y < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 2 , 3 , 4 , 5 , 6 ], dtype = int32 ) > Add a tensor and a list of same shape: x = [ 1 , 2 , 3 , 4 , 5 ] y = tf . constant ([ 1 , 2 , 3 , 4 , 5 ]) tf . add ( x , y ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 2 , 4 , 6 , 8 , 10 ], dtype = int32 ) > Warning: If one of the inputs ( x or y ) is a tensor and the other is a
non-tensor, the non-tensor input will adopt (or get casted to) the data type
of the tensor input. This can potentially cause unwanted overflow or underflow
conversion. For example, x = tf . constant ([ 1 , 2 ], dtype = tf . int8 ) y = [ 2 ** 7 + 1 , 2 ** 7 + 2 ] tf . add ( x , y ) < tf . Tensor : shape = ( 2 ,), dtype = int8 , numpy = array ([ - 126 , - 124 ], dtype = int8 ) > When adding two input values of different shapes, Add follows NumPy
broadcasting rules. The two input array shapes are compared element-wise.
Starting with the trailing dimensions, the two dimensions either have to be
equal or one of them needs to be 1 . For example, x = np . ones ( 6 ) . reshape ( 1 , 2 , 1 , 3 ) y = np . ones ( 6 ) . reshape ( 2 , 1 , 3 , 1 ) tf . add ( x , y ) . shape . as_list () [ 2 , 2 , 3 , 3 ] Another example with two arrays of different dimension. x = np . ones ([ 1 , 2 , 1 , 4 ]) y = np . ones ([ 3 , 4 ]) tf . add ( x , y ) . shape . as_list () [ 1 , 2 , 3 , 4 ] The reduction version of this elementwise operation is tf.math.reduce_sum Args x A tf.Tensor . Must be one of the following types: bfloat16, half,
float16, float32, float64, uint8, uint16, uint32, uint64, int8, int16,
int32, int64, complex64, complex128, string. y A tf.Tensor . Must have the same type as x. name A name for the operation (optional) __and__ View source __and__ ( y , name = None ) Returns the truth value of elementwise x & y . Logical AND function. Requires that x and y have the same shape or have broadcast-compatible shapes. For example, y can be: A single Python boolean, where the result will be calculated by applying
logical AND with the single element to each element in x . A tf.Tensor object of dtype tf.bool of the same shape or broadcast-compatible shape. In this case, the result will be the element-wise logical AND of x and y . A tf.RaggedTensor object of dtype tf.bool of the same shape or broadcast-compatible shape. In this case, the result will be the element-wise logical AND of x and y . For example: # `y` is a Python boolean x = tf . ragged . constant ([[ True , False ], [ True ]]) y = True x & y < tf . RaggedTensor [[ True , False ], [ True ]] > tf . math . logical_and ( x , y ) # Equivalent of x & y < tf . RaggedTensor [[ True , False ], [ True ]] > y & x < tf . RaggedTensor [[ True , False ], [ True ]] > tf . math . reduce_all ( x & y ) # Reduce to a scalar bool Tensor. < tf . Tensor : shape = (), dtype = bool , numpy = False > # `y` is a tf.Tensor of the same shape. x = tf . ragged . constant ([[ True , False ], [ True , False ]]) y = tf . constant ([[ True , False ], [ False , True ]]) x & y < tf . RaggedTensor [[ True , False ], [ False , False ]] > # `y` is a tf.Tensor of a broadcast-compatible shape. x = tf . ragged . constant ([[ True , False ], [ True ]]) y = tf . constant ([[ True ], [ False ]]) x & y < tf . RaggedTensor [[ True , False ], [ False ]] > # `y` is a `tf.RaggedTensor` of the same shape. x = tf . ragged . constant ([[ True , False ], [ True ]]) y = tf . ragged . constant ([[ False , True ], [ True ]]) x & y < tf . RaggedTensor [[ False , False ], [ True ]] > # `y` is a `tf.RaggedTensor` of a broadcast-compatible shape. x = tf . ragged . constant ([[[ True , True , False ]], [[]], [[ True , False ]]]) y = tf . ragged . constant ([[[ True ]], [[ True ]], [[ False ]]], ragged_rank = 1 ) x & y < tf . RaggedTensor [[[ True , True , False ]], [[]], [[ False , False ]]] > Args y A Python boolean or a tf.Tensor or tf.RaggedTensor of dtype tf.bool . name A name for the operation (optional). Returns A tf.RaggedTensor of dtype tf.bool with the shape that x and y broadcast to. __bool__ View source __bool__ () Raises TypeError when a RaggedTensor is used as a Python bool. To prevent RaggedTensor from being used as a bool, this function always raise
TypeError when being called. For example: x = tf . ragged . constant ([[ 1 , 2 ], [ 3 ]]) result = True if x else False # Evaluate x as a bool value. Traceback ( most recent call last ): TypeError : RaggedTensor may not be used as a boolean . x = tf . ragged . constant ([[ 1 ]]) r = ( x == 1 ) # tf.RaggedTensor [[True]] if r : # Evaluate r as a bool value. pass Traceback ( most recent call last ): TypeError : RaggedTensor may not be used as a boolean . __div__ View source __div__ ( y , name = None ) Divides x / y elementwise (using Python 2 division operator semantics). (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide. This function divides x and y , forcing Python 2 semantics. That is, if x and y are both integers then the result will be an integer. This is in
contrast to Python 3, where division with / is always a float while division
with // is always an integer. Args x Tensor numerator of real numeric type. y Tensor denominator of real numeric type. name A name for the operation (optional). Returns x / y returns the quotient of x and y. Migrate to TF2 This function is deprecated in TF2. Prefer using the Tensor division operator, tf.divide , or tf.math.divide , which obey the Python 3 division operator
semantics.


Page: https://www.tensorflow.org/api_docs/python/tf/RaggedTensorSpec
View source on GitHub Type specification for a tf.RaggedTensor . Inherits From: TypeSpec , TraceType View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.RaggedTensorSpec tf . RaggedTensorSpec ( shape = None , dtype = tf . dtypes . float32 , ragged_rank = None , row_splits_dtype = tf . dtypes . int64 , flat_values_spec = None ) Used in the notebooks Used in the guide Ragged tensors Subword tokenizers Args shape The shape of the RaggedTensor, or None to allow any shape.  If a
shape is specified, then all ragged dimensions must have size None . dtype tf.DType of values in the RaggedTensor. ragged_rank Python integer, the number of times the RaggedTensor's
flat_values is partitioned.  Defaults to shape.ndims - 1 . row_splits_dtype dtype for the RaggedTensor's row_splits tensor. One
of tf.int32 or tf.int64 . flat_values_spec TypeSpec for flat_value of the RaggedTensor. It shall be
provided when the flat_values is a CompositeTensor rather then Tensor.
If both dtype and flat_values_spec and  are provided, dtype must
be the same as flat_values_spec.dtype . (experimental) Attributes dtype The tf.dtypes.DType specified by this type for the RaggedTensor. rt = tf . ragged . constant ([[ "a" ], [ "b" , "c" ]], dtype = tf . string ) tf . type_spec_from_value ( rt ) . dtype tf . string flat_values_spec The TypeSpec of the flat_values of RaggedTensor. ragged_rank The number of times the RaggedTensor's flat_values is partitioned. Defaults to shape.ndims - 1 . values = tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 ], [ 5 , 6 ], [ 7 , 8 , 9 , 10 ]]) tf . type_spec_from_value ( values ) . ragged_rank 1 rt1 = tf . RaggedTensor . from_uniform_row_length ( values , 2 ) tf . type_spec_from_value ( rt1 ) . ragged_rank 2 row_splits_dtype The tf.dtypes.DType of the RaggedTensor's row_splits . rt = tf . ragged . constant ([[ 1 , 2 , 3 ], [ 4 ]], row_splits_dtype = tf . int64 ) tf . type_spec_from_value ( rt ) . row_splits_dtype tf . int64 shape The statically known shape of the RaggedTensor. rt = tf . ragged . constant ([[ 0 ], [ 1 , 2 ]]) tf . type_spec_from_value ( rt ) . shape TensorShape ([ 2 , None ]) rt = tf . ragged . constant ([[[ 0 , 1 ]], [[ 1 , 2 ], [ 3 , 4 ]]], ragged_rank = 1 ) tf . type_spec_from_value ( rt ) . shape TensorShape ([ 2 , None , 2 ]) value_type The Python type for values that are compatible with this TypeSpec. In particular, all values that are compatible with this TypeSpec must be an
instance of this type. Methods experimental_as_proto View source experimental_as_proto () -> struct_pb2 . TypeSpecProto Returns a proto representation of the TypeSpec instance. Do NOT override for custom non-TF types. experimental_from_proto View source @classmethod experimental_from_proto ( proto : struct_pb2 . TypeSpecProto ) -> 'TypeSpec' Returns a TypeSpec instance based on the serialized proto. Do NOT override for custom non-TF types. Args proto Proto generated using 'experimental_as_proto'. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ struct_pb2 . TypeSpecProto ] Returns the type of proto associated with TypeSpec serialization. Do NOT override for custom non-TF types. from_value View source @classmethod from_value ( value ) is_compatible_with View source is_compatible_with ( spec_or_value ) Returns true if spec_or_value is compatible with this TypeSpec. Prefer using "is_subtype_of" and "most_specific_common_supertype" wherever
possible. Args spec_or_value A TypeSpec or TypeSpec associated value to compare against. is_subtype_of View source is_subtype_of ( other : tf . types . experimental . TraceType ) -> bool Returns True if self is a subtype of other . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args other A TraceType object. most_specific_common_supertype View source most_specific_common_supertype ( others : Sequence [ tf . types . experimental . TraceType ] ) -> Optional [ 'TypeSpec' ] Returns the most specific supertype TypeSpec  of self and others . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args others A sequence of TraceTypes. most_specific_compatible_type View source most_specific_compatible_type ( other : 'TypeSpec' ) -> 'TypeSpec' Returns the most specific TypeSpec compatible with self and other . (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use most_specific_common_supertype instead. Deprecated. Please use most_specific_common_supertype instead.
Do not override this function. Args other A TypeSpec . Raises ValueError If there is no TypeSpec that is compatible with both self and other . __eq__ View source __eq__ ( other ) -> bool Return self==value. __ne__ View source __ne__ ( other ) -> bool Return self!=value.


Page: https://www.tensorflow.org/api_docs/python/tf/RegisterGradient
View source on GitHub A decorator for registering the gradient function for an op type. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.RegisterGradient tf . RegisterGradient ( op_type ) This decorator is only used when defining a new op type. For an op
with m inputs and n outputs, the gradient function is a function
that takes the original Operation and n Tensor objects
(representing the gradients with respect to each output of the op),
and returns m Tensor objects (representing the partial gradients
with respect to each input of the op). For example, assuming that operations of type "Sub" take two
inputs x and y , and return a single output x - y , the
following gradient function would be registered: @tf . RegisterGradient ( "Sub" ) def _sub_grad ( unused_op , grad ): return grad , tf . negative ( grad ) The decorator argument op_type is the string type of an
operation. This corresponds to the OpDef.name field for the proto
that defines the operation. Args op_type The string type of an operation. This corresponds to the OpDef.name field for the proto that defines the operation. Raises TypeError If op_type is not string. Methods __call__ View source __call__ ( f : _T ) -> _T Registers the function f as gradient function for op_type .


Page: https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor
View source on GitHub Represents a sparse tensor. View aliases Main aliases tf.SparseTensor Compat aliases for migration See Migration guide for
more details. tf.compat.v1.SparseTensor , tf.compat.v1.sparse.SparseTensor tf . sparse . SparseTensor ( indices , values , dense_shape ) Used in the notebooks Used in the guide Used in the tutorials Working with sparse tensors Migrate from TPU embedding_columns to TPUEmbedding layer tf.data: Build TensorFlow input pipelines Ragged tensors Introduction to Tensors Client-efficient large-model federated learning via `federated_select` and sparse aggregation Text generation with an RNN TFX Estimator Component Tutorial TFX Keras Component Tutorial Graph-based Neural Structured Learning in TFX TensorFlow represents a sparse tensor as three separate dense tensors: indices , values , and dense_shape .  In Python, the three tensors are
collected into a SparseTensor class for ease of use.  If you have separate indices , values , and dense_shape tensors, wrap them in a SparseTensor object before passing to the ops below. Concretely, the sparse tensor SparseTensor(indices, values, dense_shape) comprises the following components, where N and ndims are the number
of values and number of dimensions in the SparseTensor , respectively: indices : A 2-D int64 tensor of shape [N, ndims] , which specifies the
indices of the elements in the sparse tensor that contain nonzero values
(elements are zero-indexed). For example, indices=[[1,3], [2,4]] specifies
that the elements with indexes of [1,3] and [2,4] have nonzero values. values : A 1-D tensor of any type and shape [N] , which supplies the
values for each element in indices . For example, given indices=[[1,3],
[2,4]] , the parameter values=[18, 3.6] specifies that element [1,3] of
the sparse tensor has a value of 18, and element [2,4] of the tensor has a
value of 3.6. dense_shape : A 1-D int64 tensor of shape [ndims] , which specifies the
dense_shape of the sparse tensor. Takes a list indicating the number of
elements in each dimension. For example, dense_shape=[3,6] specifies a
two-dimensional 3x6 tensor, dense_shape=[2,3,4] specifies a
three-dimensional 2x3x4 tensor, and dense_shape=[9] specifies a
one-dimensional tensor with 9 elements. The corresponding dense tensor satisfies: dense . shape = dense_shape dense [ tuple ( indices [ i ])] = values [ i ] By convention, indices should be sorted in row-major order (or equivalently
lexicographic order on the tuples indices[i] ). This is not enforced when SparseTensor objects are constructed, but most ops assume correct ordering.
If the ordering of sparse tensor st is wrong, a fixed version can be
obtained by calling tf.sparse.reorder(st) . Example: The sparse tensor SparseTensor ( indices = [[ 0 , 0 ], [ 1 , 2 ]], values = [ 1 , 2 ], dense_shape = [ 3 , 4 ]) represents the dense tensor [[ 1 , 0 , 0 , 0 ] [ 0 , 0 , 2 , 0 ] [ 0 , 0 , 0 , 0 ]] Args indices A 2-D int64 tensor of shape [N, ndims] . values A 1-D tensor of any type and shape [N] . dense_shape A 1-D int64 tensor of shape [ndims] . Raises ValueError When building an eager SparseTensor if dense_shape is
unknown or contains unknown elements (None or -1). Attributes dense_shape A 1-D Tensor of int64 representing the shape of the dense tensor. dtype The DType of elements in this tensor. graph The Graph that contains the index, value, and dense_shape tensors. indices The indices of non-zero values in the represented dense tensor. op The Operation that produces values as an output. shape Get the TensorShape representing the shape of the dense tensor. values The non-zero values in the represented dense tensor. Methods consumers View source consumers () eval View source eval ( feed_dict = None , session = None ) Evaluates this sparse tensor in a Session . Calling this method will execute all preceding operations that
produce the inputs needed for the operation that produces this
tensor. Note: Before invoking SparseTensor.eval() , its graph must have been
launched in a session, and either a default session must be
available, or session must be specified explicitly. Args feed_dict A dictionary that maps Tensor objects to feed values. See tf.Session.run for a description of the valid feed values. session (Optional.) The Session to be used to evaluate this sparse
tensor. If none, the default session will be used. Returns A SparseTensorValue object. from_value View source @classmethod from_value ( sparse_tensor_value ) get_shape View source get_shape () -> tf . TensorShape Get the TensorShape representing the shape of the dense tensor. Returns A TensorShape object. set_shape View source set_shape ( shape ) Updates the TensorShape representing the shape of the dense tensor. With eager execution this operates as a shape assertion.
Here the shapes match: st = tf . SparseTensor ( indices = [[ 0 , 0 ], [ 1 , 2 ]], values = [ 1 , 2 ], dense_shape = [ 3 , 4 ]) st . set_shape ([ 3 , 4 ]) Passing a None in the new shape allows any value for that axis: st . set_shape ([ 3 , None ]) An error is raised if an incompatible shape is passed. st . set_shape ([ 1 , 4 ]) Traceback ( most recent call last ): ValueError : Tensor 's shape (3, 4) is not compatible with supplied shape [ 1 , 4 ] When executing in a tf.function , or building a model using tf.keras.Input , SparseTensor.set_shape will merge the given shape with the current shape of this tensor, and set the tensor's shape to the
merged value (see tf.TensorShape.merge_with for details): st = tf . keras . Input ( shape = [ None , None , 3 ], sparse = True ) print ( st . shape ) ( None , None , None , 3 ) Dimensions set to None are not updated: st . set_shape ([ None , 224 , 224 , None ]) print ( st . shape ) ( None , 224 , 224 , 3 ) The main use case for this is to provide additional shape information
that cannot be inferred from the graph alone. Caution: set_shape ensures that the applied shape is compatible with
the existing shape, but it does not check at runtime. Setting
incorrect shapes can result in inconsistencies between the
statically-known graph and the runtime value of tensors. Args shape A TensorShape representing the shape of this tensor, a TensorShapeProto , a list, a tuple, or None. Raises ValueError If shape is not compatible with the current shape of
this tensor. with_values View source with_values ( new_values ) Returns a copy of self with values replaced by new_values . This method produces a new SparseTensor that has the same nonzero indices and same dense_shape , but updated values. Args new_values The values of the new SparseTensor . Needs to have the same
shape as the current .values Tensor . May have a different type than
the current values . Returns A SparseTensor with identical indices and shape but updated values. Example usage: st = tf . sparse . from_dense ([[ 1 , 0 , 2 , 0 ], [ 3 , 0 , 0 , 4 ]]) tf . sparse . to_dense ( st . with_values ([ 10 , 20 , 30 , 40 ])) # 4 nonzero values < tf . Tensor : shape = ( 2 , 4 ), dtype = int32 , numpy = array ([[ 10 , 0 , 20 , 0 ], [ 30 , 0 , 0 , 40 ]], dtype = int32 ) > __div__ View source __div__ ( y ) Component-wise divides a SparseTensor by a dense Tensor. Limitation : this Op only broadcasts the dense side to the sparse side, but not
the other direction. Args sp_indices A Tensor of type int64 .
2-D. N x R matrix with the indices of non-empty values in a
SparseTensor, possibly not in canonical ordering. sp_values A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , complex64 , int64 , qint8 , quint8 , qint32 , bfloat16 , qint16 , quint16 , uint16 , complex128 , half , uint32 , uint64 .
1-D. N non-empty values corresponding to sp_indices . sp_shape A Tensor of type int64 .
1-D.  Shape of the input SparseTensor. dense A Tensor . Must have the same type as sp_values . R -D.  The dense Tensor operand. name A name for the operation (optional). Returns A Tensor . Has the same type as sp_values . __mul__ View source __mul__ ( y ) Component-wise multiplies a SparseTensor by a dense Tensor. The output locations corresponding to the implicitly zero elements in the sparse
tensor will be zero (i.e., will not take up storage space), regardless of the
contents of the dense tensor (even if it's +/-INF and that INF*0 == NaN). Limitation : this Op only broadcasts the dense side to the sparse side, but not
the other direction. Args sp_indices A Tensor of type int64 .
2-D. N x R matrix with the indices of non-empty values in a
SparseTensor, possibly not in canonical ordering. sp_values A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , complex64 , int64 , qint8 , quint8 , qint32 , bfloat16 , qint16 , quint16 , uint16 , complex128 , half , uint32 , uint64 .
1-D. N non-empty values corresponding to sp_indices . sp_shape A Tensor of type int64 .
1-D.  Shape of the input SparseTensor. dense A Tensor . Must have the same type as sp_values . R -D.  The dense Tensor operand. name A name for the operation (optional). Returns A Tensor . Has the same type as sp_values . __truediv__ View source __truediv__ ( y ) Internal helper function for 'sp_t / dense_t'.


Page: https://www.tensorflow.org/api_docs/python/tf/SparseTensorSpec
View source on GitHub Type specification for a tf.sparse.SparseTensor . Inherits From: TypeSpec , TraceType View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.SparseTensorSpec tf . SparseTensorSpec ( shape = None , dtype = tf . dtypes . float32 ) Args shape The dense shape of the SparseTensor , or None to allow any dense
shape. dtype tf.DType of values in the SparseTensor . Attributes dtype The tf.dtypes.DType specified by this type for the SparseTensor. shape The tf.TensorShape specified by this type for the SparseTensor. value_type Methods experimental_as_proto View source experimental_as_proto () -> struct_pb2 . TypeSpecProto Returns a proto representation of the TypeSpec instance. Do NOT override for custom non-TF types. experimental_from_proto View source @classmethod experimental_from_proto ( proto : struct_pb2 . TypeSpecProto ) -> 'TypeSpec' Returns a TypeSpec instance based on the serialized proto. Do NOT override for custom non-TF types. Args proto Proto generated using 'experimental_as_proto'. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ struct_pb2 . TypeSpecProto ] Returns the type of proto associated with TypeSpec serialization. Do NOT override for custom non-TF types. from_value View source @classmethod from_value ( value ) is_compatible_with View source is_compatible_with ( spec_or_value ) Returns true if spec_or_value is compatible with this TypeSpec. Prefer using "is_subtype_of" and "most_specific_common_supertype" wherever
possible. Args spec_or_value A TypeSpec or TypeSpec associated value to compare against. is_subtype_of View source is_subtype_of ( other : tf . types . experimental . TraceType ) -> bool Returns True if self is a subtype of other . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args other A TraceType object. most_specific_common_supertype View source most_specific_common_supertype ( others : Sequence [ tf . types . experimental . TraceType ] ) -> Optional [ 'TypeSpec' ] Returns the most specific supertype TypeSpec  of self and others . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args others A sequence of TraceTypes. most_specific_compatible_type View source most_specific_compatible_type ( other : 'TypeSpec' ) -> 'TypeSpec' Returns the most specific TypeSpec compatible with self and other . (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use most_specific_common_supertype instead. Deprecated. Please use most_specific_common_supertype instead.
Do not override this function. Args other A TypeSpec . Raises ValueError If there is no TypeSpec that is compatible with both self and other . __eq__ View source __eq__ ( other ) -> bool Return self==value. __ne__ View source __ne__ ( other ) -> bool Return self!=value.


Page: https://www.tensorflow.org/api_docs/python/tf/Tensor
View source on GitHub A tf.Tensor represents a multidimensional array of elements. View aliases Main aliases tf.experimental.numpy.ndarray Compat aliases for migration See Migration guide for
more details. tf.compat.v1.Tensor All elements are of a single known data type. When writing a TensorFlow program, the main object that is
manipulated and passed around is the tf.Tensor . A tf.Tensor has the following properties: a single data type (float32, int32, or string, for example) a shape TensorFlow supports eager execution and graph execution.  In eager
execution, operations are evaluated immediately.  In graph
execution, a computational graph is constructed for later
evaluation. TensorFlow defaults to eager execution.  In the example below, the
matrix multiplication results are calculated immediately. # Compute some values using a Tensor c = tf . constant ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]]) d = tf . constant ([[ 1.0 , 1.0 ], [ 0.0 , 1.0 ]]) e = tf . matmul ( c , d ) print ( e ) tf . Tensor ( [[ 1. 3. ] [ 3. 7. ]], shape = ( 2 , 2 ), dtype = float32 ) Note that during eager execution, you may discover your Tensors are actually
of type EagerTensor .  This is an internal detail, but it does give you
access to a useful function, numpy : type ( e ) < class '... ops . EagerTensor ' > print ( e . numpy ()) [[ 1. 3. ] [ 3. 7. ]] In TensorFlow, tf.function s are a common way to define graph execution. A Tensor's shape (that is, the rank of the Tensor and the size of
each dimension) may not always be fully known.  In tf.function definitions, the shape may only be partially known. Most operations produce tensors of fully-known shapes if the shapes of their
inputs are also fully known, but in some cases it's only possible to find the
shape of a tensor at execution time. A number of specialized tensors are available: see tf.Variable , tf.constant , tf.placeholder , tf.sparse.SparseTensor , and tf.RaggedTensor . Caution: when constructing a tensor from a numpy array or pandas dataframe
the underlying buffer may be re-used: a = np . array ([ 1 , 2 , 3 ]) b = tf . constant ( a ) a [ 0 ] = 4 print ( b ) # tf.Tensor([4 2 3], shape=(3,), dtype=int64) Note: this is an implementation detail that is subject to change and users
should not rely on this behaviour. For more on Tensors, see the guide . Attributes dtype The DType of elements in this tensor. name ndim shape Returns a tf.TensorShape that represents the shape of this tensor. t = tf . constant ([ 1 , 2 , 3 , 4 , 5 ]) t . shape TensorShape ([ 5 ]) tf.Tensor.shape is equivalent to tf.Tensor.get_shape() . In a tf.function or when building a model using tf.keras.Input , they return the build-time shape of the
tensor, which may be partially unknown. A tf.TensorShape is not a tensor. Use tf.shape(t) to get a tensor
containing the shape, calculated at runtime. See tf.Tensor.get_shape() , and tf.TensorShape for details and examples. Methods eval View source eval ( feed_dict = None , session = None ) Evaluates this tensor in a Session . Note: If you are not using compat.v1 libraries, you should not need this,
(or feed_dict or Session ).  In eager execution (or within tf.function )
you do not need to call eval . Calling this method will execute all preceding operations that
produce the inputs needed for the operation that produces this
tensor. Note: Before invoking Tensor.eval() , its graph must have been
launched in a session, and either a default session must be
available, or session must be specified explicitly. Args feed_dict A dictionary that maps Tensor objects to feed values. See tf.Session.run for a description of the valid feed values. session (Optional.) The Session to be used to evaluate this tensor. If
none, the default session will be used. Returns A numpy array corresponding to the value of this tensor. experimental_ref View source experimental_ref () DEPRECATED FUNCTION Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use ref() instead. get_shape View source get_shape () -> tf . TensorShape Returns a tf.TensorShape that represents the shape of this tensor. In eager execution the shape is always fully-known. a = tf . constant ([[ 1.0 , 2.0 , 3.0 ], [ 4.0 , 5.0 , 6.0 ]]) print ( a . shape ) ( 2 , 3 ) tf.Tensor.get_shape() is equivalent to tf.Tensor.shape . When executing in a tf.function or building a model using tf.keras.Input , Tensor.shape may return a partial shape (including None for unknown dimensions). See tf.TensorShape for more details. inputs = tf . keras . Input ( shape = [ 10 ]) # Unknown batch size print ( inputs . shape ) ( None , 10 ) The shape is computed using shape inference functions that are
registered for each tf.Operation . The returned tf.TensorShape is determined at build time, without
executing the underlying kernel. It is not a tf.Tensor . If you need a
shape tensor , either convert the tf.TensorShape to a tf.constant , or
use the tf.shape(tensor) function, which returns the tensor's shape at execution time. This is useful for debugging and providing early errors. For
example, when tracing a tf.function , no ops are being executed, shapes
may be unknown (See the Concrete Functions
Guide for details). @tf . function def my_matmul ( a , b ): result = a @b # the `print` executes during tracing. print ( "Result shape: " , result . shape ) return result The shape inference functions propagate shapes to the extent possible: f = my_matmul . get_concrete_function ( tf . TensorSpec ([ None , 3 ]), tf . TensorSpec ([ 3 , 5 ])) Result shape : ( None , 5 ) Tracing may fail if a shape missmatch can be detected: cf = my_matmul . get_concrete_function ( tf . TensorSpec ([ None , 3 ]), tf . TensorSpec ([ 4 , 5 ])) Traceback ( most recent call last ): ValueError : Dimensions must be equal , but are 3 and 4 for 'matmul' ( op : 'MatMul' ) with input shapes : [ ? , 3 ], [ 4 , 5 ] . In some cases, the inferred shape may have unknown dimensions. If
the caller has additional information about the values of these
dimensions, tf.ensure_shape or Tensor.set_shape() can be used to augment
the inferred shape. @tf . function def my_fun ( a ): a = tf . ensure_shape ( a , [ 5 , 5 ]) # the `print` executes during tracing. print ( "Result shape: " , a . shape ) return a cf = my_fun . get_concrete_function ( tf . TensorSpec ([ None , None ])) Result shape : ( 5 , 5 ) Returns A tf.TensorShape representing the shape of this tensor. ref View source ref () Returns a hashable reference object to this Tensor. The primary use case for this API is to put tensors in a set/dictionary.
We can't put tensors in a set/dictionary as tensor.__hash__() is no longer
available starting Tensorflow 2.0. The following will raise an exception starting 2.0 x = tf . constant ( 5 ) y = tf . constant ( 10 ) z = tf . constant ( 10 ) tensor_set = { x , y , z } Traceback ( most recent call last ): TypeError : Tensor is unhashable . Instead , use tensor . ref () as the key . tensor_dict = { x : 'five' , y : 'ten' } Traceback ( most recent call last ): TypeError : Tensor is unhashable . Instead , use tensor . ref () as the key . Instead, we can use tensor.ref() . tensor_set = { x . ref (), y . ref (), z . ref ()} x . ref () in tensor_set True tensor_dict = { x . ref (): 'five' , y . ref (): 'ten' , z . ref (): 'ten' } tensor_dict [ y . ref ()] 'ten' Also, the reference object provides .deref() function that returns the
original Tensor. x = tf . constant ( 5 ) x . ref () . deref () < tf . Tensor : shape = (), dtype = int32 , numpy = 5 > set_shape View source set_shape ( shape ) Updates the shape of this tensor. Note: It is recommended to use tf.ensure_shape instead of Tensor.set_shape , because tf.ensure_shape provides better checking for
programming errors and can create guarantees for compiler
optimization. With eager execution this operates as a shape assertion.
Here the shapes match: t = tf . constant ([[ 1 , 2 , 3 ]]) t . set_shape ([ 1 , 3 ]) Passing a None in the new shape allows any value for that axis: t . set_shape ([ 1 , None ]) An error is raised if an incompatible shape is passed. t . set_shape ([ 1 , 5 ]) Traceback ( most recent call last ): ValueError : Tensor 's shape (1, 3) is not compatible with supplied shape [ 1 , 5 ] When executing in a tf.function , or building a model using tf.keras.Input , Tensor.set_shape will merge the given shape with
the current shape of this tensor, and set the tensor's shape to the
merged value (see tf.TensorShape.merge_with for details): t = tf . keras . Input ( shape = [ None , None , 3 ]) print ( t . shape ) ( None , None , None , 3 ) Dimensions set to None are not updated: t . set_shape ([ None , 224 , 224 , None ]) print ( t . shape ) ( None , 224 , 224 , 3 ) The main use case for this is to provide additional shape information
that cannot be inferred from the graph alone. For example if you know all the images in a dataset have shape [28,28,3] you
can set it with tf.set_shape : @tf . function def load_image ( filename ): raw = tf . io . read_file ( filename ) image = tf . image . decode_png ( raw , channels = 3 ) # the `print` executes during tracing. print ( "Initial shape: " , image . shape ) image . set_shape ([ 28 , 28 , 3 ]) print ( "Final shape: " , image . shape ) return image Trace the function, see the Concrete Functions
Guide for details. cf = load_image . get_concrete_function ( tf . TensorSpec ([], dtype = tf . string )) Initial shape : ( None , None , 3 ) Final shape : ( 28 , 28 , 3 ) Similarly the tf.io.parse_tensor function could return a tensor with
any shape, even the tf.rank is unknown. If you know that all your
serialized tensors will be 2d, set it with set_shape : @tf . function def my_parse ( string_tensor ): result = tf . io . parse_tensor ( string_tensor , out_type = tf . float32 ) # the `print` executes during tracing. print ( "Initial shape: " , result . shape ) result . set_shape ([ None , None ]) print ( "Final shape: " , result . shape ) return result Trace the function concrete_parse = my_parse . get_concrete_function ( tf . TensorSpec ([], dtype = tf . string )) Initial shape : < unknown > Final shape : ( None , None ) Make sure it works: t = tf . ones ([ 5 , 3 ], dtype = tf . float32 ) serialized = tf . io . serialize_tensor ( t ) print ( serialized . dtype ) < dtype : 'string' > print ( serialized . shape ) () t2 = concrete_parse ( serialized ) print ( t2 . shape ) ( 5 , 3 ) Caution: set_shape ensures that the applied shape is compatible with
the existing shape, but it does not check at runtime. Setting
incorrect shapes can result in inconsistencies between the
statically-known graph and the runtime value of tensors. For runtime
validation of the shape, use tf.ensure_shape instead. It also modifies
the shape of the tensor. # Serialize a rank-3 tensor t = tf . ones ([ 5 , 5 , 5 ], dtype = tf . float32 ) serialized = tf . io . serialize_tensor ( t ) # The function still runs, even though it `set_shape([None,None])` t2 = concrete_parse ( serialized ) print ( t2 . shape ) ( 5 , 5 , 5 ) Args shape A TensorShape representing the shape of this tensor, a TensorShapeProto , a list, a tuple, or None. Raises ValueError If shape is not compatible with the current shape of
this tensor. __abs__ View source __abs__ ( name = None ) __add__ View source __add__ ( y ) __and__ View source __and__ ( y ) __array__ View source __array__ ( dtype = None ) __bool__ View source __bool__ () Dummy method to prevent a tensor from being used as a Python bool . This overload raises a TypeError when the user inadvertently
treats a Tensor as a boolean (most commonly in an if or while statement), in code that was not converted by AutoGraph. For example: if tf . constant ( True ): # Will raise. # ... if tf . constant ( 5 ) < tf . constant ( 7 ): # Will raise. # ... Raises TypeError . __div__ View source __div__ ( y ) __eq__ View source __eq__ ( other ) __floordiv__ View source __floordiv__ ( y ) __ge__ __ge__ ( y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Returns the truth value of (x >= y) element-wise. Note: math.greater_equal supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 , 7 ]) y = tf . constant ([ 5 , 2 , 5 , 10 ]) tf . math . greater_equal ( x , y ) == > [ True , True , True , False ] x = tf . constant ([ 5 , 4 , 6 , 7 ]) y = tf . constant ([ 5 ]) tf . math . greater_equal ( x , y ) == > [ True , False , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool . __getitem__ View source __getitem__ ( slice_spec , var = None ) Overload for Tensor. getitem . This operation extracts the specified region from the tensor.
The notation is similar to NumPy with the restriction that
currently only support basic indexing. That means that
using a non-scalar tensor as input is not currently allowed. Some useful examples: # Strip leading and trailing 2 elements foo = tf . constant ([ 1 , 2 , 3 , 4 , 5 , 6 ]) print ( foo [ 2 : - 2 ]) # => [3,4] # Skip every other row and reverse the order of the columns foo = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( foo [:: 2 ,:: - 1 ]) # => [[3,2,1], [9,8,7]] # Use scalar tensors as indices on both dimensions print ( foo [ tf . constant ( 0 ), tf . constant ( 2 )]) # => 3 # Insert another dimension foo = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( foo [ tf . newaxis , :, :]) # => [[[1,2,3], [4,5,6], [7,8,9]]] print ( foo [:, tf . newaxis , :]) # => [[[1,2,3]], [[4,5,6]], [[7,8,9]]] print ( foo [:, :, tf . newaxis ]) # => [[[1],[2],[3]], [[4],[5],[6]], [[ 7 ],[ 8 ],[ 9 ]]] # Ellipses (3 equivalent operations) foo = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( foo [ tf . newaxis , :, :]) # => [[[1,2,3], [4,5,6], [7,8,9]]] print ( foo [ tf . newaxis , ... ]) # => [[[1,2,3], [4,5,6], [7,8,9]]] print ( foo [ tf . newaxis ]) # => [[[1,2,3], [4,5,6], [7,8,9]]] # Masks foo = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) print ( foo [ foo > 2 ]) # => [3, 4, 5, 6, 7, 8, 9] Notes tf.newaxis is None as in NumPy. An implicit ellipsis is placed at the end of the slice_spec NumPy advanced indexing is currently not supported. Purpose in the API This method is exposed in TensorFlow's API so that library developers
can register dispatching for Tensor. getitem to allow it to handle
custom composite tensors & other custom objects. The API symbol is not intended to be called by users directly and does
appear in TensorFlow's generated documentation. Args tensor An tensor.Tensor object. slice_spec The arguments to Tensor. getitem . var In the case of variable slice assignment, the Variable object to slice
(i.e. tensor is the read-only view of this variable). Returns The appropriate slice of "tensor", based on "slice_spec". Raises ValueError If a slice range is negative size. TypeError If the slice indices aren't int, slice, ellipsis,
tf.newaxis or scalar int32/int64 tensors. __gt__ __gt__ ( y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Returns the truth value of (x > y) element-wise. Note: math.greater supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 2 , 5 ]) tf . math . greater ( x , y ) == > [ False , True , True ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . greater ( x , y ) == > [ False , False , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool . __invert__ View source __invert__ ( name = None ) __iter__ View source __iter__ () __le__ __le__ ( y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Returns the truth value of (x <= y) element-wise. Note: math.less_equal supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . less_equal ( x , y ) == > [ True , True , False ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 6 , 6 ]) tf . math . less_equal ( x , y ) == > [ True , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool . __len__ View source __len__ () __lt__ __lt__ ( y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Returns the truth value of (x < y) element-wise. Note: math.less supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . less ( x , y ) == > [ False , True , False ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 6 , 7 ]) tf . math . less ( x , y ) == > [ False , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool . __matmul__ View source __matmul__ ( y ) __mod__ View source __mod__ ( y ) __mul__ View source __mul__ ( y ) __ne__ View source __ne__ ( other ) __neg__ __neg__ ( name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Computes numerical negative value element-wise. I.e., \(y = -x\). Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , int8 , int16 , int32 , int64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x . If x is a SparseTensor , returns SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape) __nonzero__ View source __nonzero__ () Dummy method to prevent a tensor from being used as a Python bool . This is the Python 2.x counterpart to __bool__() above. Raises TypeError . __or__ View source __or__ ( y ) __pow__ View source __pow__ ( y ) __radd__ View source __radd__ ( x ) __rand__ View source __rand__ ( x ) __rdiv__ View source __rdiv__ ( x ) __rfloordiv__ View source __rfloordiv__ ( x ) __rmatmul__ View source __rmatmul__ ( x ) __rmod__ View source __rmod__ ( x ) __rmul__ View source __rmul__ ( x ) __ror__ View source __ror__ ( x ) __rpow__ View source __rpow__ ( x ) __rsub__ View source __rsub__ ( x ) __rtruediv__ View source __rtruediv__ ( x ) __rxor__ View source __rxor__ ( x ) __sub__ View source __sub__ ( y ) __truediv__ View source __truediv__ ( y ) __xor__ View source __xor__ ( y ) Class Variables OVERLOADABLE_OPERATORS { '__abs__' , '__add__' , '__and__' , '__div__' , '__eq__' , '__floordiv__' , '__ge__' , '__getitem__' , '__gt__' , '__invert__' , '__le__' , '__lt__' , '__matmul__' , '__mod__' , '__mul__' , '__ne__' , '__neg__' , '__or__' , '__pow__' , '__radd__' , '__rand__' , '__rdiv__' , '__rfloordiv__' , '__rmatmul__' , '__rmod__' , '__rmul__' , '__ror__' , '__rpow__' , '__rsub__' , '__rtruediv__' , '__rxor__' , '__sub__' , '__truediv__' , '__xor__' }


Page: https://www.tensorflow.org/api_docs/python/tf/TensorArray
View source on GitHub Class wrapping dynamic-sized, per-time-step, Tensor arrays. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.TensorArray tf . TensorArray ( dtype , size = None , dynamic_size = None , clear_after_read = None , tensor_array_name = None , handle = None , flow = None , infer_shape = True , element_shape = None , colocate_with_first_write_call = True , name = None ) Used in the notebooks Used in the guide Used in the tutorials Effective Tensorflow 2 Better performance with tf.function Playing CartPole with the Actor-Critic method Modeling COVID-19 spread in Europe and the effect of interventions Neural machine translation with attention Neural machine translation with a Transformer and Keras This class is meant to be used with dynamic iteration primitives such as while_loop and map_fn .  It supports gradient back-propagation via special
"flow" control flow dependencies. Note that although the array can be read multiple times and positions can be
overwritten, behavior may be undefined when storing multiple references to
the same array and clear_after_read is False. In particular, avoid using
methods like concat() to convert an intermediate TensorArray to a Tensor,
then further modifying the TensorArray, particularly if you need to backprop
through it later. Example 1: Plain reading and writing. ta = tf . TensorArray ( tf . float32 , size = 0 , dynamic_size = True , clear_after_read = False ) ta = ta . write ( 0 , 10 ) ta = ta . write ( 1 , 20 ) ta = ta . write ( 2 , 30 ) ta . read ( 0 ) < tf . Tensor : shape = (), dtype = float32 , numpy = 10.0 > ta . read ( 1 ) < tf . Tensor : shape = (), dtype = float32 , numpy = 20.0 > ta . read ( 2 ) < tf . Tensor : shape = (), dtype = float32 , numpy = 30.0 > ta . stack () < tf . Tensor : shape = ( 3 ,), dtype = float32 , numpy = array ([ 10. , 20. , 30. ], dtype = float32 ) > Example 2: Fibonacci sequence algorithm that writes in a loop then returns. @tf . function def fibonacci ( n ): ta = tf . TensorArray ( tf . float32 , size = 0 , dynamic_size = True ) ta = ta . unstack ([ 0. , 1. ]) for i in range ( 2 , n ): ta = ta . write ( i , ta . read ( i - 1 ) + ta . read ( i - 2 )) return ta . stack () fibonacci ( 7 ) < tf . Tensor : shape = ( 7 ,), dtype = float32 , numpy = array ([ 0. , 1. , 1. , 2. , 3. , 5. , 8. ], dtype = float32 ) > Example 3: A simple loop interacting with a tf.Variable . v = tf . Variable ( 1 ) @tf . function def f ( x ): ta = tf . TensorArray ( tf . int32 , size = 0 , dynamic_size = True ) for i in tf . range ( x ): v . assign_add ( i ) ta = ta . write ( i , v ) return ta . stack () f ( 5 ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 1 , 2 , 4 , 7 , 11 ], dtype = int32 ) > Args dtype (required) data type of the TensorArray. size (optional) int32 scalar Tensor : the size of the TensorArray.
Required if handle is not provided. dynamic_size (optional) Python bool: If true, writes to the TensorArray
can grow the TensorArray past its initial size.  Default: False. clear_after_read Boolean (optional, default: True).  If True, clear
TensorArray values after reading them.  This disables read-many
semantics, but allows early release of memory. tensor_array_name (optional) Python string: the name of the TensorArray.
This is used when creating the TensorArray handle.  If this value is
set, handle should be None. handle (optional) A Tensor handle to an existing TensorArray.  If this
is set, tensor_array_name should be None. Only supported in graph mode. flow (optional) A float Tensor scalar coming from an existing TensorArray.flow . Only supported in graph mode. infer_shape (optional, default: True) If True, shape inference is
enabled.  In this case, all elements must have the same shape. element_shape (optional, default: None) A TensorShape object specifying
the shape constraints of each of the elements of the TensorArray. Need
not be fully defined. colocate_with_first_write_call If True , the TensorArray will be
colocated on the same device as the Tensor used on its first write
(write operations include write , unstack , and split ).  If False ,
the TensorArray will be placed on the device determined by the device
context available during its initialization. name A name for the operation (optional). Raises ValueError if both handle and tensor_array_name are provided. TypeError if handle is provided but is not a Tensor. Attributes dtype The data type of this TensorArray. dynamic_size Python bool; if True the TensorArray can grow dynamically. element_shape The tf.TensorShape of elements in this TensorArray. flow The flow Tensor forcing ops leading to this TensorArray state. handle The reference to the TensorArray. Methods close View source close ( name = None ) Close the current TensorArray. Note: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method. concat View source concat ( name = None ) Return the values in the TensorArray as a concatenated Tensor . All of the values must have been written, their ranks must match, and
and their shapes must all match for all dimensions except the first. Args name A name for the operation (optional). Returns All the tensors in the TensorArray concatenated into one tensor. gather View source gather ( indices , name = None ) Return selected values in the TensorArray as a packed Tensor . All of selected values must have been written and their shapes
must all match. Args indices A 1-D Tensor taking values in [0, max_value) .  If the TensorArray is not dynamic, max_value=size() . name A name for the operation (optional). Returns The tensors in the TensorArray selected by indices , packed into one
tensor. grad View source grad ( source , flow = None , name = None ) identity View source identity () Returns a TensorArray with the same content and properties. Returns A new TensorArray object with flow that ensures the control dependencies
from the contexts will become control dependencies for writes, reads, etc.
Use this object for all subsequent operations. read View source read ( index , name = None ) Read the value at location index in the TensorArray. Args index 0-D.  int32 tensor with the index to read from. name A name for the operation (optional). Returns The tensor at index index . scatter View source scatter ( indices , value , name = None ) Scatter the values of a Tensor in specific indices of a TensorArray . Args indices A 1-D Tensor taking values in [0, max_value) .  If the TensorArray is not dynamic, max_value=size() . value (N+1)-D.  Tensor of type dtype .  The Tensor to unpack. name A name for the operation (optional). Returns A new TensorArray object with flow that ensures the scatter occurs.
Use this object for all subsequent operations. Raises ValueError if the shape inference fails. Note: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method. size View source size ( name = None ) Return the size of the TensorArray. split View source split ( value , lengths , name = None ) Split the values of a Tensor into the TensorArray. Args value (N+1)-D.  Tensor of type dtype .  The Tensor to split. lengths 1-D.  int32 vector with the lengths to use when splitting value along its first dimension. name A name for the operation (optional). Returns A new TensorArray object with flow that ensures the split occurs.
Use this object for all subsequent operations. Raises ValueError if the shape inference fails. Note: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method. stack View source stack ( name = None ) Return the values in the TensorArray as a stacked Tensor . All of the values must have been written and their shapes must all match.
If input shapes have rank- R , then output shape will have rank- (R+1) . For example: ta = tf . TensorArray ( tf . int32 , size = 3 ) ta = ta . write ( 0 , tf . constant ([ 1 , 2 ])) ta = ta . write ( 1 , tf . constant ([ 3 , 4 ])) ta = ta . write ( 2 , tf . constant ([ 5 , 6 ])) ta . stack () < tf . Tensor : shape = ( 3 , 2 ), dtype = int32 , numpy = array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], dtype = int32 ) > Args name A name for the operation (optional). Returns All the tensors in the TensorArray stacked into one tensor. unstack View source unstack ( value , name = None ) Unstack the values of a Tensor in the TensorArray. If input value shapes have rank- R , then the output TensorArray will
contain elements whose shapes are rank- (R-1) . Args value (N+1)-D.  Tensor of type dtype .  The Tensor to unstack. name A name for the operation (optional). Returns A new TensorArray object with flow that ensures the unstack occurs.
Use this object for all subsequent operations. Raises ValueError if the shape inference fails. Note: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method. write View source write ( index , value , name = None ) Write value into index index of the TensorArray. Args index 0-D.  int32 scalar with the index to write to. value N-D.  Tensor of type dtype .  The Tensor to write to this index. name A name for the operation (optional). Returns A new TensorArray object with flow that ensures the write occurs.
Use this object for all subsequent operations. Raises ValueError if there are more writers than specified. Note: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.


Page: https://www.tensorflow.org/api_docs/python/tf/TensorArraySpec
View source on GitHub Type specification for a tf.TensorArray . Inherits From: TypeSpec , TraceType View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.TensorArraySpec tf . TensorArraySpec ( element_shape = None , dtype = tf . dtypes . float32 , dynamic_size = False , infer_shape = True ) Args element_shape The shape of each element in the TensorArray . dtype Data type of the TensorArray . dynamic_size Whether the TensorArray can grow past its initial size. infer_shape Whether shape inference is enabled. Attributes value_type Methods experimental_as_proto View source experimental_as_proto () -> struct_pb2 . TypeSpecProto Returns a proto representation of the TypeSpec instance. Do NOT override for custom non-TF types. experimental_from_proto View source @classmethod experimental_from_proto ( proto : struct_pb2 . TypeSpecProto ) -> 'TypeSpec' Returns a TypeSpec instance based on the serialized proto. Do NOT override for custom non-TF types. Args proto Proto generated using 'experimental_as_proto'. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ struct_pb2 . TypeSpecProto ] Returns the type of proto associated with TypeSpec serialization. Do NOT override for custom non-TF types. from_value View source @staticmethod from_value ( value ) is_compatible_with View source is_compatible_with ( other ) Returns true if spec_or_value is compatible with this TypeSpec. Prefer using "is_subtype_of" and "most_specific_common_supertype" wherever
possible. Args spec_or_value A TypeSpec or TypeSpec associated value to compare against. is_subtype_of View source is_subtype_of ( other ) Returns True if self is a subtype of other . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args other A TraceType object. most_specific_common_supertype View source most_specific_common_supertype ( others ) Returns the most specific supertype of self and others . Args others A Sequence of TypeSpec . Returns None if a supertype does not exist. most_specific_compatible_type View source most_specific_compatible_type ( other : 'TypeSpec' ) -> 'TypeSpec' Returns the most specific TypeSpec compatible with self and other . (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use most_specific_common_supertype instead. Deprecated. Please use most_specific_common_supertype instead.
Do not override this function. Args other A TypeSpec . Raises ValueError If there is no TypeSpec that is compatible with both self and other . __eq__ View source __eq__ ( other ) -> bool Return self==value. __ne__ View source __ne__ ( other ) -> bool Return self!=value.


Page: https://www.tensorflow.org/api_docs/python/tf/TensorShape
View source on GitHub Represents the shape of a Tensor . Inherits From: TraceType View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.TensorShape tf . TensorShape ( dims ) Used in the notebooks Used in the guide Used in the tutorials TensorFlow 1.x vs TensorFlow 2 - Behaviors and APIs DTensor concepts Working with RNNs Distributed Input Tensorflow datasets from MongoDB collections t = tf . constant ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) t . shape TensorShape ([ 2 , 3 ]) TensorShape is the static shape representation of a Tensor.
During eager execution a Tensor always has a fully specified shape but
when tracing a tf.function it may be one of the following: Fully-known shape: has a known number of dimensions and a known size
for each dimension. e.g. TensorShape([16, 256]) Partially-known shape: has a known number of dimensions, and an unknown
size for one or more dimension. e.g. TensorShape([None, 256]) Unknown shape: has an unknown number of dimensions, and an unknown
size in all dimensions. e.g. TensorShape(None) During function tracing t.shape will return a TensorShape object
representing the shape of Tensor as it is known during tracing.
This static representation will be partially defined in cases where the
exact shape depends on the values within the tensors. To get the dynamic representation, please use tf.shape(t) which will return Tensor representing the fully defined shape of t .
This way, you can express logic that manipulates the shapes of tensors by
building other tensors that depend on the dynamic shape of t . Note: tf.RaggedTensor.shape also returns a tf.TensorShape ,
the lengths of any ragged dimensions are unknown ( None ). For example, this function prints the TensorShape' ( t.shape ), when you
trace the function, and returns a tensor <a href="../tf/shape"><code>tf.shape(t)</code></a> for given input t`: @tf . function def get_dynamic_shape ( t ): print ( "tracing..." ) print ( f "static shape is { t . shape } " ) return tf . shape ( t ) Just calling the function traces it with a fully-specified static shape: result = get_dynamic_shape ( tf . constant ([[ 1 , 1 , 1 ], [ 0 , 0 , 0 ]])) tracing ... static shape is ( 2 , 3 ) result . numpy () array ([ 2 , 3 ], dtype = int32 ) But tf.function can also trace the function with a partially specified
(or even unspecified) shape: cf1 = get_dynamic_shape . get_concrete_function ( tf . TensorSpec ( shape = [ None , 2 ])) tracing ... static shape is ( None , 2 ) cf1 ( tf . constant ([[ 1. , 0 ],[ 1 , 0 ],[ 1 , 0 ]])) . numpy () array ([ 3 , 2 ], dtype = int32 ) cf2 = get_dynamic_shape . get_concrete_function ( tf . TensorSpec ( shape = None )) tracing ... static shape is < unknown > cf2 ( tf . constant ([[[[[ 1. , 0 ]]]]])) . numpy () array ([ 1 , 1 , 1 , 1 , 2 ], dtype = int32 ) If a tensor is produced by an operation of type "Foo" , its shape
may be inferred if there is a registered shape function for "Foo" . See Shape
functions for details of shape functions and how to register them. Alternatively,
you may set the shape explicitly using tf.Tensor.ensure_shape . Args dims A list of Dimensions, or None if the shape is unspecified. Raises TypeError If dims cannot be converted to a list of dimensions. Attributes dims Deprecated.  Returns list of dimensions for this shape. Suggest TensorShape.as_list instead. ndims Deprecated accessor for rank . rank Returns the rank of this shape, or None if it is unspecified. Methods as_list View source as_list () Returns a list of integers or None for each dimension. Returns A list of integers or None for each dimension. Raises ValueError If self is an unknown shape with an unknown rank. as_proto View source as_proto () Returns this shape as a TensorShapeProto . assert_has_rank View source assert_has_rank ( rank ) Raises an exception if self is not compatible with the given rank . Args rank An integer. Raises ValueError If self does not represent a shape with the given rank . assert_is_compatible_with View source assert_is_compatible_with ( other ) Raises exception if self and other do not represent the same shape. This method can be used to assert that there exists a shape that both self and other represent. Args other Another TensorShape. Raises ValueError If self and other do not represent the same shape. assert_is_fully_defined View source assert_is_fully_defined () Raises an exception if self is not fully defined in every dimension. Raises ValueError If self does not have a known value for every dimension. assert_same_rank View source assert_same_rank ( other ) Raises an exception if self and other do not have compatible ranks. Args other Another TensorShape . Raises ValueError If self and other do not represent shapes with the
same rank. concatenate View source concatenate ( other ) Returns the concatenation of the dimension in self and other . Note: If either self or other is completely unknown,
concatenation will discard information about the other shape. In
future, we might support concatenation that preserves this
information for use with slicing. Args other Another TensorShape . Returns A TensorShape whose dimensions are the concatenation of the
dimensions in self and other . experimental_as_proto View source experimental_as_proto () -> tensor_shape_pb2 . TensorShapeProto Returns a proto representation of the TensorShape instance. experimental_from_proto View source @classmethod experimental_from_proto ( proto : tensor_shape_pb2 . TensorShapeProto ) -> 'TensorShape' Returns a TensorShape instance based on the serialized proto. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ tensor_shape_pb2 . TensorShapeProto ] Returns the type of proto associated with TensorShape serialization. is_compatible_with View source is_compatible_with ( other ) Returns True iff self is compatible with other . Two possibly-partially-defined shapes are compatible if there
exists a fully-defined shape that both shapes can represent. Thus,
compatibility allows the shape inference code to reason about
partially-defined shapes. For example: TensorShape(None) is compatible with all shapes. TensorShape([None, None]) is compatible with all two-dimensional
shapes, such as TensorShape([32, 784]), and also TensorShape(None). It is
not compatible with, for example, TensorShape([None]) or
TensorShape([None, None, None]). TensorShape([32, None]) is compatible with all two-dimensional shapes
with size 32 in the 0th dimension, and also TensorShape([None, None])
and TensorShape(None). It is not compatible with, for example,
TensorShape([32]), TensorShape([32, None, 1]) or TensorShape([64, None]). TensorShape([32, 784]) is compatible with itself, and also
TensorShape([32, None]), TensorShape([None, 784]), TensorShape([None,
None]) and TensorShape(None). It is not compatible with, for example,
TensorShape([32, 1, 784]) or TensorShape([None]). The compatibility relation is reflexive and symmetric, but not
transitive. For example, TensorShape([32, 784]) is compatible with
TensorShape(None), and TensorShape(None) is compatible with
TensorShape([4, 4]), but TensorShape([32, 784]) is not compatible with
TensorShape([4, 4]). Args other Another TensorShape. Returns True iff self is compatible with other . is_fully_defined View source is_fully_defined () Returns True iff self is fully defined in every dimension. is_subtype_of View source is_subtype_of ( other : tf . types . experimental . TraceType ) -> bool Returns True iff self is subtype of other . Shape A is a subtype of shape B if shape B can successfully represent it: A TensorShape of any rank is a subtype of TensorShape(None) . TensorShapes of equal ranks are covariant, i.e. TensorShape([A1, A2, ..]) is a subtype of TensorShape([B1, B2, ..]) iff An is a subtype of Bn. An is subtype of Bn iff An == Bn or Bn is None. TensorShapes of different defined ranks have no subtyping relation. The subtyping relation is reflexive and transitive, but not symmetric. Some examples: TensorShape([32, 784]) is a subtype of TensorShape(None) , and TensorShape([4, 4]) is also a subtype of TensorShape(None) but TensorShape([32, 784]) and TensorShape([4, 4]) are not subtypes of
each other. All two-dimensional shapes are subtypes of TensorShape([None, None]) ,
such as TensorShape([32, 784]) . There is no subtype relationship with,
for example, TensorShape([None]) or TensorShape([None, None, None]) . TensorShape([32, None]) is also a subtype of TensorShape([None, None]) and TensorShape(None) . It is not a subtype of, for example, TensorShape([32]) , TensorShape([32, None, 1]) , TensorShape([64, None]) or TensorShape([None, 32]) . TensorShape([32, 784]) is a subtype of itself, and also TensorShape([32, None]) , TensorShape([None, 784]) , TensorShape([None, None]) and TensorShape(None) .
It has no subtype relation with, for example, TensorShape([32, 1, 784]) or TensorShape([None]) . Args other Another TensorShape . Returns True iff self is subtype of other . merge_with View source merge_with ( other ) Returns a TensorShape combining the information in self and other . The dimensions in self and other are merged element-wise,
according to the rules below: Dimension ( n ) . merge_with ( Dimension ( None )) == Dimension ( n ) Dimension ( None ) . merge_with ( Dimension ( n )) == Dimension ( n ) Dimension ( None ) . merge_with ( Dimension ( None )) == Dimension ( None ) # raises ValueError for n != m Dimension ( n ) . merge_with ( Dimension ( m )) ts = tf.TensorShape([1,2])
ot1 = tf.TensorShape([1,2])
ts.merge_with(ot).as_list()
[1,2] ot2 = tf.TensorShape([1,None])
ts.merge_with(ot2).as_list()
[1,2] ot3 = tf.TensorShape([None, None])
ot3.merge_with(ot2).as_list()
[1, None] Args other Another TensorShape . Returns A TensorShape containing the combined information of self and other . Raises ValueError If self and other are not compatible. most_specific_common_supertype View source most_specific_common_supertype ( others : Sequence [ tf . types . experimental . TraceType ] ) -> Optional [ 'TensorShape' ] Returns the most specific supertype TensorShape of self and others. TensorShape([None, 1]) is the most specific TensorShape supertyping
both TensorShape([2, 1]) and TensorShape([5, 1]) . Note that TensorShape(None) is also a supertype but it is not "most specific". TensorShape([1, 2, 3]) is the most specific TensorShape supertyping
both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3] ). There are
other less specific TensorShapes that supertype above mentioned
TensorShapes, e.g. TensorShape([1, 2, None]) , TensorShape(None) . TensorShape([None, None]) is the most specific TensorShape supertyping both TensorShape([2, None]) and TensorShape([None, 3]) .
As always, TensorShape(None) is also a supertype but not the most
specific one. TensorShape(None ) is the only TensorShape supertyping both TensorShape([1, 2, 3]) and TensorShape([1, 2]) . In general, any two
shapes that have different ranks will only have TensorShape(None) as a common supertype. TensorShape(None) is the only TensorShape supertyping both TensorShape([1, 2, 3]) and TensorShape(None) . In general, the common
supertype of any shape with TensorShape(None) is TensorShape(None) . Args others Sequence of TensorShape . Returns A TensorShape which is the most specific supertype shape of self and others . None if it does not exist. most_specific_compatible_shape View source most_specific_compatible_shape ( other ) -> 'TensorShape' Returns the most specific TensorShape compatible with self and other . TensorShape([None, 1]) is the most specific TensorShape compatible with
both TensorShape([2, 1]) and TensorShape([5, 1]). Note that
TensorShape(None) is also compatible with above mentioned TensorShapes. TensorShape([1, 2, 3]) is the most specific TensorShape compatible with
both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3]). There are more
less specific TensorShapes compatible with above mentioned TensorShapes,
e.g. TensorShape([1, 2, None]), TensorShape(None). Args other Another TensorShape . Returns A TensorShape which is the most specific compatible shape of self and other . num_elements View source num_elements () Returns the total number of elements, or none for incomplete shapes. with_rank View source with_rank ( rank ) Returns a shape based on self with the given rank. This method promotes a completely unknown shape to one with a
known rank. Args rank An integer. Returns A shape that is at least as specific as self with the given rank. Raises ValueError If self does not represent a shape with the given rank . with_rank_at_least View source with_rank_at_least ( rank ) Returns a shape based on self with at least the given rank. Args rank An integer. Returns A shape that is at least as specific as self with at least the given
rank. Raises ValueError If self does not represent a shape with at least the given rank . with_rank_at_most View source with_rank_at_most ( rank ) Returns a shape based on self with at most the given rank. Args rank An integer. Returns A shape that is at least as specific as self with at most the given
rank. Raises ValueError If self does not represent a shape with at most the given rank . __add__ View source __add__ ( other ) __bool__ View source __bool__ () Returns True if this shape contains non-zero information. __concat__ View source __concat__ ( other ) __eq__ View source __eq__ ( other ) Returns True if self is equivalent to other . It first tries to convert other to TensorShape . TypeError is thrown
when the conversion fails. Otherwise, it compares each element in the
TensorShape dimensions. Two Fully known shapes, return True iff each element is equal. >>> t_a = tf . TensorShape ([ 1 , 2 ]) >>> a = [ 1 , 2 ] >>> t_b = tf . TensorShape ([ 1 , 2 ]) >>> t_c = tf . TensorShape ([ 1 , 2 , 3 ]) >>> t_a . __eq__ ( a ) True >>> t_a . __eq__ ( t_b ) True >>> t_a . __eq__ ( t_c ) False Two Partially-known shapes, return True iff each element is equal. >>> p_a = tf . TensorShape ([ 1 , None ]) >>> p_b = tf . TensorShape ([ 1 , None ]) >>> p_c = tf . TensorShape ([ 2 , None ]) >>> p_a . __eq__ ( p_b ) True >>> t_a . __eq__ ( p_a ) False >>> p_a . __eq__ ( p_c ) False Two Unknown shape , return True. >>> unk_a = tf . TensorShape ( None ) >>> unk_b = tf . TensorShape ( None ) >>> unk_a . __eq__ ( unk_b ) True >>> unk_a . __eq__ ( t_a ) False Args other A TensorShape or type that can be converted to TensorShape . Returns True if the dimensions are all equal. Raises TypeError if other can not be converted to TensorShape . __getitem__ View source __getitem__ ( key ) Returns the value of a dimension or a shape, depending on the key. Args key If key is an integer, returns the dimension at that index;
otherwise if key is a slice, returns a TensorShape whose dimensions
are those selected by the slice from self . Returns An integer if key is an integer, or a TensorShape if key is a
slice. Raises ValueError If key is a slice and self is completely unknown and
the step is set. __iter__ View source __iter__ () Returns self.dims if the rank is known, otherwise raises ValueError. __len__ View source __len__ () Returns the rank of this shape, or raises ValueError if unspecified. __nonzero__ View source __nonzero__ () Returns True if this shape contains non-zero information. __radd__ View source __radd__ ( other )


Page: https://www.tensorflow.org/api_docs/python/tf/TensorSpec
View source on GitHub Describes the type of a tf.Tensor. Inherits From: TypeSpec , TraceType View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.TensorSpec tf . TensorSpec ( shape , dtype = tf . dtypes . float32 , name = None ) Used in the notebooks Used in the guide Used in the tutorials Extension types Import a JAX model using JAX2TF Using the SavedModel format Better performance with tf.function Migrate the SavedModel workflow DeepDream Simple audio recognition: Recognizing keywords Load video data Transfer learning for video classification with MoViNet Video classification with a 3D convolutional neural network t = tf . constant ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) tf . TensorSpec . from_tensor ( t ) TensorSpec ( shape = ( 2 , 3 ), dtype = tf . int32 , name = None ) Contains metadata for describing the nature of tf.Tensor objects
accepted or returned by some TensorFlow APIs. For example, it can be used to constrain the type of inputs accepted by
a tf.function: @tf . function ( input_signature = [ tf . TensorSpec ([ 1 , None ])]) def constrained_foo ( t ): print ( "tracing..." ) return t Now the tf.function is able to assume that t is always of the type tf.TensorSpec([1, None]) which will avoid retracing as well as enforce the
type restriction on inputs. As a result, the following call with tensor of type tf.TensorSpec([1, 2]) triggers a trace and succeeds: >>> constrained_foo ( tf . constant ([[ 1. , 2 ]])) . numpy () tracing ... array ([[ 1. , 2. ]], dtype = float32 ) The following subsequent call with tensor of type tf.TensorSpec([1, 4]) does not trigger a trace and succeeds: >>> constrained_foo ( tf . constant ([[ 1. , 2 , 3 , 4 ]])) . numpy () array ([[ 1. , 2. , 3. , 4. ], dtype = float32 ) But the following call with tensor of type tf.TensorSpec([2, 2]) fails: >>> constrained_foo ( tf . constant ([[ 1. , 2 ], [ 3 , 4 ]])) . numpy () Traceback ( most recent call last ): ... TypeError : Binding inputs to tf . function ` constrained_foo ` failed ... Args shape Value convertible to tf.TensorShape . The shape of the tensor. dtype Value convertible to tf.DType . The type of the tensor values. name Optional name for the Tensor. Raises TypeError If shape is not convertible to a tf.TensorShape , or dtype is
not convertible to a tf.DType . Attributes dtype Returns the dtype of elements in the tensor. name Returns the (optionally provided) name of the described tensor. shape Returns the TensorShape that represents the shape of the tensor. value_type The Python type for values that are compatible with this TypeSpec. Methods experimental_as_proto View source experimental_as_proto () -> struct_pb2 . TensorSpecProto Returns a proto representation of the TensorSpec instance. experimental_from_proto View source @classmethod experimental_from_proto ( proto : struct_pb2 . TensorSpecProto ) -> 'TensorSpec' Returns a TensorSpec instance based on the serialized proto. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ struct_pb2 . TensorSpecProto ] Returns the type of proto associated with TensorSpec serialization. from_spec View source @classmethod from_spec ( spec , name = None ) Returns a TensorSpec with the same shape and dtype as spec . spec = tf . TensorSpec ( shape = [ 8 , 3 ], dtype = tf . int32 , name = "OriginalName" ) tf . TensorSpec . from_spec ( spec , "NewName" ) TensorSpec ( shape = ( 8 , 3 ), dtype = tf . int32 , name = 'NewName' ) Args spec The TypeSpec used to create the new TensorSpec . name The name for the new TensorSpec .  Defaults to spec.name . from_tensor View source @classmethod from_tensor ( tensor , name = None ) Returns a TensorSpec that describes tensor . tf . TensorSpec . from_tensor ( tf . constant ([ 1 , 2 , 3 ])) TensorSpec ( shape = ( 3 ,), dtype = tf . int32 , name = None ) Args tensor The tf.Tensor that should be described. name A name for the TensorSpec .  Defaults to tensor.op.name . Returns A TensorSpec that describes tensor . is_compatible_with View source is_compatible_with ( spec_or_tensor ) Returns True if spec_or_tensor is compatible with this TensorSpec. Two tensors are considered compatible if they have the same dtype
and their shapes are compatible (see tf.TensorShape.is_compatible_with ). Args spec_or_tensor A tf.TensorSpec or a tf.Tensor Returns True if spec_or_tensor is compatible with self. is_subtype_of View source is_subtype_of ( other ) Returns True if self is a subtype of other . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args other A TraceType object. most_specific_common_supertype View source most_specific_common_supertype ( others : Sequence [ tf . types . experimental . TraceType ] ) -> Optional [ 'TypeSpec' ] Returns the most specific supertype TypeSpec  of self and others . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args others A sequence of TraceTypes. most_specific_compatible_type View source most_specific_compatible_type ( other : 'TypeSpec' ) -> 'TypeSpec' Returns the most specific TypeSpec compatible with self and other . (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use most_specific_common_supertype instead. Deprecated. Please use most_specific_common_supertype instead.
Do not override this function. Args other A TypeSpec . Raises ValueError If there is no TypeSpec that is compatible with both self and other . __eq__ View source __eq__ ( other ) Return self==value. __ne__ View source __ne__ ( other ) Return self!=value.


Page: https://www.tensorflow.org/api_docs/python/tf/TypeSpec
View source on GitHub Specifies a TensorFlow value type. Inherits From: TraceType View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.TypeSpec , tf.compat.v1.data.experimental.Structure A tf.TypeSpec provides metadata describing an object accepted or returned
by TensorFlow APIs.  Concrete subclasses, such as tf.TensorSpec and tf.RaggedTensorSpec , are used to describe different value types. For example, tf.function 's input_signature argument accepts a list
(or nested structure) of TypeSpec s. Creating new subclasses of TypeSpec (outside of TensorFlow core) is not
currently supported.  In particular, we may make breaking changes to the
private methods and properties defined by this base class. Example: spec = tf . TensorSpec ( shape = [ None , None ], dtype = tf . int32 ) @tf . function ( input_signature = [ spec ]) def double ( x ): return x * 2 double ( tf . constant ([[ 1 , 2 ], [ 3 , 4 ]])) < tf . Tensor : shape = ( 2 , 2 ), dtype = int32 , numpy = array ([[ 2 , 4 ], [ 6 , 8 ]], dtype = int32 ) > Attributes value_type The Python type for values that are compatible with this TypeSpec. In particular, all values that are compatible with this TypeSpec must be an
instance of this type. Methods experimental_as_proto View source experimental_as_proto () -> struct_pb2 . TypeSpecProto Returns a proto representation of the TypeSpec instance. Do NOT override for custom non-TF types. experimental_from_proto View source @classmethod experimental_from_proto ( proto : struct_pb2 . TypeSpecProto ) -> 'TypeSpec' Returns a TypeSpec instance based on the serialized proto. Do NOT override for custom non-TF types. Args proto Proto generated using 'experimental_as_proto'. experimental_type_proto View source @classmethod experimental_type_proto () -> Type [ struct_pb2 . TypeSpecProto ] Returns the type of proto associated with TypeSpec serialization. Do NOT override for custom non-TF types. is_compatible_with View source is_compatible_with ( spec_or_value ) Returns true if spec_or_value is compatible with this TypeSpec. Prefer using "is_subtype_of" and "most_specific_common_supertype" wherever
possible. Args spec_or_value A TypeSpec or TypeSpec associated value to compare against. is_subtype_of View source is_subtype_of ( other : tf . types . experimental . TraceType ) -> bool Returns True if self is a subtype of other . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args other A TraceType object. most_specific_common_supertype View source most_specific_common_supertype ( others : Sequence [ tf . types . experimental . TraceType ] ) -> Optional [ 'TypeSpec' ] Returns the most specific supertype TypeSpec  of self and others . Implements the tf.types.experimental.func.TraceType interface. If not overridden by a subclass, the default behavior is to assume the
TypeSpec is covariant upon attributes that implement TraceType and
invariant upon rest of the attributes as well as the structure and type
of the TypeSpec. Args others A sequence of TraceTypes. most_specific_compatible_type View source most_specific_compatible_type ( other : 'TypeSpec' ) -> 'TypeSpec' Returns the most specific TypeSpec compatible with self and other . (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use most_specific_common_supertype instead. Deprecated. Please use most_specific_common_supertype instead.
Do not override this function. Args other A TypeSpec . Raises ValueError If there is no TypeSpec that is compatible with both self and other . __eq__ View source __eq__ ( other ) -> bool Return self==value. __ne__ View source __ne__ ( other ) -> bool Return self!=value.


Page: https://www.tensorflow.org/api_docs/python/tf/UnconnectedGradients
View source on GitHub Controls how gradient computation behaves when y does not depend on x. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.UnconnectedGradients The gradient of y with respect to x can be zero in two different ways: there
could be no differentiable path in the graph connecting x to y (and so we can
statically prove that the gradient is zero) or it could be that runtime values
of tensors in a particular execution lead to a gradient of zero (say, if a
relu unit happens to not be activated). To allow you to distinguish between
these two cases you can choose what value gets returned for the gradient when
there is no path in the graph from x to y: NONE : Indicates that [None] will be returned if there is no path from x
to y ZERO : Indicates that a zero tensor will be returned in the shape of x. Class Variables NONE <UnconnectedGradients.NONE: 'none'> ZERO <UnconnectedGradients.ZERO: 'zero'>


Page: https://www.tensorflow.org/api_docs/python/tf/Variable
View source on GitHub See the variable guide . tf . Variable ( initial_value = None , trainable = None , validate_shape = True , caching_device = None , name = None , variable_def = None , dtype = None , import_scope = None , constraint = None , synchronization = tf . VariableSynchronization . AUTO , aggregation = tf . compat . v1 . VariableAggregation . NONE , shape = None , experimental_enable_variable_lifting = True ) Used in the notebooks Used in the guide Used in the tutorials Migrating model checkpoints Introduction to gradients and automatic differentiation Introduction to Variables Advanced automatic differentiation Better performance with tf.function Scalable model compression Custom training loop with Keras and MultiWorkerMirroredStrategy Neural style transfer Learned data compression Learnable Distributions Zoo A variable maintains shared, persistent state manipulated by a program. The Variable() constructor requires an initial value for the variable, which
can be a Tensor of any type and shape. This initial value defines the type
and shape of the variable. After construction, the type and shape of the
variable are fixed. The value can be changed using one of the assign methods. v = tf . Variable ( 1. ) v . assign ( 2. ) < tf . Variable ... shape = () dtype = float32 , numpy = 2.0 > v . assign_add ( 0.5 ) < tf . Variable ... shape = () dtype = float32 , numpy = 2.5 > The shape argument to Variable 's constructor allows you to construct a
variable with a less defined shape than its initial_value : v = tf . Variable ( 1. , shape = tf . TensorShape ( None )) v . assign ([[ 1. ]]) < tf . Variable ... shape = < unknown > dtype = float32 , numpy = array ([[ 1. ]], ... ) > Just like any Tensor , variables created with Variable() can be used as
inputs to operations. Additionally, all the operators overloaded for the Tensor class are carried over to variables. w = tf . Variable ([[ 1. ], [ 2. ]]) x = tf . constant ([[ 3. , 4. ]]) tf . matmul ( w , x ) < tf . Tensor : ... shape = ( 2 , 2 ), ... numpy = array ([[ 3. , 4. ], [ 6. , 8. ]], dtype = float32 ) > tf . sigmoid ( w + x ) < tf . Tensor : ... shape = ( 2 , 2 ), ... > When building a machine learning model it is often convenient to distinguish
between variables holding trainable model parameters and other variables such
as a step variable used to count training steps. To make this easier, the
variable constructor supports a trainable=<bool> parameter. tf.GradientTape watches trainable variables by default: with tf . GradientTape ( persistent = True ) as tape : trainable = tf . Variable ( 1. ) non_trainable = tf . Variable ( 2. , trainable = False ) x1 = trainable * 2. x2 = non_trainable * 3. tape . gradient ( x1 , trainable ) < tf . Tensor : ... shape = (), dtype = float32 , numpy = 2.0 > assert tape . gradient ( x2 , non_trainable ) is None # Unwatched Variables are automatically tracked when assigned to attributes of types
inheriting from tf.Module . m = tf . Module () m . v = tf . Variable ([ 1. ]) m . trainable_variables ( < tf . Variable ... shape = ( 1 ,) ... numpy = array ([ 1. ], dtype = float32 )>,) This tracking then allows saving variable values to training checkpoints , or to SavedModels which include
serialized TensorFlow graphs. Variables are often captured and manipulated by tf.function s. This works the
same way the un-decorated function would have: v = tf . Variable ( 0. ) read_and_decrement = tf . function ( lambda : v . assign_sub ( 0.1 )) read_and_decrement () < tf . Tensor : shape = (), dtype = float32 , numpy =- 0.1 > read_and_decrement () < tf . Tensor : shape = (), dtype = float32 , numpy =- 0.2 > Variables created inside a tf.function must be owned outside the function
and be created only once: class M ( tf . Module ): @tf . function def __call__ ( self , x ): if not hasattr ( self , "v" ): # Or set self.v to None in __init__ self . v = tf . Variable ( x ) return self . v * x m = M () m ( 2. ) < tf . Tensor : shape = (), dtype = float32 , numpy = 4.0 > m ( 3. ) < tf . Tensor : shape = (), dtype = float32 , numpy = 6.0 > m . v < tf . Variable ... shape = () dtype = float32 , numpy = 2.0 > See the tf.function documentation for details. Args initial_value A Tensor , or Python object convertible to a Tensor ,
which is the initial value for the Variable. The initial value must have
a shape specified unless validate_shape is set to False. Can also be a
callable with no argument that returns the initial value when called. In
that case, dtype must be specified. (Note that initializer functions
from init_ops.py must first be bound to a shape before being used here.) trainable If True , GradientTapes automatically watch uses of this
variable. Defaults to True , unless synchronization is set to ON_READ , in which case it defaults to False . validate_shape If False , allows the variable to be initialized with a
value of unknown shape. If True , the default, the shape of initial_value must be known. caching_device Note: This argument is only valid when using a v1-style Session . Optional device string describing where the Variable should
be cached for reading. Defaults to the Variable's device. If not None ,
caches on another device. Typical use is to cache on the device where
the Ops using the Variable reside, to deduplicate copying through Switch and other conditional statements. name Optional name for the variable. Defaults to 'Variable' and gets
uniquified automatically. variable_def VariableDef protocol buffer. If not None , recreates the
Variable object with its contents, referencing the variable's nodes in
the graph, which must already exist. The graph is not changed. variable_def and the other arguments are mutually exclusive. dtype If set, initial_value will be converted to the given type. If None , either the datatype will be kept (if initial_value is a
Tensor), or convert_to_tensor will decide. import_scope Optional string . Name scope to add to the Variable. Only
used when initializing from protocol buffer. constraint An optional projection function to be applied to the variable
after being updated by an Optimizer (e.g. used to implement norm
constraints or value constraints for layer weights). The function must
take as input the unprojected Tensor representing the value of the
variable and return the Tensor for the projected value (which must have
the same shape). Constraints are not safe to use when doing asynchronous
distributed training. synchronization Indicates when a distributed variable will be
aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to
synchronize. aggregation Indicates how a distributed variable will be aggregated.
Accepted values are constants defined in the class tf.VariableAggregation . shape (optional) The shape of this variable. If None, the shape of initial_value will be used. When setting this argument to tf.TensorShape(None) (representing an unspecified shape), the variable
can be assigned with values of different shapes. experimental_enable_variable_lifting Whether to lift the variable out if
it's in a tf.function . Default is True . When this argument
is True , variable creation will follow the behavior and
restrictions described here .
If this argument is False , that description doesn't apply,
and you can freely create and use the variable in the tf.function , as if it's a "mutable tf.Tensor ". You can't
return the variable though. Raises ValueError If both variable_def and initial_value are specified. ValueError If the initial value is not specified, or does not have a
shape and validate_shape is True . Attributes aggregation constraint Returns the constraint function associated with this variable. device The device of this variable. dtype The DType of this variable. graph The Graph of this variable. initial_value Returns the Tensor used as the initial value for the variable. Note that this is different from initialized_value() which runs
the op that initializes the variable before returning its value.
This method returns the tensor that is used by the op that initializes
the variable. initializer The initializer operation for this variable. name The name of this variable. op The Operation of this variable. shape The TensorShape of this variable. synchronization trainable Child Classes class SaveSliceInfo Methods assign View source assign ( value , use_locking = False , name = None , read_value = True ) Assigns a new value to the variable. This is essentially a shortcut for assign(self, value) . Args value A Tensor . The new value for this variable. use_locking If True , use locking during the assignment. name The name of the operation to be created read_value if True, will return something which evaluates to the new
value of the variable; if False will return the assign op. Returns The updated variable. If read_value is false, instead returns None in
Eager mode and the assign op in graph mode. assign_add View source assign_add ( delta , use_locking = False , name = None , read_value = True ) Adds a value to this variable. This is essentially a shortcut for assign_add(self, delta) . Args delta A Tensor . The value to add to this variable. use_locking If True , use locking during the operation. name The name of the operation to be created read_value if True, will return something which evaluates to the new
value of the variable; if False will return the assign op. Returns The updated variable. If read_value is false, instead returns None in
Eager mode and the assign op in graph mode. assign_sub View source assign_sub ( delta , use_locking = False , name = None , read_value = True ) Subtracts a value from this variable. This is essentially a shortcut for assign_sub(self, delta) . Args delta A Tensor . The value to subtract from this variable. use_locking If True , use locking during the operation. name The name of the operation to be created read_value if True, will return something which evaluates to the new
value of the variable; if False will return the assign op. Returns The updated variable. If read_value is false, instead returns None in
Eager mode and the assign op in graph mode. batch_scatter_update View source batch_scatter_update ( sparse_delta , use_locking = False , name = None ) Assigns tf.IndexedSlices to this variable batch-wise. Analogous to batch_gather . This assumes that this variable and the
sparse_delta IndexedSlices have a series of leading dimensions that are the
same for all of them, and the updates are performed on the last dimension of
indices. In other words, the dimensions should be the following: num_prefix_dims = sparse_delta.indices.ndims - 1 batch_dim = num_prefix_dims + 1 sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[
     batch_dim:] where sparse_delta.updates.shape[:num_prefix_dims] == sparse_delta.indices.shape[:num_prefix_dims] == var.shape[:num_prefix_dims] And the operation performed can be expressed as: var[i_1, ..., i_n,
     sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[
        i_1, ..., i_n, j] When sparse_delta.indices is a 1D tensor, this operation is equivalent to scatter_update . To avoid this operation one can looping over the first ndims of the
variable and using scatter_update on the subtensors that result of slicing
the first dimension. This is a valid option for ndims = 1 , but less
efficient than this implementation. Args sparse_delta tf.IndexedSlices to be assigned to this variable. use_locking If True , use locking during the operation. name the name of the operation. Returns The updated variable. Raises TypeError if sparse_delta is not an IndexedSlices . count_up_to View source count_up_to ( limit ) Increments this variable until it reaches limit . (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Prefer Dataset.range instead. When that Op is run it tries to increment the variable by 1 . If
incrementing the variable would bring it above limit then the Op raises
the exception OutOfRangeError . If no error is raised, the Op outputs the value of the variable before
the increment. This is essentially a shortcut for count_up_to(self, limit) . Args limit value at which incrementing the variable raises an error. Returns A Tensor that will hold the variable value before the increment. If no
other Op modifies this variable, the values produced will all be
distinct. eval View source eval ( session = None ) In a session, computes and returns the value of this variable. This is not a graph construction method, it does not add ops to the graph. This convenience method requires a session where the graph
containing this variable has been launched. If no session is
passed, the default session is used.  See tf.compat.v1.Session for more
information on launching a graph and on sessions. v = tf . Variable ([ 1 , 2 ]) init = tf . compat . v1 . global_variables_initializer () with tf . compat . v1 . Session () as sess : sess . run ( init ) # Usage passing the session explicitly. print ( v . eval ( sess )) # Usage with the default session.  The 'with' block # above makes 'sess' the default session. print ( v . eval ()) Args session The session to use to evaluate this variable. If none, the
default session is used. Returns A numpy ndarray with a copy of the value of this variable. experimental_ref View source experimental_ref () DEPRECATED FUNCTION Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use ref() instead. from_proto View source @staticmethod from_proto ( variable_def , import_scope = None ) Returns a Variable object created from variable_def . gather_nd View source gather_nd ( indices , name = None ) Gather slices from params into a Tensor with shape specified by indices . See tf.gather_nd for details. Args indices A Tensor . Must be one of the following types: int32 , int64 .
Index tensor. name A name for the operation (optional). Returns A Tensor . Has the same type as params . get_shape View source get_shape () -> tf . TensorShape Alias of Variable.shape . initialized_value View source initialized_value () Returns the value of the initialized variable. (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts. You should use this instead of the variable itself to initialize another
variable with a value that depends on the value of this variable. # Initialize 'v' with a random tensor. v = tf . Variable ( tf . random . truncated_normal ([ 10 , 40 ])) # Use `initialized_value` to guarantee that `v` has been # initialized before its value is used to initialize `w`. # The random values are picked only once. w = tf . Variable ( v . initialized_value () * 2.0 ) Returns A Tensor holding the value of this variable after its initializer
has run. load View source load ( value , session = None ) Load new value into this variable. (deprecated) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Prefer Variable.assign which has equivalent behavior in 2.X. Writes new value to variable's memory. Doesn't add ops to the graph. This convenience method requires a session where the graph
containing this variable has been launched. If no session is
passed, the default session is used.  See tf.compat.v1.Session for more
information on launching a graph and on sessions. v = tf . Variable ([ 1 , 2 ]) init = tf . compat . v1 . global_variables_initializer () with tf . compat . v1 . Session () as sess : sess . run ( init ) # Usage passing the session explicitly. v . load ([ 2 , 3 ], sess ) print ( v . eval ( sess )) # prints [2 3] # Usage with the default session.  The 'with' block # above makes 'sess' the default session. v . load ([ 3 , 4 ], sess ) print ( v . eval ()) # prints [3 4] Args value New variable value session The session to use to evaluate this variable. If none, the
default session is used. Raises ValueError Session is not passed and no default session read_value View source read_value () Returns the value of this variable, read in the current context. Can be different from value() if it's on another device, with control
dependencies, etc. Returns A Tensor containing the value of the variable. ref View source ref () Returns a hashable reference object to this Variable. The primary use case for this API is to put variables in a set/dictionary.
We can't put variables in a set/dictionary as variable.__hash__() is no
longer available starting Tensorflow 2.0. The following will raise an exception starting 2.0 x = tf . Variable ( 5 ) y = tf . Variable ( 10 ) z = tf . Variable ( 10 ) variable_set = { x , y , z } Traceback ( most recent call last ): TypeError : Variable is unhashable . Instead , use tensor . ref () as the key . variable_dict = { x : 'five' , y : 'ten' } Traceback ( most recent call last ): TypeError : Variable is unhashable . Instead , use tensor . ref () as the key . Instead, we can use variable.ref() . variable_set = { x . ref (), y . ref (), z . ref ()} x . ref () in variable_set True variable_dict = { x . ref (): 'five' , y . ref (): 'ten' , z . ref (): 'ten' } variable_dict [ y . ref ()] 'ten' Also, the reference object provides .deref() function that returns the
original Variable. x = tf . Variable ( 5 ) x . ref () . deref () < tf . Variable 'Variable:0' shape = () dtype = int32 , numpy = 5 > scatter_add View source scatter_add ( sparse_delta , use_locking = False , name = None ) Adds tf.IndexedSlices to this variable. Args sparse_delta tf.IndexedSlices to be added to this variable. use_locking If True , use locking during the operation. name the name of the operation. Returns The updated variable. Raises TypeError if sparse_delta is not an IndexedSlices . scatter_div View source scatter_div ( sparse_delta , use_locking = False , name = None ) Divide this variable by tf.IndexedSlices . Args sparse_delta tf.IndexedSlices to divide this variable by. use_locking If True , use locking during the operation. name the name of the operation. Returns The updated variable. Raises TypeError if sparse_delta is not an IndexedSlices . scatter_max View source scatter_max ( sparse_delta , use_locking = False , name = None ) Updates this variable with the max of tf.IndexedSlices and itself. Args sparse_delta tf.IndexedSlices to use as an argument of max with this
variable. use_locking If True , use locking during the operation. name the name of the operation. Returns The updated variable. Raises TypeError if sparse_delta is not an IndexedSlices . scatter_min View source scatter_min ( sparse_delta , use_locking = False , name = None ) Updates this variable with the min of tf.IndexedSlices and itself. Args sparse_delta tf.IndexedSlices to use as an argument of min with this
variable. use_locking If True , use locking during the operation. name the name of the operation. Returns The updated variable. Raises TypeError if sparse_delta is not an IndexedSlices . scatter_mul View source scatter_mul ( sparse_delta , use_locking = False , name = None ) Multiply this variable by tf.IndexedSlices . Args sparse_delta tf.IndexedSlices to multiply this variable by. use_locking If True , use locking during the operation. name the name of the operation. Returns The updated variable. Raises TypeError if sparse_delta is not an IndexedSlices . scatter_nd_add View source scatter_nd_add ( indices , updates , name = None ) Applies sparse addition to individual values or slices in a Variable. The Variable has rank P and indices is a Tensor of rank Q . indices must be integer tensor, containing indices into self.
It must be shape [d_0, ..., d_{Q-2}, K] where 0 < K <= P . The innermost dimension of indices (with length K ) corresponds to
indices into elements (if K = P ) or slices (if K < P ) along the K th
dimension of self. updates is Tensor of rank Q-1+P-K with shape: [ d_0 , ... , d_ { Q - 2 }, self . shape [ K ], ... , self . shape [ P - 1 ]] . For example, say we want to add 4 scattered elements to a rank-1 tensor to
8 elements. In Python, that update would look like this: v = tf . Variable ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]) indices = tf . constant ([[ 4 ], [ 3 ], [ 1 ] ,[ 7 ]]) updates = tf . constant ([ 9 , 10 , 11 , 12 ]) v . scatter_nd_add ( indices , updates ) print ( v ) The resulting update to v would look like this: [ 1 , 13 , 3 , 14 , 14 , 6 , 7 , 20 ] See tf.scatter_nd for more details about how to make updates to
slices. Args indices The indices to be used in the operation. updates The values to be used in the operation. name the name of the operation. Returns The updated variable. scatter_nd_sub View source scatter_nd_sub ( indices , updates , name = None ) Applies sparse subtraction to individual values or slices in a Variable. Assuming the variable has rank P and indices is a Tensor of rank Q . indices must be integer tensor, containing indices into self.
It must be shape [d_0, ..., d_{Q-2}, K] where 0 < K <= P . The innermost dimension of indices (with length K ) corresponds to
indices into elements (if K = P ) or slices (if K < P ) along the K th
dimension of self. updates is Tensor of rank Q-1+P-K with shape: [ d_0 , ... , d_ { Q - 2 }, self . shape [ K ], ... , self . shape [ P - 1 ]] . For example, say we want to add 4 scattered elements to a rank-1 tensor to
8 elements. In Python, that update would look like this: v = tf . Variable ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]) indices = tf . constant ([[ 4 ], [ 3 ], [ 1 ] ,[ 7 ]]) updates = tf . constant ([ 9 , 10 , 11 , 12 ]) v . scatter_nd_sub ( indices , updates ) print ( v ) After the update v would look like this: [ 1 , - 9 , 3 , - 6 , - 4 , 6 , 7 , - 4 ] See tf.scatter_nd for more details about how to make updates to
slices. Args indices The indices to be used in the operation. updates The values to be used in the operation. name the name of the operation. Returns The updated variable. scatter_nd_update View source scatter_nd_update ( indices , updates , name = None ) Applies sparse assignment to individual values or slices in a Variable. The Variable has rank P and indices is a Tensor of rank Q . indices must be integer tensor, containing indices into self.
It must be shape [d_0, ..., d_{Q-2}, K] where 0 < K <= P . The innermost dimension of indices (with length K ) corresponds to
indices into elements (if K = P ) or slices (if K < P ) along the K th
dimension of self. updates is Tensor of rank Q-1+P-K with shape: [ d_0 , ... , d_ { Q - 2 }, self . shape [ K ], ... , self . shape [ P - 1 ]] . For example, say we want to add 4 scattered elements to a rank-1 tensor to
8 elements. In Python, that update would look like this: v = tf . Variable ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]) indices = tf . constant ([[ 4 ], [ 3 ], [ 1 ] ,[ 7 ]]) updates = tf . constant ([ 9 , 10 , 11 , 12 ]) v . scatter_nd_update ( indices , updates ) print ( v ) The resulting update to v would look like this: [ 1 , 11 , 3 , 10 , 9 , 6 , 7 , 12 ] See tf.scatter_nd for more details about how to make updates to
slices. Args indices The indices to be used in the operation. updates The values to be used in the operation. name the name of the operation. Returns The updated variable. scatter_sub View source scatter_sub ( sparse_delta , use_locking = False , name = None ) Subtracts tf.IndexedSlices from this variable. Args sparse_delta tf.IndexedSlices to be subtracted from this variable. use_locking If True , use locking during the operation. name the name of the operation. Returns The updated variable. Raises TypeError if sparse_delta is not an IndexedSlices . scatter_update View source scatter_update ( sparse_delta , use_locking = False , name = None ) Assigns tf.IndexedSlices to this variable. Args sparse_delta tf.IndexedSlices to be assigned to this variable. use_locking If True , use locking during the operation. name the name of the operation. Returns The updated variable. Raises TypeError if sparse_delta is not an IndexedSlices . set_shape View source set_shape ( shape ) Overrides the shape for this variable. Args shape the TensorShape representing the overridden shape. sparse_read View source sparse_read ( indices , name = None ) Gather slices from params axis axis according to indices. This function supports a subset of tf.gather, see tf.gather for details on
usage. Args indices The index Tensor .  Must be one of the following types: int32 , int64 . Must be in range [0, params.shape[axis]) . name A name for the operation (optional). Returns A Tensor . Has the same type as params . to_proto View source to_proto ( export_scope = None ) Converts a Variable to a VariableDef protocol buffer. Args export_scope Optional string . Name scope to remove. Returns A VariableDef protocol buffer, or None if the Variable is not
in the specified name scope. value View source value () Returns the last snapshot of this variable. You usually do not need to call this method as all ops that need the value
of the variable call it automatically through a convert_to_tensor() call. Returns a Tensor which holds the value of the variable.  You can not
assign a new value to this tensor as it is not a reference to the variable. To avoid copies, if the consumer of the returned value is on the same device
as the variable, this actually returns the live value of the variable, not
a copy.  Updates to the variable are seen by the consumer.  If the consumer
is on a different device it will get a copy of the variable. Returns A Tensor containing the value of the variable. __abs__ View source __abs__ ( name = None ) __add__ View source __add__ ( y ) __and__ View source __and__ ( y ) __div__ View source __div__ ( y ) __eq__ View source __eq__ ( other ) Compares two variables element-wise for equality. __floordiv__ View source __floordiv__ ( y ) __ge__ __ge__ ( y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Returns the truth value of (x >= y) element-wise. Note: math.greater_equal supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 , 7 ]) y = tf . constant ([ 5 , 2 , 5 , 10 ]) tf . math . greater_equal ( x , y ) == > [ True , True , True , False ] x = tf . constant ([ 5 , 4 , 6 , 7 ]) y = tf . constant ([ 5 ]) tf . math . greater_equal ( x , y ) == > [ True , False , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool . __getitem__ View source __getitem__ ( slice_spec ) Creates a slice helper object given a variable. This allows creating a sub-tensor from part of the current contents
of a variable. See tf.Tensor. getitem for detailed examples
of slicing. This function in addition also allows assignment to a sliced range.
This is similar to __setitem__ functionality in Python. However,
the syntax is different so that the user can capture the assignment
operation for grouping or passing to sess.run() in TF1.
For example, import tensorflow as tf A = tf . Variable ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]], dtype = tf . float32 ) print ( A [: 2 , : 2 ]) # => [[1,2], [4,5]] A [: 2 ,: 2 ] . assign ( 22. * tf . ones (( 2 , 2 )))) print ( A ) # => [[22, 22, 3], [22, 22, 6], [7,8,9]] Note that assignments currently do not support NumPy broadcasting
semantics. Args var An ops.Variable object. slice_spec The arguments to Tensor. getitem . Returns The appropriate slice of "tensor", based on "slice_spec".
As an operator. The operator also has a assign() method
that can be used to generate an assignment operator. Raises ValueError If a slice range is negative size. TypeError TypeError: If the slice indices aren't int, slice,
ellipsis, tf.newaxis or int32/int64 tensors. __gt__ __gt__ ( y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Returns the truth value of (x > y) element-wise. Note: math.greater supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 2 , 5 ]) tf . math . greater ( x , y ) == > [ False , True , True ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . greater ( x , y ) == > [ False , False , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool . __invert__ View source __invert__ ( name = None ) __iter__ View source __iter__ () When executing eagerly, iterates over the value of the variable. __le__ __le__ ( y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Returns the truth value of (x <= y) element-wise. Note: math.less_equal supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . less_equal ( x , y ) == > [ True , True , False ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 6 , 6 ]) tf . math . less_equal ( x , y ) == > [ True , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool . __lt__ __lt__ ( y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Returns the truth value of (x < y) element-wise. Note: math.less supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . less ( x , y ) == > [ False , True , False ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 6 , 7 ]) tf . math . less ( x , y ) == > [ False , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool . __matmul__ View source __matmul__ ( y ) __mod__ View source __mod__ ( y ) __mul__ View source __mul__ ( y ) __ne__ View source __ne__ ( other ) Compares two variables element-wise for equality. __neg__ __neg__ ( name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Computes numerical negative value element-wise. I.e., \(y = -x\). Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , int8 , int16 , int32 , int64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x . __or__ View source __or__ ( y ) __pow__ View source __pow__ ( y ) __radd__ View source __radd__ ( x ) __rand__ View source __rand__ ( x ) __rdiv__ View source __rdiv__ ( x ) __rfloordiv__ View source __rfloordiv__ ( x ) __rmatmul__ View source __rmatmul__ ( x ) __rmod__ View source __rmod__ ( x ) __rmul__ View source __rmul__ ( x ) __ror__ View source __ror__ ( x ) __rpow__ View source __rpow__ ( x ) __rsub__ View source __rsub__ ( x ) __rtruediv__ View source __rtruediv__ ( x ) __rxor__ View source __rxor__ ( x ) __sub__ View source __sub__ ( y ) __truediv__ View source __truediv__ ( y ) __xor__ View source __xor__ ( y )


Page: https://tensorflow.org/guide/variable
View on TensorFlow.org Run in Google Colab View source on GitHub Download notebook A TensorFlow variable is the recommended way to represent shared, persistent state your program manipulates. This guide covers how to create, update, and manage instances of tf.Variable in TensorFlow. Variables are created and tracked via the tf.Variable class. A tf.Variable represents a tensor whose value can be changed by running ops on it.  Specific ops allow you to read and modify the values of this tensor. Higher level libraries like tf.keras use tf.Variable to store model parameters. Setup This notebook discusses variable placement.  If you want to see on what device your variables are placed, uncomment this line. import tensorflow as tf # Uncomment to see where your variables get placed (see below) # tf.debugging.set_log_device_placement(True) 2024-08-15 03:11:13.352524: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-15 03:11:13.373928: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-15 03:11:13.380240: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered Create a variable To create a variable, provide an initial value.  The tf.Variable will have the same dtype as the initialization value. my_tensor = tf . constant ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]]) my_variable = tf . Variable ( my_tensor ) # Variables can be all kinds of types, just like tensors bool_variable = tf . Variable ([ False , False , False , True ]) complex_variable = tf . Variable ([ 5 + 4 j , 6 + 1 j ]) WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723691475.985751  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691475.989650  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691475.992820  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691475.996591  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691476.008057  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691476.011646  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691476.014444  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691476.017845  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691476.021212  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691476.024742  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691476.027614  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691476.031160  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.244355  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.246449  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.248442  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.250525  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.252548  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.254492  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.257197  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.259182  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.261129  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.263068  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.264968  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.266962  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.304803  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.307421  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.309352  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.311366  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.313328  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.315262  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.317178  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.319165  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.321104  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.323540  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.325885  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1723691477.328388  199297 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355 A variable looks and acts like a tensor, and, in fact, is a data structure backed by a tf.Tensor .  Like tensors, they have a dtype and a shape, and can be exported to NumPy. print ( "Shape: " , my_variable . shape ) print ( "DType: " , my_variable . dtype ) print ( "As NumPy: " , my_variable . numpy ()) Shape:  (2, 2)
DType:  <dtype: 'float32'>
As NumPy:  [[1. 2.]
 [3. 4.]] Most tensor operations work on variables as expected, although variables cannot be reshaped. print ( "A variable:" , my_variable ) print ( " \n Viewed as a tensor:" , tf . convert_to_tensor ( my_variable )) print ( " \n Index of highest value:" , tf . math . argmax ( my_variable )) # This creates a new tensor; it does not reshape the variable. print ( " \n Copying and reshaping: " , tf . reshape ( my_variable , [ 1 , 4 ])) A variable: <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
array([[1., 2.],
       [3., 4.]], dtype=float32)>

Viewed as a tensor: tf.Tensor(
[[1. 2.]
 [3. 4.]], shape=(2, 2), dtype=float32)

Index of highest value: tf.Tensor([1 1], shape=(2,), dtype=int64)

Copying and reshaping:  tf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32) As noted above, variables are backed by tensors. You can reassign the tensor using tf.Variable.assign .  Calling assign does not (usually) allocate a new tensor; instead, the existing tensor's memory is reused. a = tf . Variable ([ 2.0 , 3.0 ]) # This will keep the same dtype, float32 a . assign ([ 1 , 2 ]) # Not allowed as it resizes the variable: try : a . assign ([ 1.0 , 2.0 , 3.0 ]) except Exception as e : print ( f " { type ( e ) . __name__ } : { e } " ) ValueError: Cannot assign value to variable ' Variable:0': Shape mismatch.The variable shape (2,), and the assigned value shape (3,) are incompatible. If you use a variable like a tensor in operations, you will usually operate on the backing tensor. Creating new variables from existing variables duplicates the backing tensors. Two variables will not share the same memory. a = tf . Variable ([ 2.0 , 3.0 ]) # Create b based on the value of a b = tf . Variable ( a ) a . assign ([ 5 , 6 ]) # a and b are different print ( a . numpy ()) print ( b . numpy ()) # There are other versions of assign print ( a . assign_add ([ 2 , 3 ]) . numpy ()) # [7. 9.] print ( a . assign_sub ([ 7 , 9 ]) . numpy ()) # [0. 0.] [5. 6.]
[2. 3.]
[7. 9.]
[0. 0.] Lifecycles, naming, and watching In Python-based TensorFlow, tf.Variable instance have the same lifecycle as other Python objects. When there are no references to a variable it is automatically deallocated. Variables can also be named which can help you track and debug them.  You can give two variables the same name. # Create a and b; they will have the same name but will be backed by # different tensors. a = tf . Variable ( my_tensor , name = "Mark" ) # A new variable with the same name, but different value # Note that the scalar add is broadcast b = tf . Variable ( my_tensor + 1 , name = "Mark" ) # These are elementwise-unequal, despite having the same name print ( a == b ) tf.Tensor(
[[False False]
 [False False]], shape=(2, 2), dtype=bool) Variable names are preserved when saving and loading models. By default, variables in models will acquire unique variable names automatically, so you don't need to assign them yourself unless you want to. Although variables are important for differentiation, some variables will not need to be differentiated.  You can turn off gradients for a variable by setting trainable to false at creation. An example of a variable that would not need gradients is a training step counter. step_counter = tf . Variable ( 1 , trainable = False ) Placing variables and tensors For better performance, TensorFlow will attempt to place tensors and variables on the fastest device compatible with its dtype .  This means most variables are placed on a GPU if one is available. However, you can override this.  In this snippet, place a float tensor and a variable on the CPU, even if a GPU is available.  By turning on device placement logging (see Setup ), you can see where the variable is placed. Note: Although manual placement works, using distribution strategies can be a more convenient and scalable way to optimize your computation. If you run this notebook on different backends with and without a GPU you will see different logging. Note that logging device placement must be turned on at the start of the session. with tf . device ( 'CPU:0' ): # Create some tensors a = tf . Variable ([[ 1.0 , 2.0 , 3.0 ], [ 4.0 , 5.0 , 6.0 ]]) b = tf . constant ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ], [ 5.0 , 6.0 ]]) c = tf . matmul ( a , b ) print ( c ) tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32) It's possible to set the location of a variable or tensor on one device and do the computation on another device.  This will introduce delay, as data needs to be copied between the devices. You might do this, however, if you had multiple GPU workers but only want one copy of the variables. with tf . device ( 'CPU:0' ): a = tf . Variable ([[ 1.0 , 2.0 , 3.0 ], [ 4.0 , 5.0 , 6.0 ]]) b = tf . Variable ([[ 1.0 , 2.0 , 3.0 ]]) with tf . device ( 'GPU:0' ): # Element-wise multiply k = a * b print ( k ) tf.Tensor(
[[ 1.  4.  9.]
 [ 4. 10. 18.]], shape=(2, 3), dtype=float32) Note: Because tf.config.set_soft_device_placement is turned on by default, even if you run this code on a device without a GPU, it will still run.  The multiplication step will happen on the CPU. For more on distributed training, refer to the guide . Next steps To understand how variables are typically used, see our guide on automatic differentiation .


Page: https://www.tensorflow.org/api_docs/python/tf/VariableAggregation
View source on GitHub Indicates how a distributed variable will be aggregated. tf.distribute.Strategy distributes a model by making multiple copies
(called "replicas") acting on different elements of the input batch in a
data parallel model. When performing some variable-update operation,
for example var.assign_add(x) , in a model, we need to resolve how to combine
the different values for x computed in the different replicas. NONE : This is the default, giving an error if you use a
variable-update operation with multiple replicas. SUM : Add the updates across replicas. MEAN : Take the arithmetic mean ("average") of the updates across replicas. ONLY_FIRST_REPLICA : This is for when every replica is performing the same
update, but we only want to perform the update once. Used, e.g., for the
global step counter. For example: strategy = tf . distribute . MirroredStrategy ([ "GPU:0" , "GPU:1" ]) with strategy . scope (): v = tf . Variable ( 5.0 , aggregation = tf . VariableAggregation . MEAN ) @tf . function def update_fn (): return v . assign_add ( 1.0 ) strategy . run ( update_fn ) PerReplica :{ 0 : < tf . Tensor : shape = (), dtype = float32 , numpy = 6.0 > , 1 : < tf . Tensor : shape = (), dtype = float32 , numpy = 6.0 > } Class Variables MEAN <VariableAggregationV2.MEAN: 2> NONE <VariableAggregationV2.NONE: 0> ONLY_FIRST_REPLICA <VariableAggregationV2.ONLY_FIRST_REPLICA: 3> SUM <VariableAggregationV2.SUM: 1>


Page: https://www.tensorflow.org/api_docs/python/tf/VariableSynchronization
View source on GitHub Indicates when a distributed variable will be synced. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.VariableSynchronization AUTO : Indicates that the synchronization will be determined by the current DistributionStrategy (eg. With MirroredStrategy this would be ON_WRITE ). NONE : Indicates that there will only be one copy of the variable, so
there is no need to sync. ON_WRITE : Indicates that the variable will be updated across devices
every time it is written. ON_READ : Indicates that the variable will be aggregated across devices
when it is read (eg. when checkpointing or when evaluating an op that uses
the variable). Example: >>> temp_grad = [ tf . Variable ([ 0. ], trainable = False , ... synchronization = tf . VariableSynchronization . ON_READ , ... aggregation = tf . VariableAggregation . MEAN ... )] Class Variables AUTO <VariableSynchronization.AUTO: 0> NONE <VariableSynchronization.NONE: 1> ON_READ <VariableSynchronization.ON_READ: 3> ON_WRITE <VariableSynchronization.ON_WRITE: 2>


Page: https://www.tensorflow.org/api_docs/python/tf/constant_initializer
View source on GitHub Initializer that generates tensors with constant values. tf . constant_initializer ( value = 0 , support_partition = False ) Used in the notebooks Used in the tutorials TFP Probabilistic Layers: Regression Initializers allow you to pre-specify an initialization strategy, encoded in
the Initializer object, without knowing the shape and dtype of the variable
being initialized. tf.constant_initializer returns an object which when called returns a tensor
populated with the value specified in the constructor. This value must be
convertible to the requested dtype . The argument value can be a scalar constant value, or a list of
values. Scalars broadcast to whichever shape is requested from the
initializer. If value is a list, then the length of the list must be equal to the number
of elements implied by the desired shape of the tensor. If the total number of
elements in value is not equal to the number of elements required by the
tensor shape, the initializer will raise a TypeError . Examples: def make_variables ( k , initializer ): return ( tf . Variable ( initializer ( shape = [ k ], dtype = tf . float32 )), tf . Variable ( initializer ( shape = [ k , k ], dtype = tf . float32 ))) v1 , v2 = make_variables ( 3 , tf . constant_initializer ( 2. )) v1 < tf . Variable ... shape = ( 3 ,) ... numpy = array ([ 2. , 2. , 2. ], dtype = float32 ) > v2 < tf . Variable ... shape = ( 3 , 3 ) ... numpy = array ([[ 2. , 2. , 2. ], [ 2. , 2. , 2. ], [ 2. , 2. , 2. ]], dtype = float32 ) > make_variables ( 4 , tf . random_uniform_initializer ( minval =- 1. , maxval = 1. )) ( < tf . Variable ... shape = ( 4 ,) dtype = float32 ... > , < tf . Variable ... shape = ( 4 , 4 ) ... value = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ] init = tf . constant_initializer ( value ) # Fitting shape tf . Variable ( init ( shape = [ 2 , 4 ], dtype = tf . float32 )) < tf . Variable ... array ([[ 0. , 1. , 2. , 3. ], [ 4. , 5. , 6. , 7. ]], dtype = float32 ) > # Larger shape tf . Variable ( init ( shape = [ 3 , 4 ], dtype = tf . float32 )) Traceback ( most recent call last ): TypeError : ... value has 8 elements , shape is ( 3 , 4 ) with 12 elements ... # Smaller shape tf . Variable ( init ( shape = [ 2 , 3 ], dtype = tf . float32 )) Traceback ( most recent call last ): TypeError : ... value has 8 elements , shape is ( 2 , 3 ) with 6 elements ... Args value A Python scalar, list or tuple of values, or a N-dimensional numpy
array. All elements of the initialized variable will be set to the
corresponding value in the value argument. support_partition If true, the initizer supports passing partition
offset and partition shape arguments to variable creators. This is
particularly useful when initializing sharded variables where each
variable shard is initialized to a slice of constant initializer. Raises TypeError If the input value is not one of the expected types. Methods from_config View source @classmethod from_config ( config ) Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform ( - 1 , 1 ) config = initializer . get_config () initializer = RandomUniform . from_config ( config ) Args config A Python dictionary.
It will typically be the output of get_config . Returns An Initializer instance. get_config View source get_config () Returns the configuration of the initializer as a JSON-serializable dict. Returns A JSON-serializable Python dict. __call__ View source __call__ ( shape , dtype = None , ** kwargs ) Returns a tensor object initialized as specified by the initializer. Args shape Shape of the tensor. dtype Optional dtype of the tensor. If not provided the dtype of the
tensor created will be the type of the inital value. **kwargs Additional keyword arguments. Raises TypeError If the initializer cannot create a tensor of the requested
dtype.


Page: https://www.tensorflow.org/api_docs/python/tf/name_scope
View source on GitHub A context manager for use when defining a Python op. tf . name_scope ( name ) -> None Used in the notebooks Used in the guide Used in the tutorials Migrating model checkpoints Displaying text data in TensorBoard Graph-based Neural Structured Learning in TFX This context manager pushes a name scope, which will make the name of all
operations added within it have a prefix. For example, to define a new Python op called my_op : def my_op ( a , b , c , name = None ): with tf . name_scope ( "MyOp" ) as scope : a = tf . convert_to_tensor ( a , name = "a" ) b = tf . convert_to_tensor ( b , name = "b" ) c = tf . convert_to_tensor ( c , name = "c" ) # Define some computation that uses `a`, `b`, and `c`. return foo_op ( ... , name = scope ) When executed, the Tensors a , b , c , will have names MyOp/a , MyOp/b ,
and MyOp/c . Inside a tf.function , if the scope name already exists, the name will be
made unique by appending _n . For example, calling my_op the second time
will generate MyOp_1/a , etc. Args name The prefix to use on all names created within the name scope. Raises ValueError If name is not a string. Attributes name Methods __enter__ View source __enter__ () -> str Start the scope block. Returns The scope name. __exit__ View source __exit__ ( type_arg : None , value_arg : None , traceback_arg : None ) -> bool Raise any exception triggered within the runtime context.


Page: https://www.tensorflow.org/api_docs/python/tf/ones_initializer
View source on GitHub Initializer that generates tensors initialized to 1. Initializers allow you to pre-specify an initialization strategy, encoded in
the Initializer object, without knowing the shape and dtype of the variable
being initialized. Examples: def make_variables ( k , initializer ): return ( tf . Variable ( initializer ( shape = [ k ], dtype = tf . float32 )), tf . Variable ( initializer ( shape = [ k , k ], dtype = tf . float32 ))) v1 , v2 = make_variables ( 3 , tf . ones_initializer ()) v1 < tf . Variable ... shape = ( 3 ,) ... numpy = array ([ 1. , 1. , 1. ], dtype = float32 ) > v2 < tf . Variable ... shape = ( 3 , 3 ) ... numpy = array ([[ 1. , 1. , 1. ], [ 1. , 1. , 1. ], [ 1. , 1. , 1. ]], dtype = float32 ) > make_variables ( 4 , tf . random_uniform_initializer ( minval =- 1. , maxval = 1. )) ( < tf . Variable ... shape = ( 4 ,) dtype = float32 ... > , < tf . Variable ... shape = ( 4 , 4 ) ... Methods from_config View source @classmethod from_config ( config ) Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform ( - 1 , 1 ) config = initializer . get_config () initializer = RandomUniform . from_config ( config ) Args config A Python dictionary.
It will typically be the output of get_config . Returns An Initializer instance. get_config View source get_config () Returns the configuration of the initializer as a JSON-serializable dict. Returns A JSON-serializable Python dict. __call__ View source __call__ ( shape , dtype = tf . dtypes . float32 , ** kwargs ) Returns a tensor object initialized as specified by the initializer. Args shape Shape of the tensor. dtype Optional dtype of the tensor. Only numeric or boolean dtypes are
supported. **kwargs Additional keyword arguments. Raises ValuesError If the dtype is not numeric or boolean.


Page: https://www.tensorflow.org/api_docs/python/tf/random_normal_initializer
View source on GitHub Initializer that generates tensors with a normal distribution. tf . random_normal_initializer ( mean = 0.0 , stddev = 0.05 , seed = None ) Used in the notebooks Used in the tutorials pix2pix: Image-to-image translation with a conditional GAN Initializers allow you to pre-specify an initialization strategy, encoded in
the Initializer object, without knowing the shape and dtype of the variable
being initialized. Examples: def make_variables ( k , initializer ): return ( tf . Variable ( initializer ( shape = [ k ], dtype = tf . float32 )), tf . Variable ( initializer ( shape = [ k , k ], dtype = tf . float32 ))) v1 , v2 = make_variables ( 3 , tf . random_normal_initializer ( mean = 1. , stddev = 2. )) v1 < tf . Variable ... shape = ( 3 ,) ... numpy = array ([ ... ], dtype = float32 ) > v2 < tf . Variable ... shape = ( 3 , 3 ) ... numpy = make_variables ( 4 , tf . random_uniform_initializer ( minval =- 1. , maxval = 1. )) ( < tf . Variable ... shape = ( 4 ,) dtype = float32 ... > , < tf . Variable ... shape = ( 4 , 4 ) ... Args mean a python scalar or a scalar tensor. Mean of the random values to
generate. stddev a python scalar or a scalar tensor. Standard deviation of the random
values to generate. seed A Python integer. Used to create random seeds. See tf.random.set_seed for behavior. Methods from_config View source @classmethod from_config ( config ) Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform ( - 1 , 1 ) config = initializer . get_config () initializer = RandomUniform . from_config ( config ) Args config A Python dictionary.
It will typically be the output of get_config . Returns An Initializer instance. get_config View source get_config () Returns the configuration of the initializer as a JSON-serializable dict. Returns A JSON-serializable Python dict. __call__ View source __call__ ( shape , dtype = tf . dtypes . float32 , ** kwargs ) Returns a tensor object initialized as specified by the initializer. Args shape Shape of the tensor. dtype Optional dtype of the tensor. Only floating point types are
supported. **kwargs Additional keyword arguments. Raises ValueError If the dtype is not floating point


Page: https://www.tensorflow.org/api_docs/python/tf/random_uniform_initializer
View source on GitHub Initializer that generates tensors with a uniform distribution. tf . random_uniform_initializer ( minval =- 0.05 , maxval = 0.05 , seed = None ) Used in the notebooks Used in the tutorials Parametrized Quantum Circuits for Reinforcement Learning Initializers allow you to pre-specify an initialization strategy, encoded in
the Initializer object, without knowing the shape and dtype of the variable
being initialized. Examples: def make_variables ( k , initializer ): return ( tf . Variable ( initializer ( shape = [ k ], dtype = tf . float32 )), tf . Variable ( initializer ( shape = [ k , k ], dtype = tf . float32 ))) v1 , v2 = make_variables ( 3 , tf . ones_initializer ()) v1 < tf . Variable ... shape = ( 3 ,) ... numpy = array ([ 1. , 1. , 1. ], dtype = float32 ) > v2 < tf . Variable ... shape = ( 3 , 3 ) ... numpy = array ([[ 1. , 1. , 1. ], [ 1. , 1. , 1. ], [ 1. , 1. , 1. ]], dtype = float32 ) > make_variables ( 4 , tf . random_uniform_initializer ( minval =- 1. , maxval = 1. )) ( < tf . Variable ... shape = ( 4 ,) dtype = float32 ... > , < tf . Variable ... shape = ( 4 , 4 ) ... Args minval A python scalar or a scalar tensor. Lower bound of the range of
random values to generate (inclusive). maxval A python scalar or a scalar tensor. Upper bound of the range of
random values to generate (exclusive). seed A Python integer. Used to create random seeds. See tf.random.set_seed for behavior. Methods from_config View source @classmethod from_config ( config ) Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform ( - 1 , 1 ) config = initializer . get_config () initializer = RandomUniform . from_config ( config ) Args config A Python dictionary.
It will typically be the output of get_config . Returns An Initializer instance. get_config View source get_config () Returns the configuration of the initializer as a JSON-serializable dict. Returns A JSON-serializable Python dict. __call__ View source __call__ ( shape , dtype = tf . dtypes . float32 , ** kwargs ) Returns a tensor object initialized as specified by the initializer. Args shape Shape of the tensor. dtype Optional dtype of the tensor. Only floating point and integer
types are supported. **kwargs Additional keyword arguments. Raises ValueError If the dtype is not numeric.


Page: https://www.tensorflow.org/api_docs/python/tf/zeros_initializer
View source on GitHub Initializer that generates tensors initialized to 0. Initializers allow you to pre-specify an initialization strategy, encoded in
the Initializer object, without knowing the shape and dtype of the variable
being initialized. Examples: def make_variables ( k , initializer ): return ( tf . Variable ( initializer ( shape = [ k ], dtype = tf . float32 )), tf . Variable ( initializer ( shape = [ k , k ], dtype = tf . float32 ))) v1 , v2 = make_variables ( 3 , tf . zeros_initializer ()) v1 < tf . Variable ... shape = ( 3 ,) ... numpy = array ([ 0. , 0. , 0. ], dtype = float32 ) > v2 < tf . Variable ... shape = ( 3 , 3 ) ... numpy = array ([[ 0. , 0. , 0. ], [ 0. , 0. , 0. ], [ 0. , 0. , 0. ]], dtype = float32 ) > make_variables ( 4 , tf . random_uniform_initializer ( minval =- 1. , maxval = 1. )) ( < tf . Variable ... shape = ( 4 ,) dtype = float32 ... > , < tf . Variable ... shape = ( 4 , 4 ) ... Methods from_config View source @classmethod from_config ( config ) Instantiates an initializer from a configuration dictionary. Example: initializer = RandomUniform ( - 1 , 1 ) config = initializer . get_config () initializer = RandomUniform . from_config ( config ) Args config A Python dictionary.
It will typically be the output of get_config . Returns An Initializer instance. get_config View source get_config () Returns the configuration of the initializer as a JSON-serializable dict. Returns A JSON-serializable Python dict. __call__ View source __call__ ( shape , dtype = tf . dtypes . float32 , ** kwargs ) Returns a tensor object initialized as specified by the initializer. Args shape Shape of the tensor. dtype Optional dtype of the tensor. Only numeric or boolean dtypes are
supported. **kwargs Additional keyword arguments. Raises ValuesError If the dtype is not numeric or boolean.


Page: https://www.tensorflow.org/api_docs/python/tf/debugging/Assert
View source on GitHub Asserts that the given condition is true. View aliases Main aliases tf.Assert Compat aliases for migration See Migration guide for
more details. tf.compat.v1.Assert , tf.compat.v1.debugging.Assert tf . debugging . Assert ( condition , data , summarize = None , name = None ) Used in the notebooks Used in the tutorials Client-efficient large-model federated learning via `federated_select` and sparse aggregation If condition evaluates to false, print the list of tensors in data . summarize determines how many entries of the tensors to print. Args condition The condition to evaluate. data The tensors to print out when condition is false. summarize Print this many entries of each tensor. name A name for this operation (optional). Returns assert_op An Operation that, when executed, raises a tf.errors.InvalidArgumentError if condition is not true. Raises Note: The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method. TF1 compatibility When in TF V1 mode (that is, outside tf.function ) Assert needs a control
  dependency on the output to ensure the assertion executes: # Ensure maximum element of x is smaller or equal to 1 assert_op = tf . Assert ( tf . less_equal ( tf . reduce_max ( x ), 1. ), [ x ]) with tf . control_dependencies ([ assert_op ]): ... code using x ...


Page: https://www.tensorflow.org/api_docs/python/tf/math/abs
View source on GitHub Computes the absolute value of a tensor. View aliases Main aliases tf.abs Compat aliases for migration See Migration guide for
more details. tf.compat.v1.abs tf . math . abs ( x , name = None ) Used in the notebooks Used in the guide Used in the tutorials Training checkpoints Matrix approximation with Core APIs Estimators Better performance with tf.function TF-NumPy Type Promotion CycleGAN Neural style transfer Simple audio recognition: Recognizing keywords pix2pix: Image-to-image translation with a conditional GAN Integrated gradients Given a tensor of integer or floating-point values, this operation returns a
tensor of the same type, where each element contains the absolute value of the
corresponding element in the input. Given a tensor x of complex numbers, this operation returns a tensor of type float32 or float64 that is the absolute value of each element in x . For
a complex number \(a + bj\), its absolute value is computed as
\(\sqrt{a^2 + b^2}\). For example: # real number x = tf . constant ([ - 2.25 , 3.25 ]) tf . abs ( x ) < tf . Tensor : shape = ( 2 ,), dtype = float32 , numpy = array ([ 2.25 , 3.25 ], dtype = float32 ) > # complex number x = tf . constant ([[ - 2.25 + 4.75 j ], [ - 3.25 + 5.75 j ]]) tf . abs ( x ) < tf . Tensor : shape = ( 2 , 1 ), dtype = float64 , numpy = array ([[ 5.25594901 ], [ 6.60492241 ]]) > Args x A Tensor or SparseTensor of type float16 , float32 , float64 , int32 , int64 , complex64 or complex128 . name A name for the operation (optional). Returns A Tensor or SparseTensor of the same size, type and sparsity as x ,
  with absolute values. Note, for complex64 or complex128 input, the
  returned Tensor will be of type float32 or float64 , respectively. If x is a SparseTensor , returns SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)


Page: https://www.tensorflow.org/api_docs/python/tf/math/acos
View source on GitHub Computes acos of x element-wise. View aliases Main aliases tf.acos Compat aliases for migration See Migration guide for
more details. tf.compat.v1.acos tf . math . acos ( x , name = None ) Used in the notebooks Used in the tutorials Universal Sentence Encoder Universal Sentence Encoder-Lite demo Provided an input tensor, the tf.math.acos operation
returns the inverse cosine of each element of the tensor.
If y = tf.math.cos(x) then, x = tf.math.acos(y) . Input range is [-1, 1] and the output has a range of [0, pi] . For example: x = tf . constant ([ 1.0 , - 0.5 , 3.4 , 0.2 , 0.0 , - 2 ], dtype = tf . float32 ) tf . math . acos ( x ) < tf . Tensor : shape = ( 6 ,), dtype = float32 , numpy = array ([ 0. , 2.0943952 , nan , 1.3694383 , 1.5707964 , nan ], dtype = float32 ) > Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x.


Page: https://www.tensorflow.org/api_docs/python/tf/math/acosh
Computes inverse hyperbolic cosine of x element-wise. View aliases Main aliases tf.acosh Compat aliases for migration See Migration guide for
more details. tf.compat.v1.acosh tf . math . acosh ( x : Annotated [ Any , TV_Acosh_T ], name = None ) -> Annotated [ Any , TV_Acosh_T ] Given an input tensor, the function computes inverse hyperbolic cosine of every element.
Input range is [1, inf] . It returns nan if the input lies outside the range. x = tf . constant ([ - 2 , - 0.5 , 1 , 1.2 , 200 , 10000 , float ( "inf" )]) tf . math . acosh ( x ) == > [ nan nan 0. 0.62236255 5.9914584 9.903487 inf ] Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/add
View source on GitHub Returns x + y element-wise. View aliases Main aliases tf.add Compat aliases for migration See Migration guide for
more details. tf.compat.v1.add tf . math . add ( x , y , name = None ) Used in the notebooks Used in the guide Used in the tutorials TF-NumPy Type Promotion Distributed training with Core APIs and DTensor Logistic regression for binary classification with Core APIs Multilayer perceptrons for digit recognition with Core APIs Quickstart for the TensorFlow Core APIs Customization basics: tensors and operations Introduction to Fairness Indicators Building Your Own Federated Learning Algorithm Custom Federated Algorithms, Part 1: Introduction to the Federated Core Client-efficient large-model federated learning via `federated_select` and sparse aggregation Example usages below. Add a scalar and a list: x = [ 1 , 2 , 3 , 4 , 5 ] y = 1 tf . add ( x , y ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 2 , 3 , 4 , 5 , 6 ], dtype = int32 ) > Note that binary + operator can be used instead: x = tf . convert_to_tensor ([ 1 , 2 , 3 , 4 , 5 ]) y = tf . convert_to_tensor ( 1 ) x + y < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 2 , 3 , 4 , 5 , 6 ], dtype = int32 ) > Add a tensor and a list of same shape: x = [ 1 , 2 , 3 , 4 , 5 ] y = tf . constant ([ 1 , 2 , 3 , 4 , 5 ]) tf . add ( x , y ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 2 , 4 , 6 , 8 , 10 ], dtype = int32 ) > Warning: If one of the inputs ( x or y ) is a tensor and the other is a
non-tensor, the non-tensor input will adopt (or get casted to) the data type
of the tensor input. This can potentially cause unwanted overflow or underflow
conversion. For example, x = tf . constant ([ 1 , 2 ], dtype = tf . int8 ) y = [ 2 ** 7 + 1 , 2 ** 7 + 2 ] tf . add ( x , y ) < tf . Tensor : shape = ( 2 ,), dtype = int8 , numpy = array ([ - 126 , - 124 ], dtype = int8 ) > When adding two input values of different shapes, Add follows NumPy
broadcasting rules. The two input array shapes are compared element-wise.
Starting with the trailing dimensions, the two dimensions either have to be
equal or one of them needs to be 1 . For example, x = np . ones ( 6 ) . reshape ( 1 , 2 , 1 , 3 ) y = np . ones ( 6 ) . reshape ( 2 , 1 , 3 , 1 ) tf . add ( x , y ) . shape . as_list () [ 2 , 2 , 3 , 3 ] Another example with two arrays of different dimension. x = np . ones ([ 1 , 2 , 1 , 4 ]) y = np . ones ([ 3 , 4 ]) tf . add ( x , y ) . shape . as_list () [ 1 , 2 , 3 , 4 ] The reduction version of this elementwise operation is tf.math.reduce_sum Args x A tf.Tensor . Must be one of the following types: bfloat16, half,
float16, float32, float64, uint8, uint16, uint32, uint64, int8, int16,
int32, int64, complex64, complex128, string. y A tf.Tensor . Must have the same type as x. name A name for the operation (optional)


Page: https://www.tensorflow.org/api_docs/python/tf/math/add_n
View source on GitHub Returns the element-wise sum of a list of tensors. View aliases Main aliases tf.add_n Compat aliases for migration See Migration guide for
more details. tf.compat.v1.add_n , tf.compat.v1.math.add_n tf . math . add_n ( inputs , name = None ) Used in the notebooks Used in the guide Used in the tutorials Validating correctness & numerical equivalence Use TF1.x models in TF2 workflows Effective Tensorflow 2 Distributed training with TensorFlow Use a GPU Neural style transfer Custom training with tf.distribute.Strategy Custom training loop with Keras and MultiWorkerMirroredStrategy Parameter server training with ParameterServerStrategy Overfit and underfit All inputs in the list must have the same shape. This op does not broadcast its inputs. If you need broadcasting, use tf.math.add (or the + operator)
instead. For example: a = tf . constant ([[ 3 , 5 ], [ 4 , 8 ]]) b = tf . constant ([[ 1 , 6 ], [ 2 , 9 ]]) tf . math . add_n ([ a , b , a ]) . numpy () array ([[ 7 , 16 ], [ 10 , 25 ]], dtype = int32 ) See Also: tf.reduce_sum(inputs, axis=0) - This performs the same mathematical
operation, but tf.add_n may be more efficient because it sums the
tensors directly. reduce_sum on the other hand calls tf.convert_to_tensor on the list of tensors, unnecessarily stacking them
into a single tensor before summing. Args inputs A list of tf.Tensor or tf.IndexedSlices objects, each with the
same shape and type. tf.IndexedSlices objects will be converted into
dense tensors prior to adding. name A name for the operation (optional). Returns A tf.Tensor of the same shape and type as the elements of inputs . Raises ValueError If inputs don't all have same shape and dtype or the shape
cannot be inferred.


Page: https://www.tensorflow.org/api_docs/python/tf/approx_top_k
Returns min/max k values and their indices of the input operand in an approximate manner. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.approx_top_k tf . approx_top_k ( input : Annotated [ Any , TV_ApproxTopK_T ], k : int , reduction_dimension : int = - 1 , recall_target : float = 0.95 , is_max_k : bool = True , reduction_input_size_override : int = - 1 , aggregate_to_topk : bool = True , name = None ) See https://arxiv.org/abs/2206.14286 for the algorithm details.
This op is only optimized on TPU currently. Args input A Tensor . Must be one of the following types: half , bfloat16 , float32 .
Array to search. Must be at least 1-D of the floating type k An int that is >= 0 . Specifies the number of min/max-k. reduction_dimension An optional int . Defaults to -1 .
Integer dimension along which to search. Default: -1. recall_target An optional float . Defaults to 0.95 .
Recall target for the approximation. Range in (0,1] is_max_k An optional bool . Defaults to True .
When true, computes max-k; otherwise computes min-k. reduction_input_size_override An optional int . Defaults to -1 .
When set to a positive value, it overrides the size determined by input[reduction_dim] for evaluating the recall. This option is useful when
the given input is only a subset of the overall computation in SPMD or
distributed pipelines, where the true input size cannot be deferred by the input shape. aggregate_to_topk An optional bool . Defaults to True .
When true, aggregates approximate results to top-k. When false, returns the
approximate results. The number of the approximate results is implementation
defined and is greater equals to the specified k . name A name for the operation (optional). Returns A tuple of Tensor objects (values, indices). values A Tensor . Has the same type as input . indices A Tensor of type int32 .


Page: https://www.tensorflow.org/api_docs/python/tf/math/argmax
View source on GitHub Returns the index with the largest value across axes of a tensor. View aliases Main aliases tf.argmax tf . math . argmax ( input , axis = None , output_type = tf . dtypes . int64 , name = None ) Used in the notebooks Used in the guide Used in the tutorials Multilayer perceptrons for digit recognition with Core APIs Migrate metrics and optimizers Distributed training with Core APIs and DTensor Introduction to Tensors Introduction to Variables Transfer learning with YAMNet for environmental sound classification Transfer learning with TensorFlow Hub Custom training: walkthrough Simple audio recognition: Recognizing keywords Distributed training with DTensors In case of identity returns the smallest index. For example: A = tf . constant ([ 2 , 20 , 30 , 3 , 6 ]) tf . math . argmax ( A ) # A[2] is maximum in tensor A < tf . Tensor : shape = (), dtype = int64 , numpy = 2 > B = tf . constant ([[ 2 , 20 , 30 , 3 , 6 ], [ 3 , 11 , 16 , 1 , 8 ], [ 14 , 45 , 23 , 5 , 27 ]]) tf . math . argmax ( B , 0 ) < tf . Tensor : shape = ( 5 ,), dtype = int64 , numpy = array ([ 2 , 2 , 0 , 2 , 2 ]) > tf . math . argmax ( B , 1 ) < tf . Tensor : shape = ( 3 ,), dtype = int64 , numpy = array ([ 2 , 2 , 1 ]) > C = tf . constant ([ 0 , 0 , 0 , 0 ]) tf . math . argmax ( C ) # Returns smallest index in case of ties < tf . Tensor : shape = (), dtype = int64 , numpy = 0 > Args input A Tensor . axis An integer, the axis to reduce across. Default to 0. output_type An optional output dtype ( tf.int32 or tf.int64 ). Defaults
to tf.int64 . name An optional name for the operation. Returns A Tensor of type output_type .


Page: https://www.tensorflow.org/api_docs/python/tf/math/argmin
View source on GitHub Returns the index with the smallest value across axes of a tensor. View aliases Main aliases tf.argmin tf . math . argmin ( input , axis = None , output_type = tf . dtypes . int64 , name = None ) Used in the notebooks Used in the guide Matrix approximation with Core APIs Returns the smallest index in case of ties. Args input A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , complex64 , int64 , qint8 , quint8 , qint32 , bfloat16 , uint16 , complex128 , half , uint32 , uint64 . axis A Tensor . Must be one of the following types: int32 , int64 .
int32 or int64, must be in the range -rank(input), rank(input)) .
Describes which axis of the input Tensor to reduce across. For vectors,
use axis = 0. output_type An optional tf.DType from: tf.int32, tf.int64 . Defaults to tf.int64 . name A name for the operation (optional). Returns A Tensor of type output_type . Usage: import tensorflow as tf a = [ 1 , 10 , 26.9 , 2.8 , 166.32 , 62.3 ] b = tf . math . argmin ( input = a ) c = tf . keras . backend . eval ( b ) # c = 0 # here a[0] = 1 which is the smallest element of a across axis 0


Page: https://www.tensorflow.org/api_docs/python/tf/argsort
View source on GitHub Returns the indices of a tensor that give its sorted order along an axis. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.argsort tf . argsort ( values , axis =- 1 , direction = 'ASCENDING' , stable = False , name = None ) Used in the notebooks Used in the tutorials MoViNet for streaming action recognition Image Classification with TensorFlow Hub Client-efficient large-model federated learning via `federated_select` and sparse aggregation values = [ 1 , 10 , 26.9 , 2.8 , 166.32 , 62.3 ] sort_order = tf . argsort ( values ) sort_order . numpy () array ([ 0 , 3 , 1 , 2 , 5 , 4 ], dtype = int32 ) For a 1D tensor: sorted = tf . gather ( values , sort_order ) assert tf . reduce_all ( sorted == tf . sort ( values )) For higher dimensions, the output has the same shape as values , but along the given axis, values represent the index of the sorted
element in that slice of the tensor at the given position. mat = [[ 30 , 20 , 10 ], [ 20 , 10 , 30 ], [ 10 , 30 , 20 ]] indices = tf . argsort ( mat ) indices . numpy () array ([[ 2 , 1 , 0 ], [ 1 , 0 , 2 ], [ 0 , 2 , 1 ]], dtype = int32 ) If axis=-1 these indices can be used to apply a sort using tf.gather : tf . gather ( mat , indices , batch_dims =- 1 ) . numpy () array ([[ 10 , 20 , 30 ], [ 10 , 20 , 30 ], [ 10 , 20 , 30 ]], dtype = int32 ) See also tf.sort : Sort along an axis. tf.math.top_k : A partial sort that returns a fixed number of top values
and corresponding indices. Args values 1-D or higher numeric Tensor . axis The axis along which to sort. The default is -1, which sorts the last
axis. direction The direction in which to sort the values ( 'ASCENDING' or 'DESCENDING' ). stable If True, equal elements in the original tensor will not be
re-ordered in the returned order. Unstable sort is not yet implemented,
but will eventually be the default for performance reasons. If you require
a stable order, pass stable=True for forwards compatibility. name Optional name for the operation. Returns An int32 Tensor with the same shape as values . The indices that would
sort each slice of the given values along the given axis . Raises ValueError If axis is not a constant scalar, or the direction is invalid. tf.errors.InvalidArgumentError If the values.dtype is not a float or int type.


Page: https://www.tensorflow.org/api_docs/python/tf/dtypes/as_dtype
View source on GitHub Converts the given type_value to a tf.DType . View aliases Main aliases tf.as_dtype Compat aliases for migration See Migration guide for
more details. tf.compat.v1.as_dtype , tf.compat.v1.dtypes.as_dtype tf . dtypes . as_dtype ( type_value ) Inputs can be existing tf.DType objects, a DataType enum ,
a string type name, or a numpy.dtype . Examples: tf . as_dtype ( 2 ) # Enum value for float64. tf . float64 tf . as_dtype ( 'float' ) tf . float32 tf . as_dtype ( np . int32 ) tf . int32 Note: DType values are interned (i.e. a single instance of each dtype is
stored in a map). When passed a new DType object, as_dtype always returns
the interned value. Args type_value A value that can be converted to a tf.DType object. Returns A DType corresponding to type_value . Raises TypeError If type_value cannot be converted to a DType .


Page: https://www.tensorflow.org/api_docs/python/tf/strings/as_string
Converts each entry in the given tensor to strings. View aliases Main aliases tf.as_string Compat aliases for migration See Migration guide for
more details. tf.compat.v1.as_string , tf.compat.v1.dtypes.as_string , tf.compat.v1.strings.as_string tf . strings . as_string ( input : Annotated [ Any , TV_AsString_T ], precision : int = - 1 , scientific : bool = False , shortest : bool = False , width : int = - 1 , fill : str = '' , name = None ) -> Annotated [ Any , _atypes . String ] Used in the notebooks Used in the guide Used in the tutorials Migrating Keras 2 code to multi-backend Keras 3 Signatures in TensorFlow Lite Recommending movies: retrieval using a sequential model Using TensorFlow Recommenders with TFX TFX Keras Component Tutorial Supports many numeric types and boolean. For Unicode, see the https://www.tensorflow.org/tutorials/representation/unicode tutorial. Examples: tf . strings . as_string ([ 3 , 2 ]) < tf . Tensor : shape = ( 2 ,), dtype = string , numpy = array ([ b '3' , b '2' ], dtype = object ) > tf . strings . as_string ([ 3.1415926 , 2.71828 ], precision = 2 ) . numpy () array ([ b '3.14' , b '2.72' ], dtype = object ) Args input A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 , complex64 , complex128 , bool , variant , string . precision An optional int . Defaults to -1 .
The post-decimal precision to use for floating point numbers.
Only used if precision > -1. scientific An optional bool . Defaults to False .
Use scientific notation for floating point numbers. shortest An optional bool . Defaults to False .
Use shortest representation (either scientific or standard) for
floating point numbers. width An optional int . Defaults to -1 .
Pad pre-decimal numbers to this width.
Applies to both floating point and integer numbers.
Only used if width > -1. fill An optional string . Defaults to "" .
The value to pad if width > -1.  If empty, pads with spaces.
Another typical value is '0'.  String cannot be longer than 1 character. name A name for the operation (optional). Returns A Tensor of type string .


Page: https://www.tensorflow.org/api_docs/python/tf/math/asin
Computes the trignometric inverse sine of x element-wise. View aliases Main aliases tf.asin Compat aliases for migration See Migration guide for
more details. tf.compat.v1.asin tf . math . asin ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] The tf.math.asin operation returns the inverse of tf.math.sin , such that
if y = tf.math.sin(x) then, x = tf.math.asin(y) . Note: The output of tf.math.asin will lie within the invertible range
of sine, i.e [-pi/2, pi/2]. For example: # Note: [1.047, 0.785] ~= [(pi/3), (pi/4)] x = tf . constant ([ 1.047 , 0.785 ]) y = tf . math . sin ( x ) # [0.8659266, 0.7068252] tf . math . asin ( y ) # [1.047, 0.785] = x Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/asinh
Computes inverse hyperbolic sine of x element-wise. View aliases Main aliases tf.asinh Compat aliases for migration See Migration guide for
more details. tf.compat.v1.asinh tf . math . asinh ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Given an input tensor, this function computes inverse hyperbolic sine
  for every element in the tensor. Both input and output has a range of [-inf, inf] . x = tf . constant ([ - float ( "inf" ), - 2 , - 0.5 , 1 , 1.2 , 200 , 10000 , float ( "inf" )]) tf . math . asinh ( x ) == > [ - inf - 1.4436355 - 0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf ] Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/debugging/assert_equal
View source on GitHub Assert the condition x == y holds element-wise. View aliases Main aliases tf.assert_equal tf . debugging . assert_equal ( x , y , message = None , summarize = None , name = None ) Used in the notebooks Used in the tutorials Bayesian Modeling with Joint Distribution TensorFlow Probability Case Study: Covariance Estimation This Op checks that x[i] == y[i] holds for every pair of (possibly
broadcast) elements of x and y . If both x and y are empty, this is
trivially satisfied. If x == y does not hold, message , as well as the first summarize entries of x and y are printed, and InvalidArgumentError is raised. When using inside tf.function , this API takes effects during execution.
It's recommended to use this API with tf.control_dependencies to
ensure the correct execution order. In the following example, without tf.control_dependencies , errors may
not be raised at all.
Check tf.control_dependencies for more details. def check_size ( x ): with tf . control_dependencies ([ tf . debugging . assert_equal ( tf . size ( x ), 3 , message = 'Bad tensor size' )]): return x check_size ( tf . ones ([ 2 , 3 ], tf . float32 )) Traceback ( most recent call last ): InvalidArgumentError : ... Args x Numeric Tensor . y Numeric Tensor , same dtype as and broadcastable to x . message A string to prefix to the default message. (optional) summarize Print this many entries of each tensor. (optional) name A name for this operation (optional).  Defaults to "assert_equal". Returns Op that raises InvalidArgumentError if x == y is False. This can
be used with tf.control_dependencies inside of tf.function s to
block followup computation until the check has executed. Raises InvalidArgumentError if the check can be performed immediately and x == y is False. The check can be performed immediately during eager
execution or if x and y are statically known. eager compatibility returns None


Page: https://www.tensorflow.org/api_docs/python/tf/debugging/assert_greater
View source on GitHub Assert the condition x > y holds element-wise. View aliases Main aliases tf.assert_greater tf . debugging . assert_greater ( x , y , message = None , summarize = None , name = None ) This Op checks that x[i] > y[i] holds for every pair of (possibly
broadcast) elements of x and y . If both x and y are empty, this is
trivially satisfied. If x > y does not hold, message , as well as the first summarize entries of x and y are printed, and InvalidArgumentError is raised. When using inside tf.function , this API takes effects during execution.
It's recommended to use this API with tf.control_dependencies to
ensure the correct execution order. In the following example, without tf.control_dependencies , errors may
not be raised at all.
Check tf.control_dependencies for more details. def check_size ( x ): with tf . control_dependencies ([ tf . debugging . assert_greater ( tf . size ( x ), 9 , message = 'Bad tensor size' )]): return x check_size ( tf . ones ([ 2 , 3 ], tf . float32 )) Traceback ( most recent call last ): InvalidArgumentError : ... Args x Numeric Tensor . y Numeric Tensor , same dtype as and broadcastable to x . message A string to prefix to the default message. (optional) summarize Print this many entries of each tensor. (optional) name A name for this operation (optional).  Defaults to "assert_greater". Returns Op that raises InvalidArgumentError if x > y is False. This can
be used with tf.control_dependencies inside of tf.function s to
block followup computation until the check has executed. Raises InvalidArgumentError if the check can be performed immediately and x == y is False. The check can be performed immediately during eager
execution or if x and y are statically known. eager compatibility returns None


Page: https://www.tensorflow.org/api_docs/python/tf/debugging/assert_less
View source on GitHub Assert the condition x < y holds element-wise. View aliases Main aliases tf.assert_less tf . debugging . assert_less ( x , y , message = None , summarize = None , name = None ) This Op checks that x[i] < y[i] holds for every pair of (possibly
broadcast) elements of x and y . If both x and y are empty, this is
trivially satisfied. If x < y does not hold, message , as well as the first summarize entries of x and y are printed, and InvalidArgumentError is raised. When using inside tf.function , this API takes effects during execution.
It's recommended to use this API with tf.control_dependencies to
ensure the correct execution order. In the following example, without tf.control_dependencies , errors may
not be raised at all.
Check tf.control_dependencies for more details. def check_size ( x ): with tf . control_dependencies ([ tf . debugging . assert_less ( tf . size ( x ), 3 , message = 'Bad tensor size' )]): return x check_size ( tf . ones ([ 2 , 3 ], tf . float32 )) Traceback ( most recent call last ): InvalidArgumentError : ... Args x Numeric Tensor . y Numeric Tensor , same dtype as and broadcastable to x . message A string to prefix to the default message. (optional) summarize Print this many entries of each tensor. (optional) name A name for this operation (optional).  Defaults to "assert_less". Returns Op that raises InvalidArgumentError if x < y is False. This can
be used with tf.control_dependencies inside of tf.function s to
block followup computation until the check has executed. Raises InvalidArgumentError if the check can be performed immediately and x == y is False. The check can be performed immediately during eager
execution or if x and y are statically known. eager compatibility returns None


Page: https://www.tensorflow.org/api_docs/python/tf/debugging/assert_rank
View source on GitHub Assert that x has rank equal to rank . View aliases Main aliases tf.assert_rank tf . debugging . assert_rank ( x , rank , message = None , name = None ) This Op checks that the rank of x is equal to rank . If x has a different rank, message , as well as the shape of x are
printed, and InvalidArgumentError is raised. Args x Tensor . rank Scalar integer Tensor . message A string to prefix to the default message. name A name for this operation (optional). Defaults to
"assert_rank". Returns Op raising InvalidArgumentError unless x has specified rank.
If static checks determine x has correct rank, a no_op is returned.
This can be used with tf.control_dependencies inside of tf.function s
to block followup computation until the check has executed. Raises InvalidArgumentError if the check can be performed immediately and x does not have rank rank . The check can be performed immediately
during eager execution or if the shape of x is statically known. eager compatibility returns None


Page: https://www.tensorflow.org/api_docs/python/tf/math/atan
Computes the trignometric inverse tangent of x element-wise. View aliases Main aliases tf.atan Compat aliases for migration See Migration guide for
more details. tf.compat.v1.atan tf . math . atan ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] The tf.math.atan operation returns the inverse of tf.math.tan , such that
if y = tf.math.tan(x) then, x = tf.math.atan(y) . Note: The output of tf.math.atan will lie within the invertible range
of tan, i.e (-pi/2, pi/2). For example: # Note: [1.047, 0.785] ~= [(pi/3), (pi/4)] x = tf . constant ([ 1.047 , 0.785 ]) y = tf . math . tan ( x ) # [1.731261, 0.99920404] tf . math . atan ( y ) # [1.047, 0.785] = x Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/atan2
Computes arctangent of y/x element-wise, respecting signs of the arguments. View aliases Main aliases tf.atan2 Compat aliases for migration See Migration guide for
more details. tf.compat.v1.atan2 , tf.compat.v1.math.atan2 tf . math . atan2 ( y : Annotated [ Any , tf . raw_ops . Any ], x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the tutorials TFP Release Notes notebook (0.12.1) This is the angle \( \theta \in [-\pi, \pi] \) such that
\[ x = r \cos(\theta) \]
and
\[ y = r \sin(\theta) \]
where \(r = \sqrt{x^2 + y^2} \). For example: x = [ 1. , 1. ] y = [ 1. , - 1. ] print (( tf . math . atan2 ( y , x ) * ( 180 / np . pi )) . numpy ()) [ 45. - 45. ] Args y A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 . x A Tensor . Must have the same type as y . name A name for the operation (optional). Returns A Tensor . Has the same type as y .


Page: https://www.tensorflow.org/api_docs/python/tf/math/atanh
Computes inverse hyperbolic tangent of x element-wise. View aliases Main aliases tf.atanh Compat aliases for migration See Migration guide for
more details. tf.compat.v1.atanh tf . math . atanh ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Given an input tensor, this function computes inverse hyperbolic tangent
  for every element in the tensor. Input range is [-1,1] and output range is [-inf, inf] . If input is -1 , output will be -inf and if the
  input is 1 , output will be inf . Values outside the range will have nan as output. x = tf . constant ([ - float ( "inf" ), - 1 , - 0.5 , 1 , 0 , 0.5 , 10 , float ( "inf" )]) tf . math . atanh ( x ) == > [ nan - inf - 0.54930615 inf 0. 0.54930615 nan nan ] Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/batch_to_space
View source on GitHub BatchToSpace for N-D tensors of type T. tf . batch_to_space ( input , block_shape , crops , name = None ) This operation reshapes the "batch" dimension 0 into M + 1 dimensions of
shape block_shape + [batch] , interleaves these blocks back into the grid
defined by the spatial dimensions [1, ..., M] , to obtain a result with the
same rank as the input.  The spatial dimensions of this intermediate result
are then optionally cropped according to crops to produce the output.  This
is the reverse of SpaceToBatch (see tf.space_to_batch ). Args input A N-D Tensor with shape input_shape = [batch] + spatial_shape +
remaining_shape , where spatial_shape has M dimensions. block_shape A 1-D Tensor with shape [M]. Must be one of the following
types: int32 , int64 . All values must be >= 1. For backwards
compatibility with TF 1.0, this parameter may be an int, in which case it
is converted to numpy.array([block_shape, block_shape],
dtype=numpy.int64) . crops A  2-D Tensor with shape [M, 2] . Must be one of the
following types: int32 , int64 . All values must be >= 0. crops[i] = [crop_start, crop_end] specifies the amount to crop from
input dimension i + 1 , which corresponds to spatial dimension i .
It is required that crop_start[i] + crop_end[i] <= block_shape[i] * input_shape[i + 1] .
This operation is equivalent to the following steps: Reshape input to reshaped of shape: [block_shape[0], ...,
block_shape[M-1], batch / prod(block_shape), input_shape[1], ...,
input_shape[N-1]] Permute dimensions of reshaped to produce permuted of shape
[batch / prod(block_shape),  input_shape[1], block_shape[0], ...,
input_shape[M], block_shape[M-1], input_shape[M+1],
..., input_shape[N-1]] Reshape permuted to produce reshaped_permuted of shape
[batch / prod(block_shape), input_shape[1] * block_shape[0], ...,
input_shape[M] * block_shape[M-1], input_shape[M+1], ...,
input_shape[N-1]] Crop the start and end of dimensions [1, ..., M] of reshaped_permuted according to crops to produce the output
of shape:
[batch / prod(block_shape),  input_shape[1] *
 block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M] *
 block_shape[M-1] - crops[M-1,0] - crops[M-1,1],  input_shape[M+1],
 ..., input_shape[N-1]] name A name for the operation (optional). Examples: For the following input of shape [4, 1, 1, 1] , block_shape = [2, 2] , and crops = [[0, 0], [0, 0]] : [[[[ 1 ]]], [[[ 2 ]]], [[[ 3 ]]], [[[ 4 ]]]] The output tensor has shape [1, 2, 2, 1] and value: x = [[[[ 1 ], [ 2 ]], [[ 3 ], [ 4 ]]]] For the following input of shape [4, 1, 1, 3] , block_shape = [2, 2] , and crops = [[0, 0], [0, 0]] : [[[ 1 , 2 , 3 ]], [[ 4 , 5 , 6 ]], [[ 7 , 8 , 9 ]], [[ 10 , 11 , 12 ]]] The output tensor has shape [1, 2, 2, 3] and value: x = [[[[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], [[ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]]] For the following
input of shape [4, 2, 2, 1] , block_shape = [2, 2] , and crops = [[0, 0], [0, 0]] : x = [[[[ 1 ], [ 3 ]], [[ 9 ], [ 11 ]]], [[[ 2 ], [ 4 ]], [[ 10 ], [ 12 ]]], [[[ 5 ], [ 7 ]], [[ 13 ], [ 15 ]]], [[[ 6 ], [ 8 ]], [[ 14 ], [ 16 ]]]] The output tensor has shape [1, 4, 4, 1] and value: x = [[[ 1 ], [ 2 ], [ 3 ], [ 4 ]], [[ 5 ], [ 6 ], [ 7 ], [ 8 ]], [[ 9 ], [ 10 ], [ 11 ], [ 12 ]], [[ 13 ], [ 14 ], [ 15 ], [ 16 ]]] For the following input of shape [8, 1, 3, 1] , block_shape = [2, 2] , and crops = [[0, 0], [2, 0]] : x = [[[[ 0 ], [ 1 ], [ 3 ]]], [[[ 0 ], [ 9 ], [ 11 ]]], [[[ 0 ], [ 2 ], [ 4 ]]], [[[ 0 ], [ 10 ], [ 12 ]]], [[[ 0 ], [ 5 ], [ 7 ]]], [[[ 0 ], [ 13 ], [ 15 ]]], [[[ 0 ], [ 6 ], [ 8 ]]], [[[ 0 ], [ 14 ], [ 16 ]]]] The output tensor has shape [2, 2, 4, 1] and value: x = [[[[ 1 ], [ 2 ], [ 3 ], [ 4 ]], [[ 5 ], [ 6 ], [ 7 ], [ 8 ]]], [[[ 9 ], [ 10 ], [ 11 ], [ 12 ]], [[ 13 ], [ 14 ], [ 15 ], [ 16 ]]]] Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/bitcast
Bitcasts a tensor from one type to another without copying data. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.bitcast tf . bitcast ( input : Annotated [ Any , TV_Bitcast_T ], type : TV_Bitcast_type , name = None ) -> Annotated [ Any , TV_Bitcast_type ] Given a tensor input , this operation returns a tensor that has the same buffer
data as input with datatype type . If the input datatype T is larger than the output datatype type then the
shape changes from [...] to [..., sizeof( T )/sizeof( type )]. If T is smaller than type , the operator requires that the rightmost
dimension be equal to sizeof( type )/sizeof( T ). The shape then goes from
[..., sizeof( type )/sizeof( T )] to [...]. tf.bitcast() and tf.cast() work differently when real dtype is casted as a complex dtype
(e.g. tf.complex64 or tf.complex128) as tf.cast() make imaginary part 0 while tf.bitcast()
gives module error.
For example, Example 1: a = [ 1. , 2. , 3. ] equality_bitcast = tf . bitcast ( a , tf . complex128 ) Traceback ( most recent call last ): InvalidArgumentError : Cannot bitcast from 1 to 18 [ Op : Bitcast ] equality_cast = tf . cast ( a , tf . complex128 ) print ( equality_cast ) tf . Tensor ([ 1. + 0. j 2. + 0. j 3. + 0. j ], shape = ( 3 ,), dtype = complex128 ) Example 2: tf . bitcast ( tf . constant ( 0xffffffff , dtype = tf . uint32 ), tf . uint8 ) < tf . Tensor : shape = ( 4 ,), dtype = uint8 , numpy = array ([ 255 , 255 , 255 , 255 ], dtype = uint8 ) > Example 3: x = [ 1. , 2. , 3. ] y = [ 0. , 2. , 3. ] equality = tf . equal ( x , y ) equality_cast = tf . cast ( equality , tf . float32 ) equality_bitcast = tf . bitcast ( equality_cast , tf . uint8 ) print ( equality ) tf . Tensor ([ False True True ], shape = ( 3 ,), dtype = bool ) print ( equality_cast ) tf . Tensor ([ 0. 1. 1. ], shape = ( 3 ,), dtype = float32 ) print ( equality_bitcast ) tf . Tensor ( [[ 0 0 0 0 ] [ 0 0 128 63 ] [ 0 0 128 63 ]], shape = ( 3 , 4 ), dtype = uint8 ) Note: Bitcast is implemented as a low-level cast, so machines with different
endian orderings will give different results. A copy from input buffer to output
buffer is made on BE machines when types are of different sizes in order to get
the same casting results as on LE machines. Args input A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , int64 , int32 , uint8 , uint16 , uint32 , uint64 , int8 , int16 , complex64 , complex128 , qint8 , quint8 , qint16 , quint16 , qint32 . type A tf.DType from: tf.bfloat16, tf.half, tf.float32, tf.float64, tf.int64, tf.int32, tf.uint8, tf.uint16, tf.uint32, tf.uint64, tf.int8, tf.int16, tf.complex64, tf.complex128, tf.qint8, tf.quint8, tf.qint16, tf.quint16, tf.qint32 . name A name for the operation (optional). Returns A Tensor of type type .


Page: https://www.tensorflow.org/api_docs/python/tf/boolean_mask
View source on GitHub Apply boolean mask to tensor. tf . boolean_mask ( tensor , mask , axis = None , name = 'boolean_mask' ) Numpy equivalent is tensor[mask] . In general, 0 < dim(mask) = K <= dim(tensor) , and mask 's shape must match
the first K dimensions of tensor 's shape.  We then have: boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd] where (i1,...,iK) is the ith True entry of mask (row-major order).
The axis could be used with mask to indicate the axis to mask from.
In that case, axis + dim(mask) <= dim(tensor) and mask 's shape must match
the first axis + dim(mask) dimensions of tensor 's shape. See also: tf.ragged.boolean_mask , which can be applied to both dense and
ragged tensors, and can be used if you need to preserve the masked dimensions
of tensor (rather than flattening them, as tf.boolean_mask does). Examples: tensor = [ 0 , 1 , 2 , 3 ] # 1-D example mask = np . array ([ True , False , True , False ]) tf . boolean_mask ( tensor , mask ) < tf . Tensor : shape = ( 2 ,), dtype = int32 , numpy = array ([ 0 , 2 ], dtype = int32 ) > tensor = [[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]] # 2-D example mask = np . array ([ True , False , True ]) tf . boolean_mask ( tensor , mask ) < tf . Tensor : shape = ( 2 , 2 ), dtype = int32 , numpy = array ([[ 1 , 2 ], [ 5 , 6 ]], dtype = int32 ) > Args tensor N-D Tensor. mask K-D boolean Tensor, K <= N and K must be known statically. axis A 0-D int Tensor representing the axis in tensor to mask from. By
default, axis is 0 which will mask from the first dimension. Otherwise K +
axis <= N. name A name for this operation (optional). Returns (N-K+1)-dimensional tensor populated by entries in tensor corresponding
to True values in mask . Raises ValueError If shapes do not conform. Examples: # 2-D example tensor = [[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]] mask = np . array ([ True , False , True ]) boolean_mask ( tensor , mask ) # [[1, 2], [5, 6]]


Page: https://www.tensorflow.org/api_docs/python/tf/broadcast_dynamic_shape
View source on GitHub Computes the shape of a broadcast given symbolic shapes. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.broadcast_dynamic_shape tf . broadcast_dynamic_shape ( shape_x , shape_y ) When shape_x and shape_y are Tensors representing shapes (i.e. the result
of calling tf.shape on another Tensor) this computes a Tensor which is the
shape of the result of a broadcasting op applied in tensors of shapes shape_x and shape_y . This is useful when validating the result of a broadcasting operation when the
tensors do not have statically known shapes. Example: shape_x = ( 1 , 2 , 3 ) shape_y = ( 5 , 1 , 3 ) tf . broadcast_dynamic_shape ( shape_x , shape_y ) < tf . Tensor : shape = ( 3 ,), dtype = int32 , numpy = array ([ 5 , 2 , 3 ], ... > Args shape_x A rank 1 integer Tensor , representing the shape of x. shape_y A rank 1 integer Tensor , representing the shape of y. Returns A rank 1 integer Tensor representing the broadcasted shape. Raises InvalidArgumentError If the two shapes are incompatible for
broadcasting.


Page: https://www.tensorflow.org/api_docs/python/tf/broadcast_static_shape
View source on GitHub Computes the shape of a broadcast given known shapes. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.broadcast_static_shape tf . broadcast_static_shape ( shape_x , shape_y ) When shape_x and shape_y are fully known TensorShape s this computes a TensorShape which is the shape of the result of a broadcasting op applied in
tensors of shapes shape_x and shape_y . For example, if shape_x is TensorShape([1, 2, 3]) and shape_y is TensorShape([5, 1, 3]) , the result is a TensorShape whose value is TensorShape([5, 2, 3]) . This is useful when validating the result of a broadcasting operation when the
tensors have statically known shapes. Example: shape_x = tf . TensorShape ([ 1 , 2 , 3 ]) shape_y = tf . TensorShape ([ 5 , 1 , 3 ]) tf . broadcast_static_shape ( shape_x , shape_y ) TensorShape ([ 5 , 2 , 3 ]) Args shape_x A TensorShape shape_y A TensorShape Returns A TensorShape representing the broadcasted shape. Raises ValueError If the two shapes can not be broadcasted.


Page: https://www.tensorflow.org/api_docs/python/tf/broadcast_to
Broadcast an array for a compatible shape. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.broadcast_to tf . broadcast_to ( input : Annotated [ Any , TV_BroadcastTo_T ], shape : Annotated [ Any , TV_BroadcastTo_Tidx ], name = None ) -> Annotated [ Any , TV_BroadcastTo_T ] Used in the notebooks Used in the guide Used in the tutorials Introduction to Tensors Eight schools Human Pose Classification with MoveNet and TensorFlow Lite FFJORD TFP Release Notes notebook (0.13.0) Broadcasting is the process of making arrays to have compatible shapes
for arithmetic operations. Two shapes are compatible if for each
dimension pair they are either equal or one of them is one. For example: x = tf . constant ([[ 1 , 2 , 3 ]]) # Shape (1, 3,) y = tf . broadcast_to ( x , [ 2 , 3 ]) print ( y ) tf . Tensor ( [[ 1 2 3 ] [ 1 2 3 ]], shape = ( 2 , 3 ), dtype = int32 ) In the above example, the input Tensor with the shape of [1, 3] is broadcasted to output Tensor with shape of [2, 3] . When broadcasting, if a tensor has fewer axes than necessary its shape is
padded on the left with ones. So this gives the same result as the previous
example: x = tf . constant ([ 1 , 2 , 3 ]) # Shape (3,) y = tf . broadcast_to ( x , [ 2 , 3 ]) When doing broadcasted operations such as multiplying a tensor
by a scalar, broadcasting (usually) confers some time or space
benefit, as the broadcasted tensor is never materialized. However, broadcast_to does not carry with it any such benefits.
The newly-created tensor takes the full memory of the broadcasted
shape. (In a graph context, broadcast_to might be fused to
subsequent operation and then be optimized away, however.) Args input A Tensor . A Tensor to broadcast. shape A Tensor . Must be one of the following types: int32 , int64 .
An 1-D int Tensor. The shape of the desired output. name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/case
View source on GitHub Create a case operation. tf . case ( pred_fn_pairs , default = None , exclusive = False , strict = False , name = 'case' ) Used in the notebooks Used in the tutorials Tutorial on Multi Armed Bandits in TF-Agents See also tf.switch_case . The pred_fn_pairs parameter is a list of pairs of size N.
Each pair contains a boolean scalar tensor and a python callable that
creates the tensors to be returned if the boolean evaluates to True. default is a callable generating a list of tensors. All the callables
in pred_fn_pairs as well as default (if provided) should return the same
number and types of tensors. If exclusive==True , all predicates are evaluated, and an exception is
thrown if more than one of the predicates evaluates to True .
If exclusive==False , execution stops at the first predicate which
evaluates to True, and the tensors generated by the corresponding function
are returned immediately. If none of the predicates evaluate to True, this
operation returns the tensors generated by default . tf.case supports nested structures as implemented in tf.nest . All of the callables must return the same (possibly nested) value
structure of lists, tuples, and/or named tuples. Singleton lists and tuples
form the only exceptions to this: when returned by a callable, they are
implicitly unpacked to single values. This behavior is disabled by passing strict=True . Example 1: Pseudocode: if ( x < y ) return 17 ; else return 23 ; Expressions: f1 = lambda : tf . constant ( 17 ) f2 = lambda : tf . constant ( 23 ) r = tf . case ([( tf . less ( x , y ), f1 )], default = f2 ) Example 2: Pseudocode: if ( x < y && x > z ) raise OpError ( "Only one predicate may evaluate to True" ); if ( x < y ) return 17 ; else if ( x > z ) return 23 ; else return - 1 ; Expressions: def f1 (): return tf . constant ( 17 ) def f2 (): return tf . constant ( 23 ) def f3 (): return tf . constant ( - 1 ) r = tf . case ([( tf . less ( x , y ), f1 ), ( tf . greater ( x , z ), f2 )], default = f3 , exclusive = True ) Args pred_fn_pairs List of pairs of a boolean scalar tensor and a callable which
returns a list of tensors. default Optional callable that returns a list of tensors. exclusive True iff at most one predicate is allowed to evaluate to True . strict A boolean that enables/disables 'strict' mode; see above. name A name for this operation (optional). Returns The tensors returned by the first pair whose predicate evaluated to True, or
those returned by default if none does. Raises TypeError If pred_fn_pairs is not a list/tuple. TypeError If pred_fn_pairs is a list but does not contain 2-tuples. TypeError If fns[i] is not callable for any i, or default is not
callable. v2 compatibility pred_fn_pairs could be a dictionary in v1. However, tf.Tensor and
tf.Variable are no longer hashable in v2, so cannot be used as a key for a
dictionary.  Please use a list or a tuple instead.


Page: https://www.tensorflow.org/api_docs/python/tf/cast
View source on GitHub Casts a tensor to a new type. View aliases Main aliases tf.dtypes.cast Compat aliases for migration See Migration guide for
more details. tf.compat.v1.cast , tf.compat.v1.dtypes.cast tf . cast ( x , dtype , name = None ) Used in the notebooks Used in the guide Used in the tutorials Multilayer perceptrons for digit recognition with Core APIs Logistic regression for binary classification with Core APIs tf.data: Build TensorFlow input pipelines Distributed training with Core APIs and DTensor Import a JAX model using JAX2TF Scalable model compression DeepDream Parameter server training with ParameterServerStrategy Learned data compression Neural style transfer The operation casts x (in case of Tensor ) or x.values (in case of SparseTensor or IndexedSlices ) to dtype . For example: x = tf . constant ([ 1.8 , 2.2 ], dtype = tf . float32 ) tf . cast ( x , tf . int32 ) < tf . Tensor : shape = ( 2 ,), dtype = int32 , numpy = array ([ 1 , 2 ], dtype = int32 ) > Notice tf.cast has an alias tf.dtypes.cast : x = tf . constant ([ 1.8 , 2.2 ], dtype = tf . float32 ) tf . dtypes . cast ( x , tf . int32 ) < tf . Tensor : shape = ( 2 ,), dtype = int32 , numpy = array ([ 1 , 2 ], dtype = int32 ) > The operation supports data types (for x and dtype ) of uint8 , uint16 , uint32 , uint64 , int8 , int16 , int32 , int64 , float16 , float32 , float64 , complex64 , complex128 , bfloat16 .
In case of casting from complex types ( complex64 , complex128 ) to real
types, only the real part of x is returned. In case of casting from real
types to complex types ( complex64 , complex128 ), the imaginary part of the
returned value is set to 0 . The handling of complex types here matches the
behavior of numpy. Note casting nan and inf values to integral types has undefined behavior. Note this operation can lead to a loss of precision when converting native
Python float and complex variables to tf.float64 or tf.complex128 tensors, since the input is first converted to the float32 data type and
then widened. It is recommended to use tf.convert_to_tensor instead of tf.cast for any non-tensor inputs. Args x A Tensor or SparseTensor or IndexedSlices of numeric type. It could
be uint8 , uint16 , uint32 , uint64 , int8 , int16 , int32 , int64 , float16 , float32 , float64 , complex64 , complex128 , bfloat16 . dtype The destination type. The list of supported dtypes is the same as x . name A name for the operation (optional). Returns A Tensor or SparseTensor or IndexedSlices with same shape as x and
same type as dtype . Raises TypeError If x cannot be cast to the dtype .


Page: https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm
View source on GitHub Clips values of multiple tensors by the ratio of the sum of their norms. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.clip_by_global_norm tf . clip_by_global_norm ( t_list , clip_norm , use_norm = None , name = None ) Given a tuple or list of tensors t_list , and a clipping ratio clip_norm ,
this operation returns a list of clipped tensors list_clipped and the global norm ( global_norm ) of all tensors in t_list . Optionally,
if you've already computed the global norm for t_list , you can specify
the global norm with use_norm . To perform the clipping, the values t_list[i] are set to: t_list [ i ] * clip_norm / max ( global_norm , clip_norm ) where: global_norm = sqrt ( sum ([ l2norm ( t ) ** 2 for t in t_list ])) If clip_norm > global_norm then the entries in t_list remain as they are,
otherwise they're all shrunk by the global ratio. If global_norm == infinity then the entries in t_list are all set to NaN to signal that an error occurred. Any of the entries of t_list that are of type None are ignored. This is the correct way to perform gradient clipping (Pascanu et al., 2012). However, it is slower than clip_by_norm() because all the parameters must be
ready before the clipping operation can be performed. Args t_list A tuple or list of mixed Tensors , IndexedSlices , or None. clip_norm A 0-D (scalar) Tensor > 0. The clipping ratio. use_norm A 0-D (scalar) Tensor of type float (optional). The global
norm to use. If not provided, global_norm() is used to compute the norm. name A name for the operation (optional). Returns list_clipped A list of Tensors of the same type as list_t . global_norm A 0-D (scalar) Tensor representing the global norm. Raises TypeError If t_list is not a sequence. References On the difficulty of training Recurrent Neural Networks: Pascanu et al., 2012 ( pdf )


Page: https://www.tensorflow.org/api_docs/python/tf/clip_by_norm
View source on GitHub Clips tensor values to a maximum L2-norm. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.clip_by_norm tf . clip_by_norm ( t , clip_norm , axes = None , name = None ) Used in the notebooks Used in the guide Advanced automatic differentiation Given a tensor t , and a maximum clip value clip_norm , this operation
normalizes t so that its L2-norm is less than or equal to clip_norm ,
along the dimensions given in axes . Specifically, in the default case
where all dimensions are used for calculation, if the L2-norm of t is
already less than or equal to clip_norm , then t is not modified. If
the L2-norm is greater than clip_norm , then this operation returns a
tensor of the same type and shape as t with its values set to: t * clip_norm / l2norm(t) In this case, the L2-norm of the output tensor is clip_norm . As another example, if t is a matrix and axes == [1] , then each row
of the output will have L2-norm less than or equal to clip_norm . If axes == [0] instead, each column of the output will be clipped. Code example: some_nums = tf . constant ([[ 1 , 2 , 3 , 4 , 5 ]], dtype = tf . float32 ) tf . clip_by_norm ( some_nums , 2.0 ) . numpy () array ([[ 0.26967996 , 0.5393599 , 0.80903983 , 1.0787199 , 1.3483998 ]], dtype = float32 ) This operation is typically used to clip gradients before applying them with
an optimizer.  Most gradient data is a collection of different shaped tensors
for different parts of the model.  Thus, this is a common usage: # Get your gradients after training loss_value , grads = grad ( model , features , labels ) # Apply some clipping grads = [ tf . clip_by_norm ( g , norm ) for g in grads ] # Continue on with training optimizer . apply_gradients ( grads ) Args t A Tensor or IndexedSlices .  This must be a floating point type. clip_norm A 0-D (scalar) Tensor > 0. A maximum clipping value, also
floating point.
Note: If a negative clip_norm is provided, it will be treated as zero. axes A 1-D (vector) Tensor of type int32 containing the dimensions to use
for computing the L2-norm. If None (the default), uses all dimensions. name A name for the operation (optional). Returns A clipped Tensor or IndexedSlices . Raises ValueError If the clip_norm tensor is not a 0-D scalar tensor. TypeError If dtype of the input is not a floating point or
complex type.


Page: https://www.tensorflow.org/api_docs/python/tf/clip_by_value
View source on GitHub Clips tensor values to a specified min and max. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.clip_by_value tf . clip_by_value ( t , clip_value_min , clip_value_max , name = None ) Used in the notebooks Used in the guide Used in the tutorials Matrix approximation with Core APIs Intro to Autoencoders DeepDream Adversarial example using FGSM Neural style transfer Data augmentation Given a tensor t , this operation returns a tensor of the same type and
shape as t with its values clipped to clip_value_min and clip_value_max .
Any values less than clip_value_min are set to clip_value_min . Any values
greater than clip_value_max are set to clip_value_max . Note: clip_value_min needs to be smaller or equal to clip_value_max for
correct results. For example: Basic usage passes a scalar as the min and max value. t = tf . constant ([[ - 10. , - 1. , 0. ], [ 0. , 2. , 10. ]]) t2 = tf . clip_by_value ( t , clip_value_min =- 1 , clip_value_max = 1 ) t2 . numpy () array ([[ - 1. , - 1. , 0. ], [ 0. , 1. , 1. ]], dtype = float32 ) The min and max can be the same size as t , or broadcastable to that size. t = tf . constant ([[ - 1 , 0. , 10. ], [ - 1 , 0 , 10 ]]) clip_min = [[ 2 ],[ 1 ]] t3 = tf . clip_by_value ( t , clip_value_min = clip_min , clip_value_max = 100 ) t3 . numpy () array ([[ 2. , 2. , 10. ], [ 1. , 1. , 10. ]], dtype = float32 ) Broadcasting fails, intentionally, if you would expand the dimensions of t t = tf . constant ([[ - 1 , 0. , 10. ], [ - 1 , 0 , 10 ]]) clip_min = [[[ 2 , 1 ]]] # Has a third axis t4 = tf . clip_by_value ( t , clip_value_min = clip_min , clip_value_max = 100 ) Traceback ( most recent call last ): InvalidArgumentError : Incompatible shapes : [ 2 , 3 ] vs . [ 1 , 1 , 2 ] It throws a TypeError if you try to clip an int to a float value
( tf.cast the input to float first). t = tf . constant ([[ 1 , 2 ], [ 3 , 4 ]], dtype = tf . int32 ) t5 = tf . clip_by_value ( t , clip_value_min =- 3.1 , clip_value_max = 3.1 ) Traceback ( most recent call last ): TypeError : Cannot convert ... Args t A Tensor or IndexedSlices . clip_value_min The minimum value to clip to. A scalar Tensor or one that
is broadcastable to the shape of t . clip_value_max The maximum value to clip to. A scalar Tensor or one that
is broadcastable to the shape of t . name A name for the operation (optional). Returns A clipped Tensor or IndexedSlices . Raises tf.errors.InvalidArgumentError : If the clip tensors would trigger array
broadcasting that would make the returned tensor larger than the input. TypeError If dtype of the input is int32 and dtype of
the clip_value_min or clip_value_max is float32


Page: https://www.tensorflow.org/api_docs/python/tf/dtypes/complex
View source on GitHub Converts two real numbers to a complex number. View aliases Main aliases tf.complex Compat aliases for migration See Migration guide for
more details. tf.compat.v1.complex , tf.compat.v1.dtypes.complex tf . dtypes . complex ( real , imag , name = None ) Used in the notebooks Used in the tutorials Scalable model compression Given a tensor real representing the real part of a complex number, and a
tensor imag representing the imaginary part of a complex number, this
operation returns complex numbers elementwise of the form \(a + bj\), where a represents the real part and b represents the imag part. The input tensors real and imag must have the same shape. For example: real = tf . constant ([ 2.25 , 3.25 ]) imag = tf . constant ([ 4.75 , 5.75 ]) tf . complex ( real , imag ) # [[2.25 + 4.75j], [3.25 + 5.75j]] Args real A Tensor . Must be one of the following types: float32 , float64 . imag A Tensor . Must have the same type as real . name A name for the operation (optional). Returns A Tensor of type complex64 or complex128 . Raises TypeError Real and imag must be correct types


Page: https://www.tensorflow.org/api_docs/python/tf/concat
View source on GitHub Concatenates tensors along one dimension. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.concat tf . concat ( values , axis , name = 'concat' ) Used in the notebooks Used in the guide Used in the tutorials Better performance with the tf.data API Ragged tensors TensorFlow basics Quickstart for the TensorFlow Core APIs Customizing what happens in `fit()` with TensorFlow Simple audio recognition: Recognizing keywords Learned data compression Integrated gradients Load a pandas DataFrame Transfer learning for video classification with MoViNet See also tf.tile , tf.stack , tf.repeat . Concatenates the list of tensors values along dimension axis .  If values[i].shape = [D0, D1, ... Daxis(i), ...Dn] , the concatenated
result has shape [ D0 , D1 , ... Raxis , ... Dn ] where Raxis = sum ( Daxis ( i )) That is, the data from the input tensors is joined along the axis dimension. The number of dimensions of the input tensors must match, and all dimensions
except axis must be equal. For example: t1 = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]] t2 = [[ 7 , 8 , 9 ], [ 10 , 11 , 12 ]] tf . concat ([ t1 , t2 ], 0 ) < tf . Tensor : shape = ( 4 , 3 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 10 , 11 , 12 ]], dtype = int32 ) > tf . concat ([ t1 , t2 ], 1 ) < tf . Tensor : shape = ( 2 , 6 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 , 7 , 8 , 9 ], [ 4 , 5 , 6 , 10 , 11 , 12 ]], dtype = int32 ) > As in Python, the axis could also be negative numbers. Negative axis are interpreted as counting from the end of the rank, i.e., axis + rank(values) -th dimension. For example: t1 = [[[ 1 , 2 ], [ 2 , 3 ]], [[ 4 , 4 ], [ 5 , 3 ]]] t2 = [[[ 7 , 4 ], [ 8 , 4 ]], [[ 2 , 10 ], [ 15 , 11 ]]] tf . concat ([ t1 , t2 ], - 1 ) < tf . Tensor : shape = ( 2 , 2 , 4 ), dtype = int32 , numpy = array ([[[ 1 , 2 , 7 , 4 ], [ 2 , 3 , 8 , 4 ]], [[ 4 , 4 , 2 , 10 ], [ 5 , 3 , 15 , 11 ]]], dtype = int32 ) > Note: If you are concatenating along a new axis consider using stack.
E.g. tf . concat ([ tf . expand_dims ( t , axis ) for t in tensors ], axis ) can be rewritten as tf . stack ( tensors , axis = axis ) Args values A list of Tensor objects or a single Tensor . axis 0-D int32 Tensor .  Dimension along which to concatenate. Must be
in the range [-rank(values), rank(values)) . As in Python, indexing for
axis is 0-based. Positive axis in the rage of [0, rank(values)) refers
to axis -th dimension. And negative axis refers to axis +
rank(values) -th dimension. name A name for the operation (optional). Returns A Tensor resulting from concatenation of the input tensors.


Page: https://www.tensorflow.org/api_docs/python/tf/cond
View source on GitHub Return true_fn() if the predicate pred is true else false_fn() . tf . cond ( pred , true_fn = None , false_fn = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Extension types Tutorial on Multi Armed Bandits in TF-Agents Sending Different Data To Particular Clients With tff.federated_select End to end example for BigQuery TensorFlow reader Instance Segmentation with Model Garden Note: This op is automatically used in a tf.function to convert Python
if-statements when the predicate is a tf.Tensor , unless autograph=False is
explicitly specified in tf.function args. For example, the following are
equivalent: @tf . function def fun1 ( x , y ): if x > 0 : # AutoGraph converts if-statement to tf.cond(). z = y + 1 else : z = y - 1 return z fun1 ( tf . constant ( 7 ), tf . constant ( 3 )) . numpy () 4 @tf . function def fun2 ( x , y ): pred = x > 0 true_fn = lambda : y + 1 false_fn = lambda : y - 1 return tf . cond ( pred , true_fn , false_fn ) # Use tf.cond() explicitly. fun1 ( tf . constant ( 7 ), tf . constant ( 3 )) . numpy () 4 For more information, see tf.function and AutoGraph guide . true_fn and false_fn both return lists of output tensors. true_fn and false_fn must have the same non-zero number and type of outputs. Warning: Any Tensors or Operations created outside of true_fn and false_fn will be executed regardless of which branch is selected at runtime. Although this behavior is consistent with the dataflow model of TensorFlow,
it has frequently surprised users who expected a lazier semantics.
Consider the following simple program: x , y = tf . constant ( 2 , dtype = tf . int32 ), tf . constant ( 4 , dtype = tf . int32 ) z = tf . multiply ( x , y ) r = tf . cond ( x < y , lambda : tf . add ( x , z ), lambda : tf . square ( y )) r . numpy () 10 If x < y , the tf.add operation will be executed and tf.square operation will not be executed. Since z is needed for at least one
branch of the cond , the tf.multiply operation is always executed,
unconditionally. Note that cond calls true_fn and false_fn exactly once (inside the
call to cond , and not at all during Session.run() ). cond stitches together the graph fragments created during the true_fn and false_fn calls with some additional graph nodes to ensure that the right
branch gets executed depending on the value of pred . tf.cond supports nested structures as implemented in tensorflow.python.util.nest . Both true_fn and false_fn must return the
same (possibly nested) value structure of lists, tuples, and/or named tuples.
Singleton lists and tuples form the only exceptions to this: when returned by true_fn and/or false_fn , they are implicitly unpacked to single values. Note: It is illegal to "directly" use tensors created inside a cond branch
outside it, e.g. by storing a reference to a branch tensor in the python
state. If you need to use a tensor created in a branch function you should
return it as an output of the branch function and use the output from tf.cond instead. Args pred A scalar determining whether to return the result of true_fn or false_fn . true_fn The callable to be performed if pred is true. false_fn The callable to be performed if pred is false. name Optional name prefix for the returned tensors. Returns Tensors returned by the call to either true_fn or false_fn . If the
callables return a singleton list, the element is extracted from the list. Raises TypeError if true_fn or false_fn is not callable. ValueError if true_fn and false_fn do not return the same number of
tensors, or return tensors of different types. Example: x = tf . constant ( 2 ) y = tf . constant ( 5 ) def f1 (): return tf . multiply ( x , 7 ) def f2 (): return tf . add ( y , 3 ) r = tf . cond ( tf . less ( x , y ), f1 , f2 ) # r is set to f1(). # Operations in f2 (e.g., tf.add) are not executed. r . numpy () 14


Page: https://www.tensorflow.org/api_docs/python/tf/constant
View source on GitHub Creates a constant tensor from a tensor-like object. tf . constant ( value , dtype = None , shape = None , name = 'Const' ) -> Union [ tf . Operation , ops . _EagerTensorBase ] Used in the notebooks Used in the guide Used in the tutorials Better performance with tf.function TF-NumPy Type Promotion Introduction to Tensors Introduction to graphs and tf.function Migrate `tf.feature_column`s to Keras preprocessing layers DeepDream Load text Playing CartPole with the Actor-Critic method Neural style transfer Custom training loop with Keras and MultiWorkerMirroredStrategy Note: All eager tf.Tensor values are immutable (in contrast to tf.Variable ). There is nothing especially constant about the value
returned from tf.constant . This function is not fundamentally different from tf.convert_to_tensor . The name tf.constant comes from the value being
embedded in a Const node in the tf.Graph . tf.constant is useful
for asserting that the value can be embedded that way. If the argument dtype is not specified, then the type is inferred from
the type of value . # Constant 1-D Tensor from a python list. tf . constant ([ 1 , 2 , 3 , 4 , 5 , 6 ]) < tf . Tensor : shape = ( 6 ,), dtype = int32 , numpy = array ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = int32 ) > # Or a numpy array a = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) tf . constant ( a ) < tf . Tensor : shape = ( 2 , 3 ), dtype = int64 , numpy = array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) > If dtype is specified, the resulting tensor values are cast to the requested dtype . tf . constant ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = tf . float64 ) < tf . Tensor : shape = ( 6 ,), dtype = float64 , numpy = array ([ 1. , 2. , 3. , 4. , 5. , 6. ]) > If shape is set, the value is reshaped to match. Scalars are expanded to
fill the shape : tf . constant ( 0 , shape = ( 2 , 3 )) < tf . Tensor : shape = ( 2 , 3 ), dtype = int32 , numpy = array ([[ 0 , 0 , 0 ], [ 0 , 0 , 0 ]], dtype = int32 ) > tf . constant ([ 1 , 2 , 3 , 4 , 5 , 6 ], shape = [ 2 , 3 ]) < tf . Tensor : shape = ( 2 , 3 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], dtype = int32 ) > tf.constant has no effect if an eager Tensor is passed as the value , it
even transmits gradients: v = tf . Variable ([ 0.0 ]) with tf . GradientTape () as g : loss = tf . constant ( v + v ) g . gradient ( loss , v ) . numpy () array ([ 2. ], dtype = float32 ) But, since tf.constant embeds the value in the tf.Graph this fails for
symbolic tensors: with tf . compat . v1 . Graph () . as_default (): i = tf . compat . v1 . placeholder ( shape = [ None , None ], dtype = tf . float32 ) t = tf . constant ( i ) Traceback ( most recent call last ): TypeError : ... tf.constant will create tensors on the current device. Inputs which are
already tensors maintain their placements unchanged. Related Ops: tf.convert_to_tensor is similar but: It has no shape argument. Symbolic tensors are allowed to pass through. with tf . compat . v1 . Graph () . as_default (): i = tf . compat . v1 . placeholder ( shape = [ None , None ], dtype = tf . float32 ) t = tf . convert_to_tensor ( i ) tf.fill : differs in a few ways: tf.constant supports arbitrary constants, not just uniform scalar
Tensors like tf.fill . tf.fill creates an Op in the graph that is expanded at runtime, so it
can efficiently represent large tensors. Since tf.fill does not embed the value, it can produce dynamically
sized outputs. Args value A constant value (or list) of output type dtype . dtype The type of the elements of the resulting tensor. shape Optional dimensions of resulting tensor. name Optional name for the tensor. Returns A Constant Tensor. Raises TypeError if shape is incorrectly specified or unsupported. ValueError if called on a symbolic tensor.


Page: https://www.tensorflow.org/api_docs/python/tf/control_dependencies
View source on GitHub Wrapper for Graph.control_dependencies() using the default graph. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.control_dependencies tf . control_dependencies ( control_inputs ) -> Graph . _ControlDependenciesController Used in the notebooks Used in the tutorials Bayesian Modeling with Joint Distribution TensorFlow Probability Case Study: Covariance Estimation See tf.Graph.control_dependencies for more details. In TensorFlow 2 with eager and/or Autograph, you should not need this method
most of the times, as ops execute in the expected order thanks to automatic
control dependencies. Only use it to manually control ordering, for example as
a workaround to known issues such as tf.function with tf.debugging.assert* and tf.py_function .
For example: @tf . function ( input_signature = [ tf . TensorSpec ([ None , None ], tf . float32 ), tf . TensorSpec ([ None , None ], tf . float32 )]) def my_assert_func_1 ( x , bias ): # `tf.function` attempts to execute `tf.math.add` in parallel to # `assert_equal`. As a result an error can get raised from `tf.math.add` # without triggering the assertion error. tf . assert_equal ( tf . shape ( x )[ 1 ], tf . shape ( bias )[ 1 ], message = 'bad shape' ) return x + bias # Error raised in either `add` or `assert` my_assert_func_1 ( tf . ones (( 2 , 5 )), tf . ones (( 2 , 7 ))) Traceback ( most recent call last ): InvalidArgumentError : ... @tf . function ( input_signature = [ tf . TensorSpec ([ None , None ], tf . float32 ), tf . TensorSpec ([ None , None ], tf . float32 )]) def my_assert_func_2 ( x , bias ): with tf . control_dependencies ( [ tf . assert_equal ( tf . shape ( x )[ 1 ], tf . shape ( bias )[ 1 ], message = 'bad shape' )]): return x + bias # Error raised in `assert` my_assert_func_2 ( tf . ones (( 2 , 5 )), tf . ones (( 2 , 7 ))) Traceback ( most recent call last ): InvalidArgumentError : ... When eager execution is enabled, any callable object in the control_inputs list will be called. Args control_inputs A list of Operation or Tensor objects which must be
executed or computed before running the operations defined in the context.
Can also be None to clear the control dependencies. If eager execution
is enabled, any callable object in the control_inputs list will be
called. Returns A context manager that specifies control dependencies for all
operations constructed within the context.


Page: https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies
View source on GitHub A TensorFlow computation, represented as a dataflow graph. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.Graph tf . Graph () -> None Used in the notebooks Used in the guide Used in the tutorials Migrating model checkpoints Validating correctness & numerical equivalence Migrate the SavedModel workflow Debug a TensorFlow 2 migrated training pipeline Migrating your TFLite code to TF2 Generating Images with Little Data Using S3GAN Exploring the TF-Hub CORD-19 Swivel Embeddings Wiki40B Language Models Migrating tf.summary usage to TF 2.x Graphs are used by tf.function s to represent the function's computations.
Each graph contains a set of tf.Operation objects, which represent units of
computation; and tf.Tensor objects, which represent the units of data that
flow between operations. Using graphs directly (deprecated) A tf.Graph can be constructed and used directly without a tf.function , as
was required in TensorFlow 1, but this is deprecated and it is recommended to
use a tf.function instead. If a graph is directly used, other deprecated
TensorFlow 1 classes are also required to execute the graph, such as a tf.compat.v1.Session . A default graph can be registered with the tf.Graph.as_default context
manager. Then, operations will be added to the graph instead of being executed
eagerly. For example: g = tf . Graph () with g . as_default (): # Define operations and tensors in `g`. c = tf . constant ( 30.0 ) assert c . graph is g tf.compat.v1.get_default_graph() can be used to obtain the default graph. Important note: This class is not thread-safe for graph construction. All
operations should be created from a single thread, or external
synchronization must be provided. Unless otherwise specified, all methods
are not thread-safe. A Graph instance supports an arbitrary number of "collections"
that are identified by name. For convenience when building a large
graph, collections can store groups of related objects: for
example, the tf.Variable uses a collection (named tf.GraphKeys.GLOBAL_VARIABLES ) for
all variables that are created during the construction of a graph. The caller
may define additional collections by specifying a new name. Attributes building_function Returns True iff this graph represents a function. collections Returns the names of the collections known to this graph. finalized True if this graph has been finalized. graph_def_versions The GraphDef version information of this graph. For details on the meaning of each version, see GraphDef . operations seed The graph-level random seed of this graph. version Methods Dismantle Dismantle () (self: handle) -> None add_to_collection View source add_to_collection ( name , value ) -> None Stores value in the collection with the given name . Note that collections are not sets, so it is possible to add a value to
a collection several times. Args name The key for the collection. The GraphKeys class contains many
standard names for collections. value The value to add to the collection. add_to_collections View source add_to_collections ( names , value ) -> None Stores value in the collections given by names . Note that collections are not sets, so it is possible to add a value to
a collection several times. This function makes sure that duplicates in names are ignored, but it will not check for pre-existing membership of value in any of the collections in names . names can be any iterable, but if names is a string, it is treated as a
single collection name. Args names The keys for the collections to add to. The GraphKeys class
contains many standard names for collections. value The value to add to the collections. as_default View source as_default () -> ContextManager [ 'Graph' ] Returns a context manager that makes this Graph the default graph. This method should be used if you want to create multiple graphs
in the same process. For convenience, a global default graph is
provided, and all ops will be added to this graph if you do not
create a new graph explicitly. Use this method with the with keyword to specify that ops created within
the scope of a block should be added to this graph. In this case, once
the scope of the with is exited, the previous default graph is set again
as default. There is a stack, so it's ok to have multiple nested levels
of as_default calls. The default graph is a property of the current thread. If you
create a new thread, and wish to use the default graph in that
thread, you must explicitly add a with g.as_default(): in that
thread's function. The following code examples are equivalent: # 1. Using Graph.as_default(): g = tf . Graph () with g . as_default (): c = tf . constant ( 5.0 ) assert c . graph is g # 2. Constructing and making default: with tf . Graph () . as_default () as g : c = tf . constant ( 5.0 ) assert c . graph is g If eager execution is enabled ops created under this context manager will be
added to the graph instead of executed eagerly. Returns A context manager for using this graph as the default graph. as_graph_def View source as_graph_def ( from_version = None , add_shapes = False , use_pybind11_proto = False ) -> tf . compat . v1 . GraphDef Returns a serialized GraphDef representation of this graph. The serialized GraphDef can be imported into another Graph (using tf.import_graph_def ) or used with the C++ Session API . This method is thread-safe. Args from_version Optional.  If this is set, returns a GraphDef containing
only the nodes that were added to this graph since its version property had the given value. add_shapes If true, adds an "_output_shapes" list attr to each node with
the inferred shapes of each of its outputs. use_pybind11_proto If true, If true, uses the c++ pybind11_proto api to
get the GraphDef proto directly from c++, instead of through a TF
buffer. See https://github.com/pybind/pybind11_protobuf for reference. Returns A GraphDef protocol buffer. Raises ValueError If the graph_def would be too large. as_graph_element View source as_graph_element ( obj , allow_tensor = True , allow_operation = True ) -> Union [ tensor_lib . Tensor , 'Operation' ] Returns the object referred to by obj , as an Operation or Tensor . This function validates that obj represents an element of this
graph, and gives an informative error message if it is not. This function is the canonical way to get/validate an object of
one of the allowed types from an external argument reference in the
Session API. This method may be called concurrently from multiple threads. Args obj A Tensor , an Operation , or the name of a tensor or operation. Can
also be any object with an _as_graph_element() method that returns a
value of one of these types. Note: _as_graph_element will be called
inside the graph's lock and so may not modify the graph. allow_tensor If true, obj may refer to a Tensor . allow_operation If true, obj may refer to an Operation . Returns The Tensor or Operation in the Graph corresponding to obj . Raises TypeError If obj is not a type we support attempting to convert
to types. ValueError If obj is of an appropriate type but invalid. For
example, an invalid string. KeyError If obj is not an object in the graph. clear_collection View source clear_collection ( name ) -> None Clears all values in a collection. Args name The key for the collection. The GraphKeys class contains many
standard names for collections. colocate_with View source @tf_contextlib . contextmanager colocate_with ( op , ignore_existing = False ) -> Iterator [ None ] Returns a context manager that specifies an op to colocate with. Note: this function is not for public use, only for internal libraries. For example: a = tf . Variable ([ 1.0 ]) with g . colocate_with ( a ): b = tf . constant ( 1.0 ) c = tf . add ( a , b ) b and c will always be colocated with a , no matter where a is eventually placed. Note: Using a colocation scope resets any existing device constraints. If op is None then ignore_existing must be True and the new
scope resets all colocation and device constraints. Args op The op to colocate all created ops with, or None . ignore_existing If true, only applies colocation of this op within the
context, rather than applying all colocation properties on the stack.
If op is None , this value must be True . Raises ValueError if op is None but ignore_existing is False. Yields A context manager that specifies the op with which to colocate
newly created ops. container View source @tf_contextlib . contextmanager container ( container_name ) -> Iterator [ str ] Returns a context manager that specifies the resource container to use. Stateful operations, such as variables and queues, can maintain their
states on devices so that they can be shared by multiple processes.
A resource container is a string name under which these stateful
operations are tracked. These resources can be released or cleared
with tf.Session.reset() . For example: with g . container ( 'experiment0' ): # All stateful Operations constructed in this context will be placed # in resource container "experiment0". v1 = tf . Variable ([ 1.0 ]) v2 = tf . Variable ([ 2.0 ]) with g . container ( "experiment1" ): # All stateful Operations constructed in this context will be # placed in resource container "experiment1". v3 = tf . Variable ([ 3.0 ]) q1 = tf . queue . FIFOQueue ( 10 , tf . float32 ) # All stateful Operations constructed in this context will be # be created in the "experiment0". v4 = tf . Variable ([ 4.0 ]) q1 = tf . queue . FIFOQueue ( 20 , tf . float32 ) with g . container ( "" ): # All stateful Operations constructed in this context will be # be placed in the default resource container. v5 = tf . Variable ([ 5.0 ]) q3 = tf . queue . FIFOQueue ( 30 , tf . float32 ) # Resets container "experiment0", after which the state of v1, v2, v4, q1 # will become undefined (such as uninitialized). tf . Session . reset ( target , [ "experiment0" ]) Args container_name container name string. Returns A context manager for defining resource containers for stateful ops,
yields the container name. control_dependencies View source control_dependencies ( control_inputs ) -> _ControlDependenciesController Returns a context manager that specifies control dependencies. Use with the with keyword to specify that all operations constructed
within the context should have control dependencies on control_inputs . For example: with g . control_dependencies ([ a , b , c ]): # `d` and `e` will only run after `a`, `b`, and `c` have executed. d = ... e = ... Multiple calls to control_dependencies() can be nested, and in
that case a new Operation will have control dependencies on the union
of control_inputs from all active contexts. with g . control_dependencies ([ a , b ]): # Ops constructed here run after `a` and `b`. with g . control_dependencies ([ c , d ]): # Ops constructed here run after `a`, `b`, `c`, and `d`. You can pass None to clear the control dependencies: with g . control_dependencies ([ a , b ]): # Ops constructed here run after `a` and `b`. with g . control_dependencies ( None ): # Ops constructed here run normally, not waiting for either `a` or `b`. with g . control_dependencies ([ c , d ]): # Ops constructed here run after `c` and `d`, also not waiting # for either `a` or `b`. Note: The control dependencies context applies only to ops that
are constructed within the context. Merely using an op or tensor
in the context does not add a control dependency. The following
example illustrates this point: # WRONG def my_func ( pred , tensor ): t = tf . matmul ( tensor , tensor ) with tf . control_dependencies ([ pred ]): # The matmul op is created outside the context, so no control # dependency will be added. return t # RIGHT def my_func ( pred , tensor ): with tf . control_dependencies ([ pred ]): # The matmul op is created in the context, so a control dependency # will be added. return tf . matmul ( tensor , tensor ) Also note that though execution of ops created under this scope will trigger
execution of the dependencies, the ops created under this scope might still
be pruned from a normal tensorflow graph. For example, in the following
snippet of code the dependencies are never executed: loss = model . loss () with tf . control_dependencies ( dependencies ): loss = loss + tf . constant ( 1 ) # note: dependencies ignored in the # backward pass return tf . gradients ( loss , model . variables ) This is because evaluating the gradient graph does not require evaluating
the constant(1) op created in the forward pass. Args control_inputs A list of Operation or Tensor objects which must be
executed or computed before running the operations defined in the
context.  Can also be None to clear the control dependencies. Returns A context manager that specifies control dependencies for all
operations constructed within the context. Raises TypeError If control_inputs is not a list of Operation or Tensor objects. create_op View source create_op ( op_type , inputs , dtypes = None , input_types = None , name = None , attrs = None , op_def = None , compute_shapes = True , compute_device = True ) -> 'Operation' Creates an Operation in this graph. (deprecated arguments) Deprecated: SOME ARGUMENTS ARE DEPRECATED: (compute_shapes) . They will be removed in a future version.
Instructions for updating:
Shapes are always computed; don't use the compute_shapes as it has no effect. This is a low-level interface for creating an Operation . Most
programs will not call this method directly, and instead use the
Python op constructors, such as tf.constant() , which add ops to
the default graph. Args op_type The Operation type to create. This corresponds to the OpDef.name field for the proto that defines the operation. inputs A list of Tensor objects that will be inputs to the Operation . dtypes (Optional) A list of DType objects that will be the types of the
tensors that the operation produces. input_types (Optional.) A list of DType s that will be the types of the
tensors that the operation consumes. By default, uses the base DType of each input in inputs . Operations that expect reference-typed inputs
must specify input_types explicitly. name (Optional.) A string name for the operation. If not specified, a
name is generated based on op_type . attrs (Optional.) A dictionary where the key is the attribute name (a
string) and the value is the respective attr attribute of the NodeDef proto that will represent the operation (an AttrValue proto). op_def (Optional.) The OpDef proto that describes the op_type that
the operation will have. compute_shapes (Optional.) Deprecated. Has no effect (shapes are always
computed). compute_device (Optional.) If True, device functions will be executed to
compute the device property of the Operation. Raises TypeError if any of the inputs is not a Tensor . ValueError if colocation conflicts with existing device assignment. Returns An Operation object. device View source @tf_contextlib . contextmanager device ( device_name_or_function ) -> Iterator [ None ] Returns a context manager that specifies the default device to use. The device_name_or_function argument may either be a device name
string, a device function, or None: If it is a device name string, all operations constructed in
this context will be assigned to the device with that name, unless
overridden by a nested device() context. If it is a function, it will be treated as a function from
Operation objects to device name strings, and invoked each time
a new Operation is created. The Operation will be assigned to
the device with the returned name. If it is None, all device() invocations from the enclosing context
will be ignored. For information about the valid syntax of device name strings, see
the documentation in DeviceNameUtils . For example: with g . device ( '/device:GPU:0' ): # All operations constructed in this context will be placed # on GPU 0. with g . device ( None ): # All operations constructed in this context will have no # assigned device. # Defines a function from `Operation` to device string. def matmul_on_gpu ( n ): if n . type == "MatMul" : return "/device:GPU:0" else : return "/cpu:0" with g . device ( matmul_on_gpu ): # All operations of type "MatMul" constructed in this context # will be placed on GPU 0; all other operations will be placed # on CPU 0. Note: The device scope may be overridden by op wrappers or
other library code. For example, a variable assignment op v.assign() must be colocated with the tf.Variable v , and
incompatible device scopes will be ignored. Args device_name_or_function The device name or function to use in the
context. Yields A context manager that specifies the default device to use for newly
created ops. Raises RuntimeError If device scopes are not properly nested. finalize View source finalize () -> None Finalizes this graph, making it read-only. After calling g.finalize() , no new operations can be added to g .  This method is used to ensure that no operations are added
to a graph when it is shared between multiple threads, for example
when using a tf.compat.v1.train.QueueRunner . get View source get () -> GraphType get_all_collection_keys View source get_all_collection_keys () -> list [ str ] Returns a list of collections used in this graph. get_collection View source get_collection ( name , scope = None ) -> list [ Any ] Returns a list of values in the collection with the given name . This is different from get_collection_ref() which always returns the
actual collection list if it exists in that it returns a new list each time
it is called. Args name The key for the collection. For example, the GraphKeys class
contains many standard names for collections. scope (Optional.) A string. If supplied, the resulting list is filtered
to include only items whose name attribute matches scope using re.match . Items without a name attribute are never returned if a
scope is supplied. The choice of re.match means that a scope without
special tokens filters by prefix. Returns The list of values in the collection with the given name , or
an empty list if no value has been added to that collection. The
list contains the values in the order under which they were
collected. get_collection_ref View source get_collection_ref ( name ) -> list [ Any ] Returns a list of values in the collection with the given name . If the collection exists, this returns the list itself, which can
be modified in place to change the collection.  If the collection does
not exist, it is created as an empty list and the list is returned. This is different from get_collection() which always returns a copy of
the collection list if it exists and never creates an empty collection. Args name The key for the collection. For example, the GraphKeys class
contains many standard names for collections. Returns The list of values in the collection with the given name , or an empty
list if no value has been added to that collection. get_name_scope View source get_name_scope () -> str Returns the current name scope. For example: with tf . name_scope ( 'scope1' ): with tf . name_scope ( 'scope2' ): print ( tf . compat . v1 . get_default_graph () . get_name_scope ()) would print the string scope1/scope2 . Returns A string representing the current name scope. get_operation_by_name View source get_operation_by_name ( name ) -> 'Operation' Returns the Operation with the given name . This method may be called concurrently from multiple threads. Args name The name of the Operation to return. Returns The Operation with the given name . Raises TypeError If name is not a string. KeyError If name does not correspond to an operation in this graph. get_operations get_operations () (self: handle) -> list get_tensor_by_name View source get_tensor_by_name ( name ) -> tf . Tensor Returns the Tensor with the given name . This method may be called concurrently from multiple threads. Args name The name of the Tensor to return. Returns The Tensor with the given name . Raises TypeError If name is not a string. KeyError If name does not correspond to a tensor in this graph. gradient_override_map View source @tf_contextlib . contextmanager gradient_override_map ( op_type_map ) -> Iterator [ None ] EXPERIMENTAL: A context manager for overriding gradient functions. This context manager can be used to override the gradient function
that will be used for ops within the scope of the context. For example: @tf . RegisterGradient ( "CustomSquare" ) def _custom_square_grad ( op , grad ): # ... with tf . Graph () . as_default () as g : c = tf . constant ( 5.0 ) s_1 = tf . square ( c ) # Uses the default gradient for tf.square. with g . gradient_override_map ({ "Square" : "CustomSquare" }): s_2 = tf . square ( s_2 ) # Uses _custom_square_grad to compute the # gradient of s_2. Args op_type_map A dictionary mapping op type strings to alternative op type
strings. Returns A context manager that sets the alternative op type to be used for one
or more ops created in that context. Raises TypeError If op_type_map is not a dictionary mapping strings to
strings. is_feedable View source is_feedable ( tensor ) -> bool Returns True if and only if tensor is feedable. is_fetchable View source is_fetchable ( tensor_or_op ) -> bool Returns True if and only if tensor_or_op is fetchable. name_scope View source @tf_contextlib . contextmanager name_scope ( name ) -> Iterator [ str ] Returns a context manager that creates hierarchical names for operations. A graph maintains a stack of name scopes. A with name_scope(...): statement pushes a new name onto the stack for the lifetime of the context. The name argument will be interpreted as follows: A string (not ending with '/') will create a new name scope, in which name is appended to the prefix of all operations created in the
context. If name has been used before, it will be made unique by
calling self.unique_name(name) . A scope previously captured from a with g.name_scope(...) as
scope: statement will be treated as an "absolute" name scope, which
makes it possible to re-enter existing scopes. A value of None or the empty string will reset the current name scope
to the top-level (empty) name scope. For example: with tf . Graph () . as_default () as g : c = tf . constant ( 5.0 , name = "c" ) assert c . op . name == "c" c_1 = tf . constant ( 6.0 , name = "c" ) assert c_1 . op . name == "c_1" # Creates a scope called "nested" with g . name_scope ( "nested" ) as scope : nested_c = tf . constant ( 10.0 , name = "c" ) assert nested_c . op . name == "nested/c" # Creates a nested scope called "inner". with g . name_scope ( "inner" ): nested_inner_c = tf . constant ( 20.0 , name = "c" ) assert nested_inner_c . op . name == "nested/inner/c" # Create a nested scope called "inner_1". with g . name_scope ( "inner" ): nested_inner_1_c = tf . constant ( 30.0 , name = "c" ) assert nested_inner_1_c . op . name == "nested/inner_1/c" # Treats `scope` as an absolute name scope, and # switches to the "nested/" scope. with g . name_scope ( scope ): nested_d = tf . constant ( 40.0 , name = "d" ) assert nested_d . op . name == "nested/d" with g . name_scope ( "" ): e = tf . constant ( 50.0 , name = "e" ) assert e . op . name == "e" The name of the scope itself can be captured by with
g.name_scope(...) as scope: , which stores the name of the scope
in the variable scope . This value can be used to name an
operation that represents the overall result of executing the ops
in a scope. For example: inputs = tf . constant ( ... ) with g . name_scope ( 'my_layer' ) as scope : weights = tf . Variable ( ... , name = "weights" ) biases = tf . Variable ( ... , name = "biases" ) affine = tf . matmul ( inputs , weights ) + biases output = tf . nn . relu ( affine , name = scope ) Note: This constructor validates the given name . Valid scope
names match one of the following regular expressions: [ A - Za - z0 - 9. ][ A - Za - z0 - 9 _ . \ -/ ] * ( for scopes at the root ) [ A - Za - z0 - 9 _ . \ -/ ] * ( for other scopes ) Args name A name for the scope. Returns A context manager that installs name as a new name scope. Raises ValueError If name is not a valid scope name, according to the rules
above. new_operations new_operations () (self: handle) -> List[TF_Operation] num_operations num_operations () (self: handle) -> int op_def_for_type View source op_def_for_type ( type ) -> op_def_pb2 . OpDef Returns the OpDef proto for type . type is a string. prevent_feeding View source prevent_feeding ( tensor ) -> None Marks the given tensor as unfeedable in this graph. prevent_fetching View source prevent_fetching ( op ) -> None Marks the given op as unfetchable in this graph. switch_to_thread_local View source switch_to_thread_local () -> None Make device, colocation and dependencies stacks thread-local. Device, colocation and dependencies stacks are not thread-local be default.
If multiple threads access them, then the state is shared.  This means that
one thread may affect the behavior of another thread. After this method is called, the stacks become thread-local.  If multiple
threads access them, then the state is not shared.  Each thread uses its own
value; a thread doesn't affect other threads by mutating such a stack. The initial value for every thread's stack is set to the current value
of the stack when switch_to_thread_local() was first called. unique_name View source unique_name ( name , mark_as_used = True ) -> str Return a unique operation name for name . Note: You rarely need to call unique_name() directly.  Most of
the time you just need to create with g.name_scope() blocks to
generate structured names. unique_name is used to generate structured names, separated by "/" , to help identify operations when debugging a graph.
Operation names are displayed in error messages reported by the
TensorFlow runtime, and in various visualization tools such as
TensorBoard. If mark_as_used is set to True , which is the default, a new
unique name is created and marked as in use. If it's set to False ,
the unique name is returned without actually being marked as used.
This is useful when the caller simply wants to know what the name
to be created will be. Args name The name for an operation. mark_as_used Whether to mark this name as being used. Returns A string to be passed to create_op() that will be used
to name the operation being created. __enter__ View source __enter__ () -> GraphType __exit__ View source __exit__ ( * args ) -> None


Page: https://www.tensorflow.org/api_docs/python/tf/conv
Computes a N-D convolution given (N+1+batch_dims)-D input and (N+2)-D filter tensors. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.conv tf . conv ( input : Annotated [ Any , TV_Conv_T ], filter : Annotated [ Any , TV_Conv_T ], strides , padding : str , explicit_paddings = [], data_format : str = 'CHANNELS_LAST' , dilations = [], batch_dims : int = 1 , groups : int = 1 , name = None ) -> Annotated [ Any , TV_Conv_T ] General function for computing a N-D convolution. It is required that 1 <= N <= 3 . Args input A Tensor . Must be one of the following types: half , bfloat16 , float32 , float64 , int32 .
Tensor of type T and shape batch_shape + spatial_shape + [in_channels] in the
case that channels_last_format = true or shape batch_shape + [in_channels] + spatial_shape if channels_last_format = false .
spatial_shape is N-dimensional with N=2 or N=3 .
Also note that batch_shape is dictated by the parameter batch_dims and defaults to 1. filter A Tensor . Must have the same type as input .
An (N+2)-D Tensor with the same type as input and shape spatial_filter_shape + [in_channels, out_channels] , where spatial_filter_shape
is N-dimensional with N=2 or N=3 . strides A list of ints .
1-D tensor of length N+2 . The stride of the sliding window for each
dimension of input . Must have strides[0] = strides[N+1] = 1 . padding A string from: "SAME", "VALID", "EXPLICIT" .
The type of padding algorithm to use. explicit_paddings An optional list of ints . Defaults to [] .
If padding is "EXPLICIT" , the list of explicit padding amounts. For the ith
dimension, the amount of padding inserted before and after the dimension is explicit_paddings[2 * i] and explicit_paddings[2 * i + 1] , respectively. If padding is not "EXPLICIT" , explicit_paddings must be empty. data_format An optional string from: "CHANNELS_FIRST", "CHANNELS_LAST" . Defaults to "CHANNELS_LAST" .
Used to set the data format. By default CHANNELS_FIRST , uses NHWC (2D) / NDHWC (3D) or if CHANNELS_LAST , uses NCHW (2D) / NCDHW (3D) . dilations An optional list of ints . Defaults to [] .
1-D tensor of length N+2 . The dilation factor for each dimension of input . If set to k > 1 , there will be k-1 skipped cells between each
filter element on that dimension. The dimension order is determined by the
value of channels_last_format , see above for details. Dilations in the batch
and depth dimensions must be 1. batch_dims An optional int . Defaults to 1 .
A positive integer specifying the number of batch dimensions for the input
tensor. Should be less than the rank of the input tensor. groups An optional int . Defaults to 1 .
A positive integer specifying the number of groups in which the input is split
along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups
results along the channel axis. Input channels and filters must both be
divisible by groups. name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/conv2d_backprop_filter_v2
Computes the gradients of convolution with respect to the filter. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.conv2d_backprop_filter_v2 tf . conv2d_backprop_filter_v2 ( input : Annotated [ Any , TV_Conv2DBackpropFilterV2_T ], filter : Annotated [ Any , TV_Conv2DBackpropFilterV2_T ], out_backprop : Annotated [ Any , TV_Conv2DBackpropFilterV2_T ], strides , padding : str , use_cudnn_on_gpu : bool = True , explicit_paddings = [], data_format : str = 'NHWC' , dilations = [ 1 , 1 , 1 , 1 ], name = None ) -> Annotated [ Any , TV_Conv2DBackpropFilterV2_T ] Args input A Tensor . Must be one of the following types: half , bfloat16 , float32 , float64 .
4-D with shape [batch, in_height, in_width, in_channels] . filter A Tensor . Must have the same type as input .
4-D with shape [filter_height, filter_width, in_channels, out_channels] .
Only shape of tensor is used. out_backprop A Tensor . Must have the same type as input .
4-D with shape [batch, out_height, out_width, out_channels] .
Gradients w.r.t. the output of the convolution. strides A list of ints .
The stride of the sliding window for each dimension of the input
of the convolution. Must be in the same order as the dimension specified with
format. padding A string from: "SAME", "VALID", "EXPLICIT" .
The type of padding algorithm to use. use_cudnn_on_gpu An optional bool . Defaults to True . explicit_paddings An optional list of ints . Defaults to [] .
If padding is "EXPLICIT" , the list of explicit padding amounts. For the ith
dimension, the amount of padding inserted before and after the dimension is explicit_paddings[2 * i] and explicit_paddings[2 * i + 1] , respectively. If padding is not "EXPLICIT" , explicit_paddings must be empty. data_format An optional string from: "NHWC", "NCHW" . Defaults to "NHWC" .
Specify the data format of the input and output data. With the
default format "NHWC", the data is stored in the order of:
    [batch, in_height, in_width, in_channels].
Alternatively, the format could be "NCHW", the data storage order of:
    [batch, in_channels, in_height, in_width]. dilations An optional list of ints . Defaults to [1, 1, 1, 1] .
1-D tensor of length 4.  The dilation factor for each dimension of input . If set to k > 1, there will be k-1 skipped cells between each filter
element on that dimension. The dimension order is determined by the value of data_format , see above for details. Dilations in the batch and depth
dimensions must be 1. name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/conv2d_backprop_input_v2
Computes the gradients of convolution with respect to the input. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.conv2d_backprop_input_v2 tf . conv2d_backprop_input_v2 ( input : Annotated [ Any , TV_Conv2DBackpropInputV2_T ], filter : Annotated [ Any , TV_Conv2DBackpropInputV2_T ], out_backprop : Annotated [ Any , TV_Conv2DBackpropInputV2_T ], strides , padding : str , use_cudnn_on_gpu : bool = True , explicit_paddings = [], data_format : str = 'NHWC' , dilations = [ 1 , 1 , 1 , 1 ], name = None ) -> Annotated [ Any , TV_Conv2DBackpropInputV2_T ] Args input A Tensor . Must be one of the following types: half , bfloat16 , float32 , float64 , int32 .
4-D with shape [batch, in_height, in_width, in_channels] .
Only shape of tensor is used. filter A Tensor . Must have the same type as input . 4-D with shape [filter_height, filter_width, in_channels, out_channels] . out_backprop A Tensor . Must have the same type as input .
4-D with shape [batch, out_height, out_width, out_channels] .
Gradients w.r.t. the output of the convolution. strides A list of ints .
The stride of the sliding window for each dimension of the input
of the convolution. Must be in the same order as the dimension specified with
format. padding A string from: "SAME", "VALID", "EXPLICIT" .
The type of padding algorithm to use. use_cudnn_on_gpu An optional bool . Defaults to True . explicit_paddings An optional list of ints . Defaults to [] .
If padding is "EXPLICIT" , the list of explicit padding amounts. For the ith
dimension, the amount of padding inserted before and after the dimension is explicit_paddings[2 * i] and explicit_paddings[2 * i + 1] , respectively. If padding is not "EXPLICIT" , explicit_paddings must be empty. data_format An optional string from: "NHWC", "NCHW" . Defaults to "NHWC" .
Specify the data format of the input and output data. With the
default format "NHWC", the data is stored in the order of:
    [batch, in_height, in_width, in_channels].
Alternatively, the format could be "NCHW", the data storage order of:
    [batch, in_channels, in_height, in_width]. dilations An optional list of ints . Defaults to [1, 1, 1, 1] .
1-D tensor of length 4.  The dilation factor for each dimension of input . If set to k > 1, there will be k-1 skipped cells between each filter
element on that dimension. The dimension order is determined by the value of data_format , see above for details. Dilations in the batch and depth
dimensions must be 1. name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor
View source on GitHub Converts the given value to a Tensor . tf . convert_to_tensor ( value , dtype = None , dtype_hint = None , name = None ) -> tf . Tensor Used in the notebooks Used in the guide Used in the tutorials Logistic regression for binary classification with Core APIs Extension types Matrix approximation with Core APIs TensorFlow basics Distributed training with Core APIs and DTensor DeepDream Custom training: walkthrough Distributed training with DTensors Load a pandas DataFrame Scalable model compression This function converts Python objects of various types to Tensor objects. It accepts Tensor objects, numpy arrays, Python lists,
and Python scalars. For example: import numpy as np def my_func ( arg ): arg = tf . convert_to_tensor ( arg , dtype = tf . float32 ) return arg # The following calls are equivalent. value_1 = my_func ( tf . constant ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]])) print ( value_1 ) tf . Tensor ( [[ 1. 2. ] [ 3. 4. ]], shape = ( 2 , 2 ), dtype = float32 ) value_2 = my_func ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]]) print ( value_2 ) tf . Tensor ( [[ 1. 2. ] [ 3. 4. ]], shape = ( 2 , 2 ), dtype = float32 ) value_3 = my_func ( np . array ([[ 1.0 , 2.0 ], [ 3.0 , 4.0 ]], dtype = np . float32 )) print ( value_3 ) tf . Tensor ( [[ 1. 2. ] [ 3. 4. ]], shape = ( 2 , 2 ), dtype = float32 ) This function can be useful when composing a new operation in Python
(such as my_func in the example above). All standard Python op
constructors apply this function to each of their Tensor-valued
inputs, which allows those ops to accept numpy arrays, Python lists,
and scalars in addition to Tensor objects. Note: This function diverges from default Numpy behavior for float and string types when None is present in a Python list or scalar. Rather
  than silently converting None values, an error will be thrown. Args value An object whose type has a registered Tensor conversion function. dtype Optional element type for the returned tensor. If missing, the type
is inferred from the type of value . dtype_hint Optional element type for the returned tensor, used when dtype
is None. In some cases, a caller may not have a dtype in mind when
converting to a tensor, so dtype_hint can be used as a soft preference. If
the conversion to dtype_hint is not possible, this argument has no
effect. name Optional name to use if a new Tensor is created. Returns A Tensor based on value . Raises TypeError If no conversion function is registered for value to dtype . RuntimeError If a registered conversion function returns an invalid value. ValueError If the value is a tensor not of given dtype in graph mode.


Page: https://www.tensorflow.org/api_docs/python/tf/math/cos
Computes cos of x element-wise. View aliases Main aliases tf.cos Compat aliases for migration See Migration guide for
more details. tf.compat.v1.cos tf . math . cos ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the tutorials TFP Release Notes notebook (0.12.1) Given an input tensor, this function computes cosine of every
  element in the tensor. Input range is (-inf, inf) and
  output range is [-1,1] . If input lies outside the boundary, nan is returned. x = tf . constant ([ - float ( "inf" ), - 9 , - 0.5 , 1 , 1.2 , 200 , 10000 , float ( "inf" )]) tf . math . cos ( x ) == > [ nan - 0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 - 0.95215535 nan ] Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/cosh
Computes hyperbolic cosine of x element-wise. View aliases Main aliases tf.cosh Compat aliases for migration See Migration guide for
more details. tf.compat.v1.cosh tf . math . cosh ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the guide TFLite Authoring Tool TensorFlow Lite Model Analyzer Given an input tensor, this function computes hyperbolic cosine of every
  element in the tensor. Input range is [-inf, inf] and output range
  is [1, inf] . x = tf . constant ([ - float ( "inf" ), - 9 , - 0.5 , 1 , 1.2 , 2 , 10 , float ( "inf" )]) tf . math . cosh ( x ) == > [ inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf ] Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/cumsum
View source on GitHub Compute the cumulative sum of the tensor x along axis . View aliases Main aliases tf.cumsum Compat aliases for migration See Migration guide for
more details. tf.compat.v1.cumsum , tf.compat.v1.math.cumsum tf . math . cumsum ( x , axis = 0 , exclusive = False , reverse = False , name = None ) Used in the notebooks Used in the guide Matrix approximation with Core APIs By default, this op performs an inclusive cumsum, which means that the first
element of the input is identical to the first element of the output:
For example: # tf.cumsum([a, b, c])   # [a, a + b, a + b + c] x = tf . constant ([ 2 , 4 , 6 , 8 ]) tf . cumsum ( x ) < tf . Tensor : shape = ( 4 ,), dtype = int32 , numpy = array ([ 2 , 6 , 12 , 20 ], dtype = int32 ) > # using varying `axis` values y = tf . constant ([[ 2 , 4 , 6 , 8 ], [ 1 , 3 , 5 , 7 ]]) tf . cumsum ( y , axis = 0 ) < tf . Tensor : shape = ( 2 , 4 ), dtype = int32 , numpy = array ([[ 2 , 4 , 6 , 8 ], [ 3 , 7 , 11 , 15 ]], dtype = int32 ) > tf . cumsum ( y , axis = 1 ) < tf . Tensor : shape = ( 2 , 4 ), dtype = int32 , numpy = array ([[ 2 , 6 , 12 , 20 ], [ 1 , 4 , 9 , 16 ]], dtype = int32 ) > By setting the exclusive kwarg to True , an exclusive cumsum is performed
instead: # tf.cumsum([a, b, c], exclusive=True)  => [0, a, a + b] x = tf . constant ([ 2 , 4 , 6 , 8 ]) tf . cumsum ( x , exclusive = True ) < tf . Tensor : shape = ( 4 ,), dtype = int32 , numpy = array ([ 0 , 2 , 6 , 12 ], dtype = int32 ) > By setting the reverse kwarg to True , the cumsum is performed in the
opposite direction: # tf.cumsum([a, b, c], reverse=True)  # [a + b + c, b + c, c] x = tf . constant ([ 2 , 4 , 6 , 8 ]) tf . cumsum ( x , reverse = True ) < tf . Tensor : shape = ( 4 ,), dtype = int32 , numpy = array ([ 20 , 18 , 14 , 8 ], dtype = int32 ) > This is more efficient than using separate tf.reverse ops.
The reverse and exclusive kwargs can also be combined: # tf.cumsum([a, b, c], exclusive=True, reverse=True)  # [b + c, c, 0] x = tf . constant ([ 2 , 4 , 6 , 8 ]) tf . cumsum ( x , exclusive = True , reverse = True ) < tf . Tensor : shape = ( 4 ,), dtype = int32 , numpy = array ([ 18 , 14 , 8 , 0 ], dtype = int32 ) > Args x A Tensor . Must be one of the following types: float32 , float64 , int64 , int32 , uint8 , uint16 , int16 , int8 , complex64 , complex128 , qint8 , quint8 , qint32 , half . axis A Tensor of type int32 (default: 0). Must be in the range [-rank(x), rank(x)) . exclusive If True , perform exclusive cumsum. reverse A bool (default: False). name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/custom_gradient
View source on GitHub Decorator to define a function with a custom gradient. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.custom_gradient tf . custom_gradient ( f = None ) Used in the notebooks Used in the guide Advanced automatic differentiation This decorator allows fine grained control over the gradients of a sequence
for operations.  This may be useful for multiple reasons, including providing
a more efficient or numerically stable gradient for a sequence of operations. For example, consider the following function that commonly occurs in the
computation of cross entropy and log likelihoods: def log1pexp ( x ): return tf . math . log ( 1 + tf . exp ( x )) Due to numerical instability, the gradient of this function evaluated at x=100
is NaN.  For example: with tf . GradientTape () as tape : tape . watch ( x ) y = log1pexp ( x ) dy_dx = tape . gradient ( y , x ) # Will be NaN when evaluated. The gradient expression can be analytically simplified to provide numerical
stability: @tf . custom_gradient def log1pexp ( x ): e = tf . exp ( x ) def grad ( upstream ): return upstream * ( 1 - 1 / ( 1 + e )) return tf . math . log ( 1 + e ), grad With this definition, the gradient dy_dx at x = 100 will be correctly
evaluated as 1.0. The variable upstream is defined as the upstream gradient. i.e. the gradient
from all the layers or functions originating from this layer. The above
example has no upstream functions, therefore upstream = dy/dy = 1.0 . Assume that x_i is log1pexp in the forward pass x_1 = x_1(x_0) , x_2 = x_2(x_1) , ..., x_i = x_i(x_i-1) , ..., x_n = x_n(x_n-1) . By
chain rule we know that dx_n/dx_0 = dx_n/dx_n-1 * dx_n-1/dx_n-2 * ... *
dx_i/dx_i-1 * ... * dx_1/dx_0 . In this case the gradient of our current function defined as dx_i/dx_i-1 = (exp(x_i) / (1 + exp(x_i))) = (1 - 1 / (1 + exp(x_i))) . The
upstream gradient upstream would be dx_n/dx_n-1 * dx_n-1/dx_n-2 * ... *
dx_i+1/dx_i . The upstream gradient multiplied by the current gradient is
then passed downstream. In case the function takes multiple variables as input, the grad function must also return  the same number of variables.
We take the function z = x * y as an example. @tf . custom_gradient def bar ( x , y ): def grad ( upstream ): dz_dx = y dz_dy = x return upstream * dz_dx , upstream * dz_dy z = x * y return z , grad x = tf . constant ( 2.0 , dtype = tf . float32 ) y = tf . constant ( 3.0 , dtype = tf . float32 ) with tf . GradientTape ( persistent = True ) as tape : tape . watch ( x ) tape . watch ( y ) z = bar ( x , y ) z < tf . Tensor : shape = (), dtype = float32 , numpy = 6.0 > tape . gradient ( z , x ) < tf . Tensor : shape = (), dtype = float32 , numpy = 3.0 > tape . gradient ( z , y ) < tf . Tensor : shape = (), dtype = float32 , numpy = 2.0 > Nesting custom gradients can lead to unintuitive results. The default
behavior does not correspond to n-th order derivatives. For example @tf . custom_gradient def op ( x ): y = op1 ( x ) @tf . custom_gradient def grad_fn ( dy ): gdy = op2 ( x , y , dy ) def grad_grad_fn ( ddy ): # Not the 2nd order gradient of op w.r.t. x. return op3 ( x , y , dy , ddy ) return gdy , grad_grad_fn return y , grad_fn The function grad_grad_fn will be calculating the first order gradient
of grad_fn with respect to dy , which is used to generate forward-mode
gradient graphs from backward-mode gradient graphs, but is not the same as
the second order gradient of op with respect to x . Instead, wrap nested @tf.custom_gradients in another function: @tf . custom_gradient def op_with_fused_backprop ( x ): y , x_grad = fused_op ( x ) def first_order_gradient ( dy ): @tf . custom_gradient def first_order_custom ( unused_x ): def second_order_and_transpose ( ddy ): return second_order_for_x ( ... ), gradient_wrt_dy ( ... ) return x_grad , second_order_and_transpose return dy * first_order_custom ( x ) return y , first_order_gradient Additional arguments to the inner @tf.custom_gradient -decorated function
control the expected return values of the innermost function. The examples above illustrate how to specify custom gradients for functions
which do not read from variables. The following example uses variables, which
require special handling because they are effectively inputs of the forward
function. weights = tf . Variable ( tf . ones ([ 2 ])) # Trainable variable weights @tf . custom_gradient def linear_poly ( x ): # Creating polynomial poly = weights [ 1 ] * x + weights [ 0 ] def grad_fn ( dpoly , variables ): # dy/dx = weights[1] and we need to left multiply dpoly grad_xs = dpoly * weights [ 1 ] # Scalar gradient grad_vars = [] # To store gradients of passed variables assert variables is not None assert len ( variables ) == 1 assert variables [ 0 ] is weights # Manually computing dy/dweights dy_dw = dpoly * tf . stack ([ x ** 1 , x ** 0 ]) grad_vars . append ( tf . reduce_sum ( tf . reshape ( dy_dw , [ 2 , - 1 ]), axis = 1 ) ) return grad_xs , grad_vars return poly , grad_fn x = tf . constant ([ 1. , 2. , 3. ]) with tf . GradientTape ( persistent = True ) as tape : tape . watch ( x ) poly = linear_poly ( x ) poly # poly = x + 1 < tf . Tensor : shape = ( 3 ,), dtype = float32 , numpy = array ([ 2. , 3. , 4. ], dtype = float32 ) > tape . gradient ( poly , x ) # conventional scalar gradient dy/dx < tf . Tensor : shape = ( 3 ,), dtype = float32 , numpy = array ([ 1. , 1. , 1. ], dtype = float32 ) > tape . gradient ( poly , weights ) < tf . Tensor : shape = ( 2 ,), dtype = float32 , numpy = array ([ 6. , 3. ], dtype = float32 ) > Above example illustrates usage of trainable variable weights .
In the example, the inner grad_fn accepts an extra variables input
parameter and also returns an extra grad_vars output. That extra argument
is passed if the forward function reads any variables. You need to
compute the gradient w.r.t. each of those variables and output it as a list
of grad_vars . Note here that default value of variables is set to None when no variables are used in the forward function. It should be noted tf.GradientTape is still watching the forward pass of a tf.custom_gradient , and will use the ops it watches. As a consequence,
calling tf.function while the tape is still watching leads
to a gradient graph being built. If an op is used in tf.function without
registered gradient, a LookupError will be raised. Users can insert tf.stop_gradient to customize this behavior. This
is demonstrated in the example below. tf.random.shuffle does not have a
registered gradient. As a result tf.stop_gradient is used to avoid the LookupError . x = tf . constant ([ 0.3 , 0.5 ], dtype = tf . float32 ) @tf . custom_gradient def test_func_with_stop_grad ( x ): @tf . function def _inner_func (): # Avoid exception during the forward pass return tf . stop_gradient ( tf . random . shuffle ( x )) # return tf.random.shuffle(x)  # This will raise res = _inner_func () def grad ( upstream ): return upstream # Arbitrarily defined custom gradient return res , grad with tf . GradientTape () as g : g . watch ( x ) res = test_func_with_stop_grad ( x ) g . gradient ( res , x ) See also tf.RegisterGradient which registers a gradient function for a
primitive TensorFlow operation. tf.custom_gradient on the other hand allows
for fine grained control over the gradient computation of a sequence of
operations. Note that if the decorated function uses Variable s, the enclosing variable
scope must be using ResourceVariables . Args f function f(*x) that returns a tuple (y, grad_fn) where: - x is a
sequence of (nested structures of) Tensor inputs to the function. - y is a (nested structure of) Tensor outputs of applying TensorFlow
operations in f to x . - grad_fn is a function with the signature g(*grad_ys) which returns a list of Tensor s the same size as
(flattened) x - the derivatives of Tensor s in y with respect to the Tensor s in x . grad_ys is a sequence of Tensor s the same size as
(flattened) y holding the initial value gradients for each Tensor in y .  In a pure mathematical sense, a vector-argument vector-valued
function f 's derivatives should be its Jacobian matrix J . Here we are
expressing the Jacobian J as a function grad_fn which defines how J will transform a vector grad_ys when left-multiplied with it ( grad_ys *
J , the vector-Jacobian product, or VJP). This functional representation
of a matrix is convenient to use for chain-rule calculation (in e.g. the
back-propagation algorithm).  If f uses Variable s (that are not part
of the inputs), i.e. through get_variable , then grad_fn should have
signature g(*grad_ys, variables=None) , where variables is a list of
the Variable s, and return a 2-tuple (grad_xs, grad_vars) , where grad_xs is the same as above, and grad_vars is a list<Tensor> with
the derivatives of Tensor s in y with respect to the variables (that
is, grad_vars has one Tensor per variable in variables). Returns A function h(x) which returns the same value as f(x)[0] and whose
gradient (as calculated by tf.gradients ) is determined by f(x)[1] .


Page: https://www.tensorflow.org/api_docs/python/tf/device
View source on GitHub Specifies the device for ops created/executed in this context. tf . device ( device_name ) -> ContextManager [ None ] Used in the notebooks Used in the guide Used in the tutorials NumPy API on TensorFlow Use a GPU Introduction to Variables Random number generation Use TPUs Customization basics: tensors and operations Solve GLUE tasks using BERT on TPU This function specifies the device to be used for ops created/executed in a
particular context. Nested contexts will inherit and also create/execute
their ops on the specified device. If a specific device is not required,
consider not using this function so that a device can be automatically
assigned.  In general the use of this function is optional. device_name can
be fully specified, as in "/job:worker/task:1/device:cpu:0", or partially
specified, containing only a subset of the "/"-separated fields. Any fields
which are specified will override device annotations from outer scopes. For example: with tf . device ( '/job:foo' ): # ops created here have devices with /job:foo with tf . device ( '/job:bar/task:0/device:gpu:2' ): # ops created here have the fully specified device above with tf . device ( '/device:gpu:1' ): # ops created here have the device '/job:foo/device:gpu:1' Args device_name The device name to use in the context. Returns A context manager that specifies the default device to use for newly
created ops. Raises RuntimeError If a function is passed in.


Page: https://www.tensorflow.org/api_docs/python/tf/math/divide
View source on GitHub Computes Python style division of x by y . View aliases Main aliases tf.divide Compat aliases for migration See Migration guide for
more details. tf.compat.v1.divide tf . math . divide ( x , y , name = None ) For example: x = tf . constant ([ 16 , 12 , 11 ]) y = tf . constant ([ 4 , 6 , 2 ]) tf . divide ( x , y ) < tf . Tensor : shape = ( 3 ,), dtype = float64 , numpy = array ([ 4. , 2. , 5.5 ]) > Args x A Tensor y A Tensor name A name for the operation (optional). Returns A Tensor with same shape as input


Page: https://www.tensorflow.org/api_docs/python/tf/dynamic_partition
Partitions data into num_partitions tensors using indices from partitions . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.dynamic_partition tf . dynamic_partition ( data : Annotated [ Any , TV_DynamicPartition_T ], partitions : Annotated [ Any , _atypes . Int32 ], num_partitions : int , name = None ) For each index tuple js of size partitions.ndim , the slice data[js, ...] becomes part of outputs[partitions[js]] .  The slices with partitions[js] = i are placed in outputs[i] in lexicographic order of js , and the first
dimension of outputs[i] is the number of entries in partitions equal to i .
In detail, outputs [ i ] . shape = [ sum ( partitions == i )] + data . shape [ partitions . ndim :] outputs [ i ] = pack ([ data [ js , ... ] for js if partitions [ js ] == i ]) data.shape must start with partitions.shape . For example: # Scalar partitions. partitions = 1 num_partitions = 2 data = [ 10 , 20 ] outputs [ 0 ] = [] # Empty with shape [0, 2] outputs [ 1 ] = [[ 10 , 20 ]] # Vector partitions. partitions = [ 0 , 0 , 1 , 1 , 0 ] num_partitions = 2 data = [ 10 , 20 , 30 , 40 , 50 ] outputs [ 0 ] = [ 10 , 20 , 50 ] outputs [ 1 ] = [ 30 , 40 ] See dynamic_stitch for an example on how to merge partitions back. Raises InvalidArgumentError in following cases: If partitions is not in range [0, num_partiions) If partitions.shape does not match prefix of data.shape argument. Args data A Tensor . partitions A Tensor of type int32 .
Any shape.  Indices in the range [0, num_partitions) . num_partitions An int that is >= 1 .
The number of partitions to output. name A name for the operation (optional). Returns A list of num_partitions Tensor objects with the same type as data .


Page: https://www.tensorflow.org/api_docs/python/tf/dynamic_stitch
Interleave the values from the data tensors into a single tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.dynamic_stitch tf . dynamic_stitch ( indices : Annotated [ List [ Any ], _atypes . Int32 ], data : Annotated [ List [ Any ], TV_DynamicStitch_T ], name = None ) -> Annotated [ Any , TV_DynamicStitch_T ] Builds a merged tensor such that merged [ indices [ m ][ i , ... , j ], ... ] = data [ m ][ i , ... , j , ... ] For example, if each indices[m] is scalar or vector, we have # Scalar indices: merged [ indices [ m ], ... ] = data [ m ][ ... ] # Vector indices: merged [ indices [ m ][ i ], ... ] = data [ m ][ i , ... ] Each data[i].shape must start with the corresponding indices[i].shape ,
and the rest of data[i].shape must be constant w.r.t. i .  That is, we
must have data[i].shape = indices[i].shape + constant .  In terms of this constant , the output shape is merged . shape = [ max ( indices ) + 1 ] + constant Values are merged in order, so if an index appears in both indices[m][i] and indices[n][j] for (m,i) < (n,j) the slice data[n][j] will appear in the
merged result. If you do not need this guarantee, ParallelDynamicStitch might
perform better on some devices. For example: indices [ 0 ] = 6 indices [ 1 ] = [ 4 , 1 ] indices [ 2 ] = [[ 5 , 2 ], [ 0 , 3 ]] data [ 0 ] = [ 61 , 62 ] data [ 1 ] = [[ 41 , 42 ], [ 11 , 12 ]] data [ 2 ] = [[[ 51 , 52 ], [ 21 , 22 ]], [[ 1 , 2 ], [ 31 , 32 ]]] merged = [[ 1 , 2 ], [ 11 , 12 ], [ 21 , 22 ], [ 31 , 32 ], [ 41 , 42 ], [ 51 , 52 ], [ 61 , 62 ]] This method can be used to merge partitions created by dynamic_partition as illustrated on the following example: # Apply function (increments x_i) on elements for which a certain condition # apply (x_i != -1 in this example). x = tf . constant ([ 0.1 , - 1. , 5.2 , 4.3 , - 1. , 7.4 ]) condition_mask = tf . not_equal ( x , tf . constant ( - 1. )) partitioned_data = tf . dynamic_partition ( x , tf . cast ( condition_mask , tf . int32 ) , 2 ) partitioned_data [ 1 ] = partitioned_data [ 1 ] + 1.0 condition_indices = tf . dynamic_partition ( tf . range ( tf . shape ( x )[ 0 ]), tf . cast ( condition_mask , tf . int32 ) , 2 ) x = tf . dynamic_stitch ( condition_indices , partitioned_data ) # Here x=[1.1, -1., 6.2, 5.3, -1, 8.4], the -1. values remain # unchanged. Args indices A list of at least 1 Tensor objects with type int32 . data A list with the same length as indices of Tensor objects with the same type. name A name for the operation (optional). Returns A Tensor . Has the same type as data .


Page: https://www.tensorflow.org/api_docs/python/tf/edit_distance
View source on GitHub Computes the Levenshtein distance between sequences. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.edit_distance tf . edit_distance ( hypothesis , truth , normalize = True , name = 'edit_distance' ) This operation takes variable-length sequences ( hypothesis and truth ),
each provided as a SparseTensor , and computes the Levenshtein distance.
You can normalize the edit distance by length of truth by setting normalize to true. For example: Given the following input, hypothesis is a tf.SparseTensor of shape [2, 1, 1] truth is a tf.SparseTensor of shape [2, 2, 2] hypothesis = tf . SparseTensor ( [[ 0 , 0 , 0 ], [ 1 , 0 , 0 ]], [ "a" , "b" ], ( 2 , 1 , 1 )) truth = tf . SparseTensor ( [[ 0 , 1 , 0 ], [ 1 , 0 , 0 ], [ 1 , 0 , 1 ], [ 1 , 1 , 0 ]], [ "a" , "b" , "c" , "a" ], ( 2 , 2 , 2 )) tf . edit_distance ( hypothesis , truth , normalize = True ) < tf . Tensor : shape = ( 2 , 2 ), dtype = float32 , numpy = array ([[ inf , 1. ], [ 0.5 , 1. ]], dtype = float32 ) > The operation returns a dense Tensor of shape [2, 2] with
edit distances normalized by truth lengths. Note: It is possible to calculate edit distance between two
sparse tensors with variable-length values. However, attempting to create
them while eager execution is enabled will result in a ValueError . For the following  inputs, # 'hypothesis' is a tensor of shape `[2, 1]` with variable-length values: #   (0,0) = ["a"] #   (1,0) = ["b"] hypothesis = tf . sparse . SparseTensor ( [[ 0 , 0 , 0 ], [ 1 , 0 , 0 ]], [ "a" , "b" ], ( 2 , 1 , 1 )) # 'truth' is a tensor of shape `[2, 2]` with variable-length values: #   (0,0) = [] #   (0,1) = ["a"] #   (1,0) = ["b", "c"] #   (1,1) = ["a"] truth = tf . sparse . SparseTensor ( [[ 0 , 1 , 0 ], [ 1 , 0 , 0 ], [ 1 , 0 , 1 ], [ 1 , 1 , 0 ]], [ "a" , "b" , "c" , "a" ], ( 2 , 2 , 2 )) normalize = True # The output would be a dense Tensor of shape `(2,)`, with edit distances normalized by 'truth' lengths . # output => array([0., 0.5], dtype=float32) Args hypothesis A SparseTensor containing hypothesis sequences. truth A SparseTensor containing truth sequences. normalize A bool . If True , normalizes the Levenshtein distance by
length of truth. name A name for the operation (optional). Returns A dense Tensor with rank R - 1 , where R is the rank of the SparseTensor inputs hypothesis and truth . Raises TypeError If either hypothesis or truth are not a SparseTensor .


Page: https://www.tensorflow.org/api_docs/python/tf/linalg/eig
View source on GitHub Computes the eigen decomposition of a batch of matrices. View aliases Main aliases tf.eig tf . linalg . eig ( tensor , name = None ) Used in the notebooks Used in the tutorials Quantum data The eigenvalues
and eigenvectors for a non-Hermitian matrix in general are complex. The
eigenvectors are not guaranteed to be linearly independent. Computes the eigenvalues and right eigenvectors of the innermost
N-by-N matrices in tensor such that tensor[...,:,:] * v[..., :,i] = e[..., i] * v[...,:,i] , for i=0...N-1. Args tensor Tensor of shape [..., N, N] . Only the lower triangular part of
each inner inner matrix is referenced. name string, optional name of the operation. Returns e Eigenvalues. Shape is [..., N] . The eigenvalues are not necessarily
ordered. v Eigenvectors. Shape is [..., N, N] . The columns of the inner most
matrices contain eigenvectors of the corresponding matrices in tensor


Page: https://www.tensorflow.org/api_docs/python/tf/linalg/eigvals
View source on GitHub Computes the eigenvalues of one or more matrices. View aliases Main aliases tf.eigvals tf . linalg . eigvals ( tensor , name = None ) Note: If your program backpropagates through this function, you should replace
it with a call to tf.linalg.eig (possibly ignoring the second output) to
avoid computing the eigen decomposition twice. This is because the
eigenvectors are used to compute the gradient w.r.t. the eigenvalues. See
_SelfAdjointEigV2Grad in linalg_grad.py. Args tensor Tensor of shape [..., N, N] . name string, optional name of the operation. Returns e Eigenvalues. Shape is [..., N] . The vector e[..., :] contains the N eigenvalues of tensor[..., :, :] .


Page: https://www.tensorflow.org/api_docs/python/tf/einsum
View source on GitHub Tensor contraction over specified indices and outer product. View aliases Main aliases tf.linalg.einsum Compat aliases for migration See Migration guide for
more details. tf.compat.v1.einsum , tf.compat.v1.linalg.einsum tf . einsum ( equation , * inputs , ** kwargs ) Used in the notebooks Used in the guide Used in the tutorials Matrix approximation with Core APIs Advanced automatic differentiation Working with RNNs Neural style transfer Quantum data Modeling COVID-19 spread in Europe and the effect of interventions Parametrized Quantum Circuits for Reinforcement Learning Research tools Einsum allows defining Tensors by defining their element-wise computation.
This computation is defined by equation , a shorthand form based on Einstein
summation. As an example, consider multiplying two matrices A and B to form a
matrix C.  The elements of C are given by: \[ C_{i,k} = \sum_j A_{i,j} B_{j,k} \] or C [ i , k ] = sum_j A [ i , j ] * B [ j , k ] The corresponding einsum equation is: ij , jk - > ik In general, to convert the element-wise equation into the equation string,
use the following procedure (intermediate strings for matrix multiplication
example provided in parentheses): remove variable names, brackets, and commas, ( ik = sum_j ij * jk ) replace "*" with ",", ( ik = sum_j ij , jk ) drop summation signs, and ( ik = ij, jk ) move the output to the right, while replacing "=" with "->". ( ij,jk->ik ) Note: If the output indices are not specified repeated indices are summed.
So ij,jk->ik can be simplified to ij,jk . Many common operations can be expressed in this way.  For example: Matrix multiplication m0 = tf . random . normal ( shape = [ 2 , 3 ]) m1 = tf . random . normal ( shape = [ 3 , 5 ]) e = tf . einsum ( 'ij,jk->ik' , m0 , m1 ) # output[i,k] = sum_j m0[i,j] * m1[j, k] print ( e . shape ) ( 2 , 5 ) Repeated indices are summed if the output indices are not specified. e = tf . einsum ( 'ij,jk' , m0 , m1 ) # output[i,k] = sum_j m0[i,j] * m1[j, k] print ( e . shape ) ( 2 , 5 ) Dot product u = tf . random . normal ( shape = [ 5 ]) v = tf . random . normal ( shape = [ 5 ]) e = tf . einsum ( 'i,i->' , u , v ) # output = sum_i u[i]*v[i] print ( e . shape ) () Outer product u = tf . random . normal ( shape = [ 3 ]) v = tf . random . normal ( shape = [ 5 ]) e = tf . einsum ( 'i,j->ij' , u , v ) # output[i,j] = u[i]*v[j] print ( e . shape ) ( 3 , 5 ) Transpose m = tf . ones ( 2 , 3 ) e = tf . einsum ( 'ij->ji' , m0 ) # output[j,i] = m0[i,j] print ( e . shape ) ( 3 , 2 ) Diag m = tf . reshape ( tf . range ( 9 ), [ 3 , 3 ]) diag = tf . einsum ( 'ii->i' , m ) print ( diag . shape ) ( 3 ,) Trace # Repeated indices are summed. trace = tf . einsum ( 'ii' , m ) # output[j,i] = trace(m) = sum_i m[i, i] assert trace == sum ( diag ) print ( trace . shape ) () Batch matrix multiplication s = tf . random . normal ( shape = [ 7 , 5 , 3 ]) t = tf . random . normal ( shape = [ 7 , 3 , 2 ]) e = tf . einsum ( 'bij,bjk->bik' , s , t ) # output[a,i,k] = sum_j s[a,i,j] * t[a, j, k] print ( e . shape ) ( 7 , 5 , 2 ) This method does not support broadcasting on named-axes. All axes with
matching labels should have the same length. If you have length-1 axes,
use tf.squeeze or tf.reshape to eliminate them. To write code that is agnostic to the number of indices in the input
use an ellipsis. The ellipsis is a placeholder for "whatever other indices
fit here". For example, to perform a NumPy-style broadcasting-batch-matrix multiplication
where the matrix multiply acts on the last two axes of the input, use: s = tf . random . normal ( shape = [ 11 , 7 , 5 , 3 ]) t = tf . random . normal ( shape = [ 11 , 7 , 3 , 2 ]) e = tf . einsum ( '...ij,...jk->...ik' , s , t ) print ( e . shape ) ( 11 , 7 , 5 , 2 ) Einsum will broadcast over axes covered by the ellipsis. s = tf . random . normal ( shape = [ 11 , 1 , 5 , 3 ]) t = tf . random . normal ( shape = [ 1 , 7 , 3 , 2 ]) e = tf . einsum ( '...ij,...jk->...ik' , s , t ) print ( e . shape ) ( 11 , 7 , 5 , 2 ) Args equation a str describing the contraction, in the same format as numpy.einsum . *inputs the inputs to contract (each one a Tensor ), whose shapes should
be consistent with equation . **kwargs optimize: Optimization strategy to use to find contraction path using
opt_einsum. Must be 'greedy', 'optimal', 'branch-2', 'branch-all' or
'auto'. (optional, default: 'greedy'). name: A name for the operation (optional). Returns The contracted Tensor , with shape determined by equation . Raises ValueError If the format of equation is incorrect, number of inputs or their shapes are inconsistent with equation .


Page: https://www.tensorflow.org/api_docs/python/tf/ensure_shape
View source on GitHub Updates the shape of a tensor and checks at runtime that the shape holds. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.ensure_shape tf . ensure_shape ( x , shape , name = None ) When executed, this operation asserts that the input tensor x 's shape
is compatible with the shape argument.
See tf.TensorShape.is_compatible_with for details. x = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) x = tf . ensure_shape ( x , [ 2 , 3 ]) Use None for unknown dimensions: x = tf . ensure_shape ( x , [ None , 3 ]) x = tf . ensure_shape ( x , [ 2 , None ]) If the tensor's shape is not compatible with the shape argument, an error
is raised: x = tf . ensure_shape ( x , [ 5 ]) Traceback ( most recent call last ): tf . errors . InvalidArgumentError : Shape of tensor dummy_input [ 3 ] is not compatible with expected shape [ 5 ] . [ Op : EnsureShape ] During graph construction (typically tracing a tf.function ), tf.ensure_shape updates the static-shape of the result tensor by
merging the two shapes. See tf.TensorShape.merge_with for details. This is most useful when you know a shape that can't be determined
statically by TensorFlow. The following trivial tf.function prints the input tensor's
static-shape before and after ensure_shape is applied. @tf . function def f ( tensor ): print ( "Static-shape before:" , tensor . shape ) tensor = tf . ensure_shape ( tensor , [ None , 3 ]) print ( "Static-shape after:" , tensor . shape ) return tensor This lets you see the effect of tf.ensure_shape when the function is traced: >>> cf = f . get_concrete_function ( tf . TensorSpec ([ None , None ])) Static - shape before : ( None , None ) Static - shape after : ( None , 3 ) cf ( tf . zeros ([ 3 , 3 ])) # Passes cf ( tf . constant ([ 1 , 2 , 3 ])) # fails Traceback ( most recent call last ): InvalidArgumentError : Shape of tensor x [ 3 ] is not compatible with expected shape [ 3 , 3 ] . The above example raises tf.errors.InvalidArgumentError , because x 's
shape, (3,) , is not compatible with the shape argument, (None, 3) Inside a tf.function or v1.Graph context it checks both the buildtime and
runtime shapes. This is stricter than tf.Tensor.set_shape which only
checks the buildtime shape. Note: This differs from tf.Tensor.set_shape in that it sets the static shape
of the resulting tensor and enforces it at runtime, raising an error if the
tensor's runtime shape is incompatible with the specified shape. tf.Tensor.set_shape sets the static shape of the tensor without enforcing it
at runtime, which may result in inconsistencies between the statically-known
shape of tensors and the runtime value of tensors. For example, of loading images of a known size: @tf . function def decode_image ( png ): image = tf . image . decode_png ( png , channels = 3 ) # the `print` executes during tracing. print ( "Initial shape: " , image . shape ) image = tf . ensure_shape ( image ,[ 28 , 28 , 3 ]) print ( "Final shape: " , image . shape ) return image When tracing a function, no ops are being executed, shapes may be unknown.
See the Concrete Functions Guide for details. concrete_decode = decode_image . get_concrete_function ( tf . TensorSpec ([], dtype = tf . string )) Initial shape : ( None , None , 3 ) Final shape : ( 28 , 28 , 3 ) image = tf . random . uniform ( maxval = 255 , shape = [ 28 , 28 , 3 ], dtype = tf . int32 ) image = tf . cast ( image , tf . uint8 ) png = tf . image . encode_png ( image ) image2 = concrete_decode ( png ) print ( image2 . shape ) ( 28 , 28 , 3 ) image = tf . concat ([ image , image ], axis = 0 ) print ( image . shape ) ( 56 , 28 , 3 ) png = tf . image . encode_png ( image ) image2 = concrete_decode ( png ) Traceback ( most recent call last ): tf . errors . InvalidArgumentError : Shape of tensor DecodePng [ 56 , 28 , 3 ] is not compatible with expected shape [ 28 , 28 , 3 ] . Caution: if you don't use the result of tf.ensure_shape the check may not
run. @tf . function def bad_decode_image ( png ): image = tf . image . decode_png ( png , channels = 3 ) # the `print` executes during tracing. print ( "Initial shape: " , image . shape ) # BAD: forgot to use the returned tensor. tf . ensure_shape ( image ,[ 28 , 28 , 3 ]) print ( "Final shape: " , image . shape ) return image image = bad_decode_image ( png ) Initial shape : ( None , None , 3 ) Final shape : ( None , None , 3 ) print ( image . shape ) ( 56 , 28 , 3 ) Args x A Tensor . shape A TensorShape representing the shape of this tensor, a TensorShapeProto , a list, a tuple, or None. name A name for this operation (optional). Defaults to "EnsureShape". Returns A Tensor . Has the same type and contents as x . Raises tf.errors.InvalidArgumentError If shape is incompatible with the shape
of x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/equal
View source on GitHub Returns the truth value of (x == y) element-wise. View aliases Main aliases tf.equal Compat aliases for migration See Migration guide for
more details. tf.compat.v1.equal , tf.compat.v1.math.equal tf . math . equal ( x , y , name = None ) Used in the notebooks Used in the guide Used in the tutorials Multilayer perceptrons for digit recognition with Core APIs Distributed training with Core APIs and DTensor Extension types MinDiff Data Preparation Tutorial on Multi Armed Bandits in TF-Agents Classify Flowers with Transfer Learning Federated Learning for Image Classification Sending Different Data To Particular Clients With tff.federated_select End to end example for BigQuery TensorFlow reader Performs a broadcast with the
arguments and then an element-wise equality comparison, returning a Tensor of
boolean values. For example: x = tf . constant ([ 2 , 4 ]) y = tf . constant ( 2 ) tf . math . equal ( x , y ) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ True , False ]) > x = tf . constant ([ 2 , 4 ]) y = tf . constant ([ 2 , 4 ]) tf . math . equal ( x , y ) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ True , True ]) > Args x A tf.Tensor . y A tf.Tensor . name A name for the operation (optional). Returns A tf.Tensor of type bool with the same size as that of x or y. Raises tf.errors.InvalidArgumentError : If shapes of arguments are incompatible


Page: https://www.tensorflow.org/api_docs/python/tf/executing_eagerly
View source on GitHub Checks whether the current thread has eager execution enabled. tf . executing_eagerly () Used in the notebooks Used in the tutorials Text classification with TensorFlow Hub: Movie reviews Neural machine translation with attention Fast Style Transfer for Arbitrary Styles Text Classification with Movie Reviews Graph regularization for sentiment classification using synthesized graphs Eager execution is enabled by default and this API returns True in most of cases. However, this API might return False in the following use
cases. Executing inside tf.function , unless under tf.init_scope or tf.config.run_functions_eagerly(True) is previously called. Executing inside a transformation function for tf.dataset . tf.compat.v1.disable_eager_execution() is called. General case: print ( tf . executing_eagerly ()) True Inside tf.function : @tf . function def fn (): with tf . init_scope (): print ( tf . executing_eagerly ()) print ( tf . executing_eagerly ()) fn () True False Inside tf.function after tf.config.run_functions_eagerly(True) is called: tf . config . run_functions_eagerly ( True ) @tf . function def fn (): with tf . init_scope (): print ( tf . executing_eagerly ()) print ( tf . executing_eagerly ()) fn () True True tf . config . run_functions_eagerly ( False ) Inside a transformation function for tf.dataset : def data_fn ( x ): print ( tf . executing_eagerly ()) return x dataset = tf . data . Dataset . range ( 100 ) dataset = dataset . map ( data_fn ) False Returns True if the current thread has eager execution enabled.


Page: https://www.tensorflow.org/api_docs/python/tf/math/exp
View source on GitHub Computes exponential of x element-wise.  \(y = e^x\). View aliases Main aliases tf.exp Compat aliases for migration See Migration guide for
more details. tf.compat.v1.exp tf . math . exp ( x , name = None ) Used in the notebooks Used in the guide Used in the tutorials Understanding masking & padding Scalable model compression Convolutional Variational Autoencoder Learned data compression Multiple changepoint detection and Bayesian model selection Modeling COVID-19 spread in Europe and the effect of interventions This function computes the exponential of the input tensor element-wise.
i.e. math.exp(x) or \(e^x\), where x is the input tensor.
\(e\) denotes Euler's number and is approximately equal to 2.718281.
Output is positive for any real input. x = tf . constant ( 2.0 ) tf . math . exp ( x ) < tf . Tensor : shape = (), dtype = float32 , numpy = 7.389056 > x = tf . constant ([ 2.0 , 8.0 ]) tf . math . exp ( x ) < tf . Tensor : shape = ( 2 ,), dtype = float32 , numpy = array ([ 7.389056 , 2980.958 ], dtype = float32 ) > For complex numbers, the exponential value is calculated as \[
e^{x+iy} = {e^x} {e^{iy} } = {e^x} ({\cos (y) + i \sin (y)})
\] For 1+1j the value would be computed as: \[
e^1 (\cos (1) + i \sin (1)) = 2.7182817 \times (0.5403023+0.84147096j)
\] x = tf . constant ( 1 + 1 j ) tf . math . exp ( x ) < tf . Tensor : shape = (), dtype = complex128 , numpy = ( 1.4686939399158851 + 2.2873552871788423 j ) > Args x A tf.Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A tf.Tensor . Has the same type as x . numpy compatibility Equivalent to np.exp


Page: https://www.tensorflow.org/api_docs/python/tf/expand_dims
View source on GitHub Returns a tensor with a length 1 axis inserted at index axis . tf . expand_dims ( input , axis , name = None ) Used in the notebooks Used in the guide Used in the tutorials Extension types Import a JAX model using JAX2TF Migrate `tf.feature_column`s to Keras preprocessing layers Understanding masking & padding Working with RNNs Integrated gradients Playing CartPole with the Actor-Critic method Generate music with an RNN DeepDream pix2pix: Image-to-image translation with a conditional GAN Given a tensor input , this operation inserts a dimension of length 1 at the
dimension index axis of input 's shape. The dimension index follows Python
indexing rules: It's zero-based, a negative index it is counted backward
from the end. This operation is useful to: Add an outer "batch" dimension to a single element. Align axes for broadcasting. To add an inner vector length axis to a tensor of scalars. For example: If you have a single image of shape [height, width, channels] : image = tf . zeros ([ 10 , 10 , 3 ]) You can add an outer batch axis by passing axis=0 : tf . expand_dims ( image , axis = 0 ) . shape . as_list () [ 1 , 10 , 10 , 3 ] The new axis location matches Python list.insert(axis, 1) : tf . expand_dims ( image , axis = 1 ) . shape . as_list () [ 10 , 1 , 10 , 3 ] Following standard Python indexing rules, a negative axis counts from the
end so axis=-1 adds an inner most dimension: tf . expand_dims ( image , - 1 ) . shape . as_list () [ 10 , 10 , 3 , 1 ] This operation requires that axis is a valid index for input.shape ,
following Python indexing rules: - 1 - tf . rank ( input ) < = axis < = tf . rank ( input ) This operation is related to: tf.squeeze , which removes dimensions of size 1. tf.reshape , which provides more flexible reshaping capability. tf.sparse.expand_dims , which provides this functionality for tf.SparseTensor Args input A Tensor . axis Integer specifying the dimension index at which to expand the
shape of input . Given an input of D dimensions, axis must be in range [-(D+1), D] (inclusive). name Optional string. The name of the output Tensor . Returns A tensor with the same data as input , with an additional dimension
inserted at the index specified by axis . Raises TypeError If axis is not specified. InvalidArgumentError If axis is out of range [-(D+1), D] .


Page: https://www.tensorflow.org/api_docs/python/tf/extract_volume_patches
Extract patches from input and put them in the "depth" output dimension. 3D extension of extract_image_patches . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.extract_volume_patches tf . extract_volume_patches ( input : Annotated [ Any , TV_ExtractVolumePatches_T ], ksizes , strides , padding : str , name = None ) -> Annotated [ Any , TV_ExtractVolumePatches_T ] Args input A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 .
5-D Tensor with shape [batch, in_planes, in_rows, in_cols, depth] . ksizes A list of ints that has length >= 5 .
The size of the sliding window for each dimension of input . strides A list of ints that has length >= 5 .
1-D of length 5. How far the centers of two consecutive patches are in input . Must be: [1, stride_planes, stride_rows, stride_cols, 1] . padding A string from: "SAME", "VALID" .
The type of padding algorithm to use. The size-related attributes are specified as follows: ksizes = [ 1 , ksize_planes , ksize_rows , ksize_cols , 1 ] strides = [ 1 , stride_planes , strides_rows , strides_cols , 1 ] name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/eye
View source on GitHub Construct an identity matrix, or a batch of matrices. View aliases Main aliases tf.linalg.eye Compat aliases for migration See Migration guide for
more details. tf.compat.v1.eye , tf.compat.v1.linalg.eye tf . eye ( num_rows , num_columns = None , batch_shape = None , dtype = tf . dtypes . float32 , name = None ) Used in the notebooks Used in the guide Used in the tutorials Advanced automatic differentiation Introduction to graphs and tf.function TensorFlow Probability Case Study: Covariance Estimation Learnable Distributions Zoo A Tour of TensorFlow Probability Bayesian Gaussian Mixture Model and Hamiltonian MCMC Bayesian Modeling with Joint Distribution See also tf.ones , tf.zeros , tf.fill , tf.one_hot . # Construct one identity matrix. tf . eye ( 2 ) == > [[ 1. , 0. ], [ 0. , 1. ]] # Construct a batch of 3 identity matrices, each 2 x 2. # batch_identity[i, :, :] is a 2 x 2 identity matrix, i = 0, 1, 2. batch_identity = tf . eye ( 2 , batch_shape = [ 3 ]) # Construct one 2 x 3 "identity" matrix tf . eye ( 2 , num_columns = 3 ) == > [[ 1. , 0. , 0. ], [ 0. , 1. , 0. ]] Args num_rows Non-negative int32 scalar Tensor giving the number of rows
in each batch matrix. num_columns Optional non-negative int32 scalar Tensor giving the number
of columns in each batch matrix.  Defaults to num_rows . batch_shape A list or tuple of Python integers or a 1-D int32 Tensor .
If provided, the returned Tensor will have leading batch dimensions of
this shape. dtype The type of an element in the resulting Tensor name A name for this Op .  Defaults to "eye". Returns A Tensor of shape batch_shape + [num_rows, num_columns]


Page: https://www.tensorflow.org/api_docs/python/tf/fftnd
ND fast Fourier transform. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.fftnd tf . fftnd ( input : Annotated [ Any , TV_FFTND_Tcomplex ], fft_length : Annotated [ Any , _atypes . Int32 ], axes : Annotated [ Any , _atypes . Int32 ], name = None ) -> Annotated [ Any , TV_FFTND_Tcomplex ] Computes the n-dimensional discrete Fourier transform over
designated dimensions of input . The designated dimensions of input are assumed to be the result of FFTND . If fft_length[i] shape(input)[i], the input is padded with zeros. If fft_length
is not given, the default shape(input) is used. Axes mean the dimensions to perform the transform on. Default is to perform on
all axes. Args input A Tensor . Must be one of the following types: complex64 , complex128 .
A complex tensor. fft_length A Tensor of type int32 .
An int32 tensor. The FFT length for each dimension. axes A Tensor of type int32 .
An int32 tensor with a same shape as fft_length. Axes to perform the transform. name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/fill
View source on GitHub Creates a tensor filled with a scalar value. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.fill tf . fill ( dims , value , name = None , layout = None ) Used in the notebooks Used in the guide Used in the tutorials Ragged tensors tf.data: Build TensorFlow input pipelines Subword tokenizers Unicode strings Scalable model compression Multiple changepoint detection and Bayesian model selection Graph-based Neural Structured Learning in TFX Graph regularization for sentiment classification using synthesized graphs Bayesian Gaussian Mixture Model and Hamiltonian MCMC See also tf.ones , tf.zeros , tf.one_hot , tf.eye . This operation creates a tensor of shape dims and fills it with value . For example: tf . fill ([ 2 , 3 ], 9 ) < tf . Tensor : shape = ( 2 , 3 ), dtype = int32 , numpy = array ([[ 9 , 9 , 9 ], [ 9 , 9 , 9 ]], dtype = int32 ) > tf.fill evaluates at graph runtime and supports dynamic shapes based on
other runtime tf.Tensors , unlike tf.constant(value, shape=dims) , which
embeds the value as a Const node. Args dims A 1-D sequence of non-negative numbers. Represents the shape of the
output tf.Tensor . Entries should be of type: int32 , int64 . value A value to fill the returned tf.Tensor . name Optional string. The name of the output tf.Tensor . layout Optional, tf.experimental.dtensor.Layout . If provided, the result
is a DTensor with the
provided layout. Returns A tf.Tensor with shape dims and the same dtype as value . Raises InvalidArgumentError dims contains negative entries. NotFoundError dims contains non-integer entries. numpy compatibility Similar to np.full . In numpy , more parameters are supported. Passing a
number argument as the shape ( np.full(5, value) ) is valid in numpy for
specifying a 1-D shaped result, while TensorFlow does not support this syntax.


Page: https://www.tensorflow.org/api_docs/python/tf/fingerprint
View source on GitHub Generates fingerprint values. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.fingerprint tf . fingerprint ( data , method = 'farmhash64' , name = None ) Generates fingerprint values of data . Fingerprint op considers the first dimension of data as the batch dimension,
and output[i] contains the fingerprint value generated from contents in data[i, ...] for all i . Fingerprint op writes fingerprint values as byte arrays. For example, the
default method farmhash64 generates a 64-bit fingerprint value at a time.
This 8-byte value is written out as an tf.uint8 array of size 8, in
little-endian order. For example, suppose that data has data type tf.int32 and shape (2, 3, 4),
and that the fingerprint method is farmhash64 . In this case, the output
shape is (2, 8), where 2 is the batch dimension size of data , and 8 is the
size of each fingerprint value in bytes. output[0, :] is generated from
12 integers in data[0, :, :] and similarly output[1, :] is generated from
other 12 integers in data[1, :, :] . Note that this op fingerprints the raw underlying buffer, and it does not
fingerprint Tensor's metadata such as data type and/or shape. For example, the
fingerprint values are invariant under reshapes and bitcasts as long as the
batch dimension remain the same: tf . fingerprint ( data ) == tf . fingerprint ( tf . reshape ( data , ... )) tf . fingerprint ( data ) == tf . fingerprint ( tf . bitcast ( data , ... )) For string data, one should expect tf.fingerprint(data) !=
tf.fingerprint(tf.string.reduce_join(data)) in general. Args data A Tensor . Must have rank 1 or higher. method A Tensor of type tf.string . Fingerprint method used by this op.
Currently, available method is farmhash64 . name A name for the operation (optional). Returns A two-dimensional Tensor of type tf.uint8 . The first dimension equals to data 's first dimension, and the second dimension size depends on the
fingerprint algorithm.


Page: https://www.tensorflow.org/api_docs/python/tf/math/floor
View source on GitHub Returns element-wise largest integer not greater than x. View aliases Main aliases tf.floor Compat aliases for migration See Migration guide for
more details. tf.compat.v1.floor tf . math . floor ( x , name = None ) Used in the notebooks Used in the tutorials Scalable model compression Both input range is (-inf, inf) and the
output range consists of all integer values. For example: x = tf . constant ([ 1.3324 , - 1.5 , 5.555 , - 2.532 , 0.99 , float ( "inf" )]) tf . floor ( x ) . numpy () array ([ 1. , - 2. , 5. , - 3. , 0. , inf ], dtype = float32 ) Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 . name A name for the operation (optional). Returns A Tensor . Has the same type as x.


Page: https://www.tensorflow.org/api_docs/python/tf/foldl
View source on GitHub foldl on the list of tensors unpacked from elems on dimension 0. (deprecated argument values) tf . foldl ( fn , elems , initializer = None , parallel_iterations = 10 , back_prop = True , swap_memory = False , name = None ) Deprecated: SOME ARGUMENT VALUES ARE DEPRECATED: (back_prop=False) . They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.foldl(fn, elems, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.foldl(fn, elems)) This foldl operator repeatedly applies the callable fn to a sequence
of elements from first to last. The elements are made of the tensors
unpacked from elems on dimension 0. The callable fn takes two tensors as
arguments. The first argument is the accumulated value computed from the
preceding invocation of fn, and the second is the value at the current
position of elems . If initializer is None, elems must contain at least
one element, and its first element is used as the initializer. Suppose that elems is unpacked into values , a list of tensors. The shape
of the result tensor is fn(initializer, values[0]).shape`. This method also allows multi-arity elems and output of fn .  If elems is a (possibly nested) list or tuple of tensors, then each of these tensors
must have a matching first (unpack) dimension.  The signature of fn may
match the structure of elems .  That is, if elems is (t1, [t2, t3, [t4, t5]]) , then an appropriate signature for fn is: fn = lambda (t1, [t2, t3, [t4, t5]]): . Args fn The callable to be performed. elems A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension.  The nested sequence of the
resulting slices will be the first argument to fn . initializer (optional) A tensor or (possibly nested) sequence of tensors,
as the initial value for the accumulator. parallel_iterations (optional) The number of iterations allowed to run in
parallel. back_prop (optional) Deprecated. False disables support for back
propagation. Prefer using tf.stop_gradient instead. swap_memory (optional) True enables GPU-CPU memory swapping. name (optional) Name prefix for the returned tensors. Returns A tensor or (possibly nested) sequence of tensors, resulting from applying fn consecutively to the list of tensors unpacked from elems , from first
to last. Raises TypeError if fn is not callable. Example elems = tf . constant ([ 1 , 2 , 3 , 4 , 5 , 6 ]) sum = tf . foldl ( lambda a , x : a + x , elems ) # sum == 21


Page: https://www.tensorflow.org/api_docs/python/tf/foldr
View source on GitHub foldr on the list of tensors unpacked from elems on dimension 0. (deprecated argument values) tf . foldr ( fn , elems , initializer = None , parallel_iterations = 10 , back_prop = True , swap_memory = False , name = None ) Deprecated: SOME ARGUMENT VALUES ARE DEPRECATED: (back_prop=False) . They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.foldr(fn, elems, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems)) This foldr operator repeatedly applies the callable fn to a sequence
of elements from last to first. The elements are made of the tensors
unpacked from elems . The callable fn takes two tensors as arguments.
The first argument is the accumulated value computed from the preceding
invocation of fn, and the second is the value at the current position of elems . If initializer is None, elems must contain at least one element,
and its first element is used as the initializer. Suppose that elems is unpacked into values , a list of tensors. The shape
of the result tensor is fn(initializer, values[0]).shape . This method also allows multi-arity elems and output of fn .  If elems is a (possibly nested) list or tuple of tensors, then each of these tensors
must have a matching first (unpack) dimension.  The signature of fn may
match the structure of elems .  That is, if elems is (t1, [t2, t3, [t4, t5]]) , then an appropriate signature for fn is: fn = lambda (t1, [t2, t3, [t4, t5]]): . Args fn The callable to be performed. elems A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension.  The nested sequence of the
resulting slices will be the first argument to fn . initializer (optional) A tensor or (possibly nested) sequence of tensors,
as the initial value for the accumulator. parallel_iterations (optional) The number of iterations allowed to run in
parallel. back_prop (optional) Deprecated. False disables support for back
propagation. Prefer using tf.stop_gradient instead. swap_memory (optional) True enables GPU-CPU memory swapping. name (optional) Name prefix for the returned tensors. Returns A tensor or (possibly nested) sequence of tensors, resulting from applying fn consecutively to the list of tensors unpacked from elems , from last
to first. Raises TypeError if fn is not callable. Example elems = [ 1 , 2 , 3 , 4 , 5 , 6 ] sum = tf . foldr ( lambda a , x : a + x , elems ) # sum == 21


Page: https://www.tensorflow.org/api_docs/python/tf/function
View source on GitHub Compiles a function into a callable TensorFlow graph. (deprecated arguments) (deprecated arguments) (deprecated arguments) View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.function tf . function ( func = None , input_signature = None , autograph = True , jit_compile = None , reduce_retracing = False , experimental_implements = None , experimental_autograph_options = None , experimental_attributes = None , experimental_relax_shapes = None , experimental_compile = None , experimental_follow_type_hints = None ) -> tf . types . experimental . PolymorphicFunction Used in the notebooks Used in the guide Used in the tutorials Better performance with tf.function Introduction to graphs and tf.function Extension types Random number generation Import a JAX model using JAX2TF Custom training with tf.distribute.Strategy Distributed Input Distributed training with DTensors Parameter server training with ParameterServerStrategy Using DTensors with Keras Deprecated: SOME ARGUMENTS ARE DEPRECATED: (experimental_compile) . They will be removed in a future version.
Instructions for updating:
experimental_compile is deprecated, use jit_compile instead Deprecated: SOME ARGUMENTS ARE DEPRECATED: (experimental_relax_shapes) . They will be removed in a future version.
Instructions for updating:
experimental_relax_shapes is deprecated, use reduce_retracing instead Deprecated: SOME ARGUMENTS ARE DEPRECATED: (experimental_follow_type_hints) . They will be removed in a future version.
Instructions for updating:
experimental_follow_type_hints is deprecated tf.function constructs a tf.types.experimental.PolymorphicFunction that
executes a TensorFlow graph ( tf.Graph ) created by trace-compiling the
TensorFlow operations in func . More information on the topic can be found
in Introduction to Graphs and tf.function . See Better Performance with tf.function for tips on performance and
known limitations. Example usage: @tf . function def f ( x , y ): return x ** 2 + y x = tf . constant ([ 2 , 3 ]) y = tf . constant ([ 3 , - 2 ]) f ( x , y ) < tf . Tensor : ... numpy = array ([ 7 , 7 ], ... ) > The trace-compilation allows non-TensorFlow operations to execute, but under
special conditions. In general, only TensorFlow operations are guaranteed to
run and create fresh results whenever the PolymorphicFunction is called. Features func may use data-dependent Python control flow statements, including if , for , while break , continue and return : @tf . function def f ( x ): if tf . reduce_sum ( x ) > 0 : return x * x else : return - x // 2 f ( tf . constant ( - 2 )) < tf . Tensor : ... numpy = 1 > func 's closure may include tf.Tensor and tf.Variable objects: @tf . function def f (): return x ** 2 + y x = tf . constant ([ - 2 , - 3 ]) y = tf . Variable ([ 3 , - 2 ]) f () < tf . Tensor : ... numpy = array ([ 7 , 7 ], ... ) > func may also use ops with side effects, such as tf.print , tf.Variable and others: v = tf . Variable ( 1 ) @tf . function def f ( x ): for i in tf . range ( x ): v . assign_add ( i ) f ( 3 ) v < tf . Variable ... numpy = 4 > Important: Any Python side-effects (appending to a list, printing with print , etc) will only happen once, when func is traced. To have
side-effects executed into your tf.function they need to be written
as TF ops: l = [] @tf . function def f ( x ): for i in x : l . append ( i + 1 ) # Caution! Will only happen once when tracing f ( tf . constant ([ 1 , 2 , 3 ])) l [ < tf . Tensor ... > ] Instead, use TensorFlow collections like tf.TensorArray : @tf . function def f ( x ): ta = tf . TensorArray ( dtype = tf . int32 , size = 0 , dynamic_size = True ) for i in range ( len ( x )): ta = ta . write ( i , x [ i ] + 1 ) return ta . stack () f ( tf . constant ([ 1 , 2 , 3 ])) < tf . Tensor : ... , numpy = array ([ 2 , 3 , 4 ], ... ) > tf.function creates polymorphic callables Internally, tf.types.experimental.PolymorphicFunction may contain multiple tf.types.experimental.ConcreteFunction s, each specialized to arguments with
different data types or shapes, since TensorFlow can perform more
optimizations on graphs of specific shapes, dtypes and values of constant
arguments. tf.function treats any pure Python values as opaque objects (best
thought of as compile-time constants), and builds a separate tf.Graph for
each set of Python arguments that it encounters.
For more information, see the tf.function guide Executing a PolymorphicFunction will select and execute the appropriate ConcreteFunction based on the argument types and values. To obtain an individual ConcreteFunction , use the PolymorphicFunction.get_concrete_function method. It can be called with the
same arguments as func and returns a tf.types.experimental.ConcreteFunction . ConcreteFunction s are backed by a
single tf.Graph : @tf . function def f ( x ): return x + 1 isinstance ( f . get_concrete_function ( 1 ) . graph , tf . Graph ) True ConcreteFunction s can be executed just like PolymorphicFunction s, but their
input is resticted to the types to which they're specialized. Retracing ConcreteFunctions are built (traced) on the fly, as the PolymorphicFunction is
called with new TensorFlow types or shapes, or with new Python values as
arguments. When PolymorphicFunction builds a new trace, it is said that func is retraced. Retracing is a frequent performance concern for tf.function as
it can be considerably slower than executing a graph that's already been
traced. It is ideal to minimize the amount of retracing in your code. Caution: Passing python scalars or lists as arguments to tf.function will
usually retrace. To avoid this, pass numeric arguments as Tensors whenever
possible: @tf . function def f ( x ): return tf . abs ( x ) f1 = f . get_concrete_function ( 1 ) f2 = f . get_concrete_function ( 2 ) # Slow - compiles new graph f1 is f2 False f1 = f . get_concrete_function ( tf . constant ( 1 )) f2 = f . get_concrete_function ( tf . constant ( 2 )) # Fast - reuses f1 f1 is f2 True Python numerical arguments should only be used when they take few distinct
values, such as hyperparameters like the number of layers in a neural network. Input signatures For Tensor arguments, PolymorphicFunction creates a new ConcreteFunction for
every unique set of input shapes and datatypes. The example below creates two
separate ConcreteFunction s, each specialized to a different shape: @tf . function def f ( x ): return x + 1 vector = tf . constant ([ 1.0 , 1.0 ]) matrix = tf . constant ([[ 3.0 ]]) f . get_concrete_function ( vector ) is f . get_concrete_function ( matrix ) False An "input signature" can be optionally provided to tf.function to control
this process. The input signature specifies the shape and type of each
Tensor argument to the function using a tf.TensorSpec object. More general
shapes can be used. This ensures only one ConcreteFunction is created, and
restricts the PolymorphicFunction to the specified shapes and types. It is
an effective way to limit retracing when Tensors have dynamic shapes. @tf . function ( input_signature = [ tf . TensorSpec ( shape = None , dtype = tf . float32 )]) def f ( x ): return x + 1 vector = tf . constant ([ 1.0 , 1.0 ]) matrix = tf . constant ([[ 3.0 ]]) f . get_concrete_function ( vector ) is f . get_concrete_function ( matrix ) True Variables may only be created once tf.function only allows creating new tf.Variable objects when it is called
for the first time: class MyModule ( tf . Module ): def __init__ ( self ): self . v = None @tf . function def __call__ ( self , x ): if self . v is None : self . v = tf . Variable ( tf . ones_like ( x )) return self . v * x In general, it is recommended to create tf.Variable s outside of tf.function .
In simple cases, persisting state across tf.function boundaries may be
implemented using a pure functional style in which state is represented by tf.Tensor s passed as arguments and returned as return values. Contrast the two styles below: state = tf . Variable ( 1 ) @tf . function def f ( x ): state . assign_add ( x ) f ( tf . constant ( 2 )) # Non-pure functional style state < tf . Variable ... numpy = 3 > state = tf . constant ( 1 ) @tf . function def f ( state , x ): state += x return state state = f ( state , tf . constant ( 2 )) # Pure functional style state < tf . Tensor : ... numpy = 3 > Python operations execute only once per trace func may contain TensorFlow operations mixed with pure Python operations.
However, when the function is executed, only the TensorFlow operations will
run. The Python operations run only once, at trace time. If TensorFlow
operations depend on results from Python operations, those results will be
frozen into the graph. @tf . function def f ( a , b ): print ( 'this runs at trace time; a is' , a , 'and b is' , b ) return b f ( 1 , tf . constant ( 1 )) this runs at trace time ; a is 1 and b is Tensor ( "..." , shape = (), dtype = int32 ) < tf . Tensor : shape = (), dtype = int32 , numpy = 1 > f ( 1 , tf . constant ( 2 )) < tf . Tensor : shape = (), dtype = int32 , numpy = 2 > f ( 2 , tf . constant ( 1 )) this runs at trace time ; a is 2 and b is Tensor ( "..." , shape = (), dtype = int32 ) < tf . Tensor : shape = (), dtype = int32 , numpy = 1 > f ( 2 , tf . constant ( 2 )) < tf . Tensor : shape = (), dtype = int32 , numpy = 2 > Args func The function to be compiled. If func is None, tf.function returns
a decorator that can be invoked with a single argument - func . In other
words, tf.function(input_signature=...)(func) is equivalent to tf.function(func, input_signature=...) . The former can be used as
decorator. input_signature A possibly nested sequence of tf.TensorSpec objects
specifying the shapes and dtypes of the Tensors that will be supplied to
this function. If None , a separate function is instantiated for each
inferred input signature.  If input_signature is specified, every input to func must be a Tensor , and func cannot accept **kwargs . autograph Whether autograph should be applied on func before tracing a
graph. Data-dependent Python control flow statements require autograph=True . For more information, see the tf.function and AutoGraph guide . jit_compile If True , compiles the function using XLA . XLA performs compiler optimizations,
such as fusion, and attempts to emit more efficient code. This may
drastically improve the performance. If set to True ,
the whole function needs to be compilable by XLA, or an errors.InvalidArgumentError is thrown.
If None (default), compiles the function with XLA when running on TPU
and goes through the regular function execution path when running on
other devices.
If False , executes the function without XLA compilation.  Set this value
to False when directly running a multi-device function on TPUs (e.g. two
TPU cores, one TPU core and its host CPU).
Not all functions are compilable, see a list of sharp corners . reduce_retracing When True, tf.function attempts to reduce the
amount of retracing, for example by using more generic shapes. This
can be controlled for user objects by customizing their associated tf.types.experimental.TraceType . experimental_implements If provided, contains a name of a "known" function
this implements. For example "mycompany.my_recurrent_cell".
This is stored as an attribute in inference function,
which can then be detected when processing serialized function.
See standardizing composite ops for details.  For an example of utilizing this attribute see this example The code above automatically detects and substitutes function that
implements "embedded_matmul" and allows TFLite to substitute its own
implementations. For instance, a tensorflow user can use this
 attribute to mark that their function also implements embedded_matmul (perhaps more efficiently!)
by specifying it using this parameter: @tf.function(experimental_implements="embedded_matmul") This can either be specified as just the string name of the function or
a NameAttrList corresponding to a list of key-value attributes associated
with the function name. The name of the function will be in the 'name'
field of the NameAttrList. To define a formal TF op for this function
implements, try the experimental composite TF project. experimental_autograph_options Optional tuple of tf.autograph.experimental.Feature values. experimental_attributes Optional dictionary of attributes to include in the
generated FunctionDefs. experimental_relax_shapes Deprecated. Use reduce_retracing instead. experimental_compile Deprecated alias to 'jit_compile'. experimental_follow_type_hints Deprecated. Please use input_signature or
reduce_retracing instead. Returns If func is not None, returns a tf.types.experimental.PolymorphicFunction .
If func is None, returns a decorator that, when invoked with a single func argument, returns a tf.types.experimental.PolymorphicFunction . Raises ValueError when attempting to use jit_compile=True , but XLA support is
not available.


Page: https://www.tensorflow.org/api_docs/python/tf/gather
View source on GitHub Gather slices from params axis axis according to indices. (deprecated arguments) tf . gather ( params , indices , validate_indices = None , axis = None , batch_dims = 0 , name = None ) Used in the notebooks Used in the guide Used in the tutorials Introduction to graphs and tf.function Introduction to tensor slicing Extension types BERT Preprocessing with TF Text Subword tokenizers Simple audio recognition: Recognizing keywords Image segmentation Multilevel Modeling Primer in TensorFlow Probability MoViNet for streaming action recognition Client-efficient large-model federated learning via `federated_select` and sparse aggregation Deprecated: SOME ARGUMENTS ARE DEPRECATED: (validate_indices) . They will be removed in a future version.
Instructions for updating:
The validate_indices argument has no effect. Indices are always validated on CPU and never validated on GPU. Gather slices from params axis axis according to indices . indices must be an integer tensor of any dimension (often 1-D). Tensor. getitem works for scalars, tf.newaxis , and python slices tf.gather extends indexing to handle tensors of indices. In the simplest case it's identical to scalar indexing: params = tf . constant ([ 'p0' , 'p1' , 'p2' , 'p3' , 'p4' , 'p5' ]) params [ 3 ] . numpy () b 'p3' tf . gather ( params , 3 ) . numpy () b 'p3' The most common case is to pass a single axis tensor of indices (this
can't be expressed as a python slice because the indices are not sequential): indices = [ 2 , 0 , 2 , 5 ] tf . gather ( params , indices ) . numpy () array ([ b 'p2' , b 'p0' , b 'p2' , b 'p5' ], dtype = object ) The indices can have any shape. When the params has 1 axis, the
output shape is equal to the input shape: tf . gather ( params , [[ 2 , 0 ], [ 2 , 5 ]]) . numpy () array ([[ b 'p2' , b 'p0' ], [ b 'p2' , b 'p5' ]], dtype = object ) The params may also have any shape. gather can select slices
across any axis depending on the axis argument (which defaults to 0).
Below it is used to gather first rows, then columns from a matrix: params = tf . constant ([[ 0 , 1.0 , 2.0 ], [ 10.0 , 11.0 , 12.0 ], [ 20.0 , 21.0 , 22.0 ], [ 30.0 , 31.0 , 32.0 ]]) tf . gather ( params , indices = [ 3 , 1 ]) . numpy () array ([[ 30. , 31. , 32. ], [ 10. , 11. , 12. ]], dtype = float32 ) tf . gather ( params , indices = [ 2 , 1 ], axis = 1 ) . numpy () array ([[ 2. , 1. ], [ 12. , 11. ], [ 22. , 21. ], [ 32. , 31. ]], dtype = float32 ) More generally: The output shape has the same shape as the input, with the
indexed-axis replaced by the shape of the indices. def result_shape ( p_shape , i_shape , axis = 0 ): return p_shape [: axis ] + i_shape + p_shape [ axis + 1 :] result_shape ([ 1 , 2 , 3 ], [], axis = 1 ) [ 1 , 3 ] result_shape ([ 1 , 2 , 3 ], [ 7 ], axis = 1 ) [ 1 , 7 , 3 ] result_shape ([ 1 , 2 , 3 ], [ 7 , 5 ], axis = 1 ) [ 1 , 7 , 5 , 3 ] Here are some examples: params . shape . as_list () [ 4 , 3 ] indices = tf . constant ([[ 0 , 2 ]]) tf . gather ( params , indices = indices , axis = 0 ) . shape . as_list () [ 1 , 2 , 3 ] tf . gather ( params , indices = indices , axis = 1 ) . shape . as_list () [ 4 , 1 , 2 ] params = tf . random . normal ( shape = ( 5 , 6 , 7 , 8 )) indices = tf . random . uniform ( shape = ( 10 , 11 ), maxval = 7 , dtype = tf . int32 ) result = tf . gather ( params , indices , axis = 2 ) result . shape . as_list () [ 5 , 6 , 10 , 11 , 8 ] This is because each index takes a slice from params , and
places it at the corresponding location in the output. For the above example # For any location in indices a , b = 0 , 1 tf . reduce_all ( # the corresponding slice of the result result [:, :, a , b , :] == # is equal to the slice of `params` along `axis` at the index. params [:, :, indices [ a , b ], :] ) . numpy () True Batching: The batch_dims argument lets you gather different items from each element
of a batch. Using batch_dims=1 is equivalent to having an outer loop over the first
axis of params and indices : params = tf . constant ([ [ 0 , 0 , 1 , 0 , 2 ], [ 3 , 0 , 0 , 0 , 4 ], [ 0 , 5 , 0 , 6 , 0 ]]) indices = tf . constant ([ [ 2 , 4 ], [ 0 , 4 ], [ 1 , 3 ]]) tf . gather ( params , indices , axis = 1 , batch_dims = 1 ) . numpy () array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], dtype = int32 ) This is equivalent to: def manually_batched_gather ( params , indices , axis ): batch_dims = 1 result = [] for p , i in zip ( params , indices ): r = tf . gather ( p , i , axis = axis - batch_dims ) result . append ( r ) return tf . stack ( result ) manually_batched_gather ( params , indices , axis = 1 ) . numpy () array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], dtype = int32 ) Higher values of batch_dims are equivalent to multiple nested loops over
the outer axes of params and indices . So the overall shape function is def batched_result_shape ( p_shape , i_shape , axis = 0 , batch_dims = 0 ): return p_shape [: axis ] + i_shape [ batch_dims :] + p_shape [ axis + 1 :] batched_result_shape ( p_shape = params . shape . as_list (), i_shape = indices . shape . as_list (), axis = 1 , batch_dims = 1 ) [ 3 , 2 ] tf . gather ( params , indices , axis = 1 , batch_dims = 1 ) . shape . as_list () [ 3 , 2 ] This comes up naturally if you need to use the indices of an operation like tf.argsort , or tf.math.top_k where the last dimension of the indices
indexes into the last dimension of input, at the corresponding location.
In this case you can use tf.gather(values, indices, batch_dims=-1) . See also: tf.Tensor. getitem : The direct tensor index operation ( t[] ), handles
scalars and python-slices tensor[..., 7, 1:-1] tf.scatter : A collection of operations similar to __setitem__ ( t[i] = x ) tf.gather_nd : An operation similar to tf.gather but gathers across
multiple axis at once (it can gather elements of a matrix instead of rows
or columns) tf.boolean_mask , tf.where : Binary indexing. tf.slice and tf.strided_slice : For lower level access to the
implementation of __getitem__ 's python-slice handling ( t[1:-1:2] ) Args params The Tensor from which to gather values. Must be at least rank axis + 1 . indices The index Tensor .  Must be one of the following types: int32 , int64 . The values must be in range [0, params.shape[axis]) . validate_indices Deprecated, does nothing. Indices are always validated on
CPU, never validated on GPU. Caution: On CPU, if an out of bound index is found, an error is raised.
On GPU, if an out of bound index is found, a 0 is stored in the
corresponding output value. axis A Tensor . Must be one of the following types: int32 , int64 . The axis in params to gather indices from. Must be greater than or equal
to batch_dims .  Defaults to the first non-batch dimension. Supports
negative indexes. batch_dims An integer .  The number of batch dimensions.  Must be less
than or equal to rank(indices) . name A name for the operation (optional). Returns A Tensor . Has the same type as params .


Page: https://www.tensorflow.org/api_docs/python/tf/gather_nd
View source on GitHub Gather slices from params into a Tensor with shape specified by indices . tf . gather_nd ( params , indices , batch_dims = 0 , name = None ) Used in the notebooks Used in the guide Used in the tutorials Introduction to tensor slicing Parametrized Quantum Circuits for Reinforcement Learning indices is a Tensor of indices into params . The index vectors are
arranged along the last axis of indices . This is similar to tf.gather , in which indices defines slices into the
first dimension of params . In tf.gather_nd , indices defines slices into
the first N dimensions of params , where N = indices.shape[-1] . Caution: On CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, a 0 is stored in the
corresponding output value. Gathering scalars In the simplest case the vectors in indices index the full rank of params : tf . gather_nd ( indices = [[ 0 , 0 ], [ 1 , 1 ]], params = [[ 'a' , 'b' ], [ 'c' , 'd' ]]) . numpy () array ([ b 'a' , b 'd' ], dtype = object ) In this case the result has 1-axis fewer than indices , and each index vector
is replaced by the scalar indexed from params . In this case the shape relationship is: index_depth = indices . shape [ - 1 ] assert index_depth == params . shape . rank result_shape = indices . shape [: - 1 ] If indices has a rank of K , it is helpful to think indices as a
(K-1)-dimensional tensor of indices into params . Gathering slices If the index vectors do not index the full rank of params then each location
in the result contains a slice of params. This example collects rows from a
matrix: tf . gather_nd ( indices = [[ 1 ], [ 0 ]], params = [[ 'a' , 'b' , 'c' ], [ 'd' , 'e' , 'f' ]]) . numpy () array ([[ b 'd' , b 'e' , b 'f' ], [ b 'a' , b 'b' , b 'c' ]], dtype = object ) Here indices contains [2] index vectors, each with a length of 1 .
The index vectors each refer to rows of the params matrix. Each
row has a shape of [3] so the output shape is [2, 3] . In this case, the relationship between the shapes is: index_depth = indices . shape [ - 1 ] outer_shape = indices . shape [: - 1 ] assert index_depth < = params . shape . rank inner_shape = params . shape [ index_depth :] output_shape = outer_shape + inner_shape It is helpful to think of the results in this case as tensors-of-tensors.
The shape of the outer tensor is set by the leading dimensions of indices .
While the shape of the inner tensors is the shape of a single slice. Batches Additionally, both params and indices can have M leading batch
dimensions that exactly match. In this case batch_dims must be set to M . For example, to collect one row from each of a batch of matrices you could
set the leading elements of the index vectors to be their location in the
batch: tf . gather_nd ( indices = [[ 0 , 1 ], [ 1 , 0 ], [ 2 , 4 ], [ 3 , 2 ], [ 4 , 1 ]], params = tf . zeros ([ 5 , 7 , 3 ])) . shape . as_list () [ 5 , 3 ] The batch_dims argument lets you omit those leading location dimensions
from the index: tf . gather_nd ( batch_dims = 1 , indices = [[ 1 ], [ 0 ], [ 4 ], [ 2 ], [ 1 ]], params = tf . zeros ([ 5 , 7 , 3 ])) . shape . as_list () [ 5 , 3 ] This is equivalent to caling a separate gather_nd for each location in the
batch dimensions. params = tf . zeros ([ 5 , 7 , 3 ]) indices = tf . zeros ([ 5 , 1 ]) batch_dims = 1 index_depth = indices . shape [ - 1 ] batch_shape = indices . shape [: batch_dims ] assert params . shape [: batch_dims ] == batch_shape outer_shape = indices . shape [ batch_dims : - 1 ] assert index_depth < = params . shape . rank inner_shape = params . shape [ batch_dims + index_depth :] output_shape = batch_shape + outer_shape + inner_shape output_shape . as_list () [ 5 , 3 ] More examples Indexing into a 3-tensor: tf . gather_nd ( indices = [[ 1 ]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([[[ b 'a1' , b 'b1' ], [ b 'c1' , b 'd1' ]]], dtype = object ) tf . gather_nd ( indices = [[ 0 , 1 ], [ 1 , 0 ]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([[ b 'c0' , b 'd0' ], [ b 'a1' , b 'b1' ]], dtype = object ) tf . gather_nd ( indices = [[ 0 , 0 , 1 ], [ 1 , 0 , 1 ]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([ b 'b0' , b 'b1' ], dtype = object ) The examples below are for the case when only indices have leading extra
dimensions. If both 'params' and 'indices' have leading batch dimensions, use
the 'batch_dims' parameter to run gather_nd in batch mode. Batched indexing into a matrix: tf . gather_nd ( indices = [[[ 0 , 0 ]], [[ 0 , 1 ]]], params = [[ 'a' , 'b' ], [ 'c' , 'd' ]]) . numpy () array ([[ b 'a' ], [ b 'b' ]], dtype = object ) Batched slice indexing into a matrix: tf . gather_nd ( indices = [[[ 1 ]], [[ 0 ]]], params = [[ 'a' , 'b' ], [ 'c' , 'd' ]]) . numpy () array ([[[ b 'c' , b 'd' ]], [[ b 'a' , b 'b' ]]], dtype = object ) Batched indexing into a 3-tensor: tf . gather_nd ( indices = [[[ 1 ]], [[ 0 ]]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([[[[ b 'a1' , b 'b1' ], [ b 'c1' , b 'd1' ]]], [[[ b 'a0' , b 'b0' ], [ b 'c0' , b 'd0' ]]]], dtype = object ) tf . gather_nd ( indices = [[[ 0 , 1 ], [ 1 , 0 ]], [[ 0 , 0 ], [ 1 , 1 ]]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([[[ b 'c0' , b 'd0' ], [ b 'a1' , b 'b1' ]], [[ b 'a0' , b 'b0' ], [ b 'c1' , b 'd1' ]]], dtype = object ) tf . gather_nd ( indices = [[[ 0 , 0 , 1 ], [ 1 , 0 , 1 ]], [[ 0 , 1 , 1 ], [ 1 , 1 , 0 ]]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([[ b 'b0' , b 'b1' ], [ b 'd0' , b 'c1' ]], dtype = object ) Examples with batched 'params' and 'indices': tf . gather_nd ( batch_dims = 1 , indices = [[ 1 ], [ 0 ]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([[ b 'c0' , b 'd0' ], [ b 'a1' , b 'b1' ]], dtype = object ) tf . gather_nd ( batch_dims = 1 , indices = [[[ 1 ]], [[ 0 ]]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([[[ b 'c0' , b 'd0' ]], [[ b 'a1' , b 'b1' ]]], dtype = object ) tf . gather_nd ( batch_dims = 1 , indices = [[[ 1 , 0 ]], [[ 0 , 1 ]]], params = [[[ 'a0' , 'b0' ], [ 'c0' , 'd0' ]], [[ 'a1' , 'b1' ], [ 'c1' , 'd1' ]]]) . numpy () array ([[ b 'c0' ], [ b 'b1' ]], dtype = object ) See also tf.gather . Args params A Tensor . The tensor from which to gather values. indices A Tensor . Must be one of the following types: int32 , int64 .
Index tensor. name A name for the operation (optional). batch_dims An integer or a scalar 'Tensor'. The number of batch dimensions. Returns A Tensor . Has the same type as params .


Page: https://www.tensorflow.org/api_docs/python/tf/get_current_name_scope
View source on GitHub Returns current full name scope specified by tf.name_scope(...) s. tf . get_current_name_scope () -> str For example, with tf . name_scope ( "outer" ): tf . get_current_name_scope () # "outer" with tf . name_scope ( "inner" ): tf . get_current_name_scope () # "outer/inner" In other words, tf.get_current_name_scope() returns the op name prefix that
will be prepended to, if an op is created at that place. Note that @tf.function resets the name scope stack as shown below. with tf . name_scope ( "outer" ): @tf . function def foo ( x ): with tf . name_scope ( "inner" ): return tf . add ( x * x ) # Op name is "inner/Add", not "outer/inner/Add"


Page: https://www.tensorflow.org/api_docs/python/tf/get_logger
View source on GitHub Return TF logger instance. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.get_logger tf . get_logger () Used in the notebooks Used in the guide Used in the tutorials Migrating your TFLite code to TF2 Customizing MinDiffModel Integrating MinDiff with MinDiffModel Integrating MinDiff without MinDiffModel Subword tokenizers TensorFlow Hub Object Detection Colab Implement Differential Privacy with TensorFlow Privacy Parametrized Quantum Circuits for Reinforcement Learning Object Detection with TensorFlow Lite Model Maker Text classification with TensorFlow Lite Model Maker Returns An instance of the Python logging library Logger. See Python documentation ( https://docs.python.org/3/library/logging.html )
for detailed API. Below is only a summary. The logger has 5 levels of logging from the most serious to the least: FATAL ERROR WARN INFO DEBUG The logger has the following methods, based on these logging levels: fatal(msg, *args, **kwargs) error(msg, *args, **kwargs) warn(msg, *args, **kwargs) info(msg, *args, **kwargs) debug(msg, *args, **kwargs) The msg can contain string formatting.  An example of logging at the ERROR level
using string formating is: tf . get_logger () . error ( "The value %d is invalid." , 3 ) You can also specify the logging verbosity.  In this case, the
WARN level log will not be emitted: tf . get_logger () . setLevel ( ERROR ) tf . get_logger () . warn ( "This is a warning." )


Page: https://www.tensorflow.org/api_docs/python/tf/get_static_value
View source on GitHub Returns the constant value of the given tensor, if efficiently calculable. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.get_static_value tf . get_static_value ( tensor , partial = False ) Used in the notebooks Used in the tutorials Bayesian Modeling with Joint Distribution This function attempts to partially evaluate the given tensor, and
returns its value as a numpy ndarray if this succeeds. Example usage: a = tf . constant ( 10 ) tf . get_static_value ( a ) 10 b = tf . constant ( 20 ) tf . get_static_value ( tf . add ( a , b )) 30 # `tf.Variable` is not supported. c = tf . Variable ( 30 ) print ( tf . get_static_value ( c )) None Using partial option is most relevant when calling get_static_value inside
a tf.function . Setting it to True will return the results but for the
values that cannot be evaluated will be None . For example: class Foo : def __init__ ( self ): self . a = tf . Variable ( 1 ) self . b = tf . constant ( 2 ) @tf . function def bar ( self , partial ): packed = tf . raw_ops . Pack ( values = [ self . a , self . b ]) static_val = tf . get_static_value ( packed , partial = partial ) tf . print ( static_val ) f = Foo () f . bar ( partial = True ) # `array([None, array(2, dtype=int32)], dtype=object)` f . bar ( partial = False ) # `None` Compatibility(V1): If constant_value(tensor) returns a non- None result, it
will no longer be possible to feed a different value for tensor . This allows
the result of this function to influence the graph that is constructed, and
permits static shape optimizations. Args tensor The Tensor to be evaluated. partial If True, the returned numpy array is allowed to have partially
evaluated values. Values that can't be evaluated will be None. Returns A numpy ndarray containing the constant value of the given tensor ,
or None if it cannot be calculated. Raises TypeError if tensor is not an tensor.Tensor.


Page: https://www.tensorflow.org/api_docs/python/tf/grad_pass_through
View source on GitHub Creates a grad-pass-through op with the forward behavior provided in f. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.grad_pass_through tf . grad_pass_through ( f ) Use this function to wrap any op, maintaining its behavior in the forward
pass, but replacing the original op in the backward graph with an identity.
For example: x = tf . Variable ( 1.0 , name = "x" ) z = tf . Variable ( 3.0 , name = "z" ) with tf . GradientTape () as tape : # y will evaluate to 9.0 y = tf . grad_pass_through ( x . assign )( z ** 2 ) # grads will evaluate to 6.0 grads = tape . gradient ( y , z ) Another example is a 'differentiable' moving average approximation, where
gradients are allowed to flow into the last value fed to the moving average,
but the moving average is still used for the forward pass: x = ... # Some scalar value # A moving average object, we don't need to know how this is implemented moving_average = MovingAverage () with backprop . GradientTape () as tape : # mavg_x will evaluate to the current running average value mavg_x = tf . grad_pass_through ( moving_average )( x ) grads = tape . gradient ( mavg_x , x ) # grads will evaluate to 1.0 Args f function f(*x) that returns a Tensor or nested structure of Tensor outputs. Returns A function h(x) which returns the same values as f(x) and whose
gradients are the same as those of an identity function.


Page: https://www.tensorflow.org/api_docs/python/tf/gradients
View source on GitHub Constructs symbolic derivatives of sum of ys w.r.t. x in xs . tf . gradients ( ys , xs , grad_ys = None , name = 'gradients' , gate_gradients = False , aggregation_method = None , stop_gradients = None , unconnected_gradients = tf . UnconnectedGradients . NONE ) Used in the notebooks Used in the tutorials Generalized Linear Models tf.gradients is only valid in a graph context. In particular,
it is valid in the context of a tf.function wrapper, where code
is executing as a graph. ys and xs are each a Tensor or a list of tensors. grad_ys is a list of Tensor , holding the gradients received by the ys . The list must be the same length as ys . gradients() adds ops to the graph to output the derivatives of ys with
respect to xs .  It returns a list of Tensor of length len(xs) where
each tensor is the sum(dy/dx) for y in ys and for x in xs . grad_ys is a list of tensors of the same length as ys that holds
the initial gradients for each y in ys .  When grad_ys is None,
we fill in a tensor of '1's of the shape of y for each y in ys .  A
user can provide their own initial grad_ys to compute the
derivatives using a different initial gradient for each y (e.g., if
one wanted to weight the gradient differently for each value in
each y). stop_gradients is a Tensor or a list of tensors to be considered constant
with respect to all xs . These tensors will not be backpropagated through,
as though they had been explicitly disconnected using stop_gradient .  Among
other things, this allows computation of partial derivatives as opposed to
total derivatives. For example: @tf . function def example (): a = tf . constant ( 0. ) b = 2 * a return tf . gradients ( a + b , [ a , b ], stop_gradients = [ a , b ]) example () [ < tf . Tensor : shape = (), dtype = float32 , numpy = 1.0 > , < tf . Tensor : shape = (), dtype = float32 , numpy = 1.0 > ] Here the partial derivatives g evaluate to [1.0, 1.0] , compared to the
total derivatives tf.gradients(a + b, [a, b]) , which take into account the
influence of a on b and evaluate to [3.0, 1.0] .  Note that the above is
equivalent to: @tf . function def example (): a = tf . stop_gradient ( tf . constant ( 0. )) b = tf . stop_gradient ( 2 * a ) return tf . gradients ( a + b , [ a , b ]) example () [ < tf . Tensor : shape = (), dtype = float32 , numpy = 1.0 > , < tf . Tensor : shape = (), dtype = float32 , numpy = 1.0 > ] stop_gradients provides a way of stopping gradient after the graph has
already been constructed, as compared to tf.stop_gradient which is used
during graph construction.  When the two approaches are combined,
backpropagation stops at both tf.stop_gradient nodes and nodes in stop_gradients , whichever is encountered first. All integer tensors are considered constant with respect to all xs , as if
they were included in stop_gradients . unconnected_gradients determines the value returned for each x in xs if it
is unconnected in the graph to ys. By default this is None to safeguard
against errors. Mathematically these gradients are zero which can be requested
using the 'zero' option. tf.UnconnectedGradients provides the
following options and behaviors: @tf . function def example ( use_zero ): a = tf . ones ([ 1 , 2 ]) b = tf . ones ([ 3 , 1 ]) if use_zero : return tf . gradients ([ b ], [ a ], unconnected_gradients = 'zero' ) else : return tf . gradients ([ b ], [ a ], unconnected_gradients = 'none' ) example ( False ) [ None ] example ( True ) [ < tf . Tensor : shape = ( 1 , 2 ), dtype = float32 , numpy = array ([[ 0. , 0. ]], ... )>] Let us take one practical example which comes during the back propogation
phase. This function is used to evaluate the derivatives of the cost function
with respect to Weights Ws and Biases bs . Below sample implementation
provides the exaplantion of what it is actually used for : @tf . function def example (): Ws = tf . constant ( 0. ) bs = 2 * Ws cost = Ws + bs # This is just an example. Please ignore the formulas. g = tf . gradients ( cost , [ Ws , bs ]) dCost_dW , dCost_db = g return dCost_dW , dCost_db example () ( < tf . Tensor : shape = (), dtype = float32 , numpy = 3.0 > , < tf . Tensor : shape = (), dtype = float32 , numpy = 1.0 > ) Args ys A Tensor or list of tensors to be differentiated. xs A Tensor or list of tensors to be used for differentiation. grad_ys Optional. A Tensor or list of tensors the same size as ys and holding the gradients computed for each y in ys . name Optional name to use for grouping all the gradient ops together.
defaults to 'gradients'. gate_gradients If True, add a tuple around the gradients returned
for an operations.  This avoids some race conditions. aggregation_method Specifies the method used to combine gradient terms.
Accepted values are constants defined in the class AggregationMethod . stop_gradients Optional. A Tensor or list of tensors not to differentiate
through. unconnected_gradients Optional. Specifies the gradient value returned when
the given input tensors are unconnected. Accepted values are constants
defined in the class tf.UnconnectedGradients and the default value is none . Returns A list of Tensor of length len(xs) where each tensor is the sum(dy/dx) for y in ys and for x in xs . Raises LookupError if one of the operations between x and y does not
have a registered gradient function. ValueError if the arguments are invalid. RuntimeError if called in Eager mode.


Page: https://www.tensorflow.org/api_docs/python/tf/math/greater
Returns the truth value of (x > y) element-wise. View aliases Main aliases tf.greater Compat aliases for migration See Migration guide for
more details. tf.compat.v1.greater , tf.compat.v1.math.greater tf . math . greater ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the guide Used in the tutorials Extension types Introduction to graphs and tf.function Parameter server training with ParameterServerStrategy TFX Estimator Component Tutorial TFX Keras Component Tutorial Note: math.greater supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 2 , 5 ]) tf . math . greater ( x , y ) == > [ False , True , True ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . greater ( x , y ) == > [ False , False , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool .


Page: https://www.tensorflow.org/api_docs/python/tf/math/greater_equal
Returns the truth value of (x >= y) element-wise. View aliases Main aliases tf.greater_equal Compat aliases for migration See Migration guide for
more details. tf.compat.v1.greater_equal , tf.compat.v1.math.greater_equal tf . math . greater_equal ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the tutorials TensorFlow Ranking Keras pipeline for distributed training Note: math.greater_equal supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 , 7 ]) y = tf . constant ([ 5 , 2 , 5 , 10 ]) tf . math . greater_equal ( x , y ) == > [ True , True , True , False ] x = tf . constant ([ 5 , 4 , 6 , 7 ]) y = tf . constant ([ 5 ]) tf . math . greater_equal ( x , y ) == > [ True , False , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool .


Page: https://www.tensorflow.org/api_docs/python/tf/group
View source on GitHub Create an op that groups multiple operations. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.group tf . group ( * inputs , ** kwargs ) Used in the notebooks Used in the guide Used in the tutorials Migrating model checkpoints Estimators Validating correctness & numerical equivalence Wiki40B Language Models When this op finishes, all ops in inputs have finished. This op has no
output. Note: In TensorFlow 2 with eager and/or Autograph, you should not require
this method, as ops execute in the expected order thanks to automatic control
dependencies. Only use tf.group when working with v1 tf.Graph code. When operating in a v1-style graph context, ops are not executed in the same
order as specified in the code; TensorFlow will attempt to execute ops in
parallel or in an order convenient to the result it is computing. tf.group allows you to request that one or more results finish before execution
continues. tf.group creates a single op (of type NoOp ), and then adds appropriate
control dependencies.  Thus, c = tf.group(a, b) will compute the same graph
as this: with tf . control_dependencies ([ a , b ]): c = tf . no_op () See also tf.tuple and tf.control_dependencies . Args *inputs Zero or more tensors to group. name A name for this operation (optional). Returns An Operation that executes all its inputs. Raises ValueError If an unknown keyword argument is provided.


Page: https://www.tensorflow.org/api_docs/python/tf/guarantee_const
View source on GitHub Promise to the TF runtime that the input tensor is a constant. (deprecated) View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.guarantee_const tf . guarantee_const ( input , name = None ) Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.
Instructions for updating:
Not for public use. The runtime is then free to make optimizations based on this. Returns the input tensor without modification. Args input A Tensor . name A name for this operation. Returns A Tensor . Has the same dtype as input .


Page: https://www.tensorflow.org/api_docs/python/tf/hessians
View source on GitHub Constructs the Hessian of sum of ys with respect to x in xs . tf . hessians ( ys , xs , gate_gradients = False , aggregation_method = None , name = 'hessians' ) Used in the notebooks Used in the tutorials Generalized Linear Models hessians() adds ops to the graph to output the Hessian matrix of ys with respect to xs .  It returns a list of Tensor of length len(xs) where each tensor is the Hessian of sum(ys) . The Hessian is a matrix of second-order partial derivatives of a scalar
tensor (see https://en.wikipedia.org/wiki/Hessian_matrix for more details). Args ys A Tensor or list of tensors to be differentiated. xs A Tensor or list of tensors to be used for differentiation. gate_gradients See gradients() documentation for details. aggregation_method See gradients() documentation for details. name Optional name to use for grouping all the gradient ops together.
defaults to 'hessians'. Returns A list of Hessian matrices of sum(ys) for each x in xs . Raises LookupError if one of the operations between xs and ys does not
have a registered gradient function.


Page: https://www.tensorflow.org/api_docs/python/tf/histogram_fixed_width
View source on GitHub Return histogram of values. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.histogram_fixed_width tf . histogram_fixed_width ( values , value_range , nbins = 100 , dtype = tf . dtypes . int32 , name = None ) Given the tensor values , this operation returns a rank 1 histogram counting
the number of entries in values that fell into every bin.  The bins are
equal width and determined by the arguments value_range and nbins . Args values Numeric Tensor . value_range Shape [2] Tensor of same dtype as values .
values <= value_range[0] will be mapped to hist[0],
values >= value_range[1] will be mapped to hist[-1]. nbins Scalar int32 Tensor .  Number of histogram bins. dtype dtype for returned histogram. name A name for this operation (defaults to 'histogram_fixed_width'). Returns A 1-D Tensor holding histogram of values. Raises TypeError If any unsupported dtype is provided. tf.errors.InvalidArgumentError If value_range does not
satisfy value_range[0] < value_range[1]. Examples: # Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf) nbins = 5 value_range = [ 0.0 , 5.0 ] new_values = [ - 1.0 , 0.0 , 1.5 , 2.0 , 5.0 , 15 ] hist = tf . histogram_fixed_width ( new_values , value_range , nbins = 5 ) hist . numpy () array ([ 2 , 1 , 1 , 0 , 2 ], dtype = int32 )


Page: https://www.tensorflow.org/api_docs/python/tf/histogram_fixed_width_bins
View source on GitHub Bins the given values for use in a histogram. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.histogram_fixed_width_bins tf . histogram_fixed_width_bins ( values , value_range , nbins = 100 , dtype = tf . dtypes . int32 , name = None ) Given the tensor values , this operation returns a rank 1 Tensor representing the indices of a histogram into which each element
of values would be binned. The bins are equal width and
determined by the arguments value_range and nbins . Args values Numeric Tensor . value_range Shape [2] Tensor of same dtype as values .
values <= value_range[0] will be mapped to hist[0],
values >= value_range[1] will be mapped to hist[-1]. nbins Scalar int32 Tensor .  Number of histogram bins. dtype dtype for returned histogram. name A name for this operation (defaults to 'histogram_fixed_width'). Returns A Tensor holding the indices of the binned values whose shape matches values . Raises TypeError If any unsupported dtype is provided. tf.errors.InvalidArgumentError If value_range does not
satisfy value_range[0] < value_range[1]. Examples: # Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf) nbins = 5 value_range = [ 0.0 , 5.0 ] new_values = [ - 1.0 , 0.0 , 1.5 , 2.0 , 5.0 , 15 ] indices = tf . histogram_fixed_width_bins ( new_values , value_range , nbins = 5 ) indices . numpy () array ([ 0 , 0 , 1 , 2 , 4 , 4 ], dtype = int32 )


Page: https://www.tensorflow.org/api_docs/python/tf/identity
View source on GitHub Return a Tensor with the same shape and contents as input. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.identity tf . identity ( input , name = None ) Used in the notebooks Used in the guide Used in the tutorials Migrate LoggingTensorHook and StopAtStepHook to Keras callbacks Substantial Undocumented Infection Facilitates the Rapid Dissemination of Novel Coronavirus (SARS-CoV2) The return value is not the same Tensor as the original, but contains the same
values.  This operation is fast when used on the same device. For example: a = tf . constant ([ 0.78 ]) a_identity = tf . identity ( a ) a . numpy () array ([ 0.78 ], dtype = float32 ) a_identity . numpy () array ([ 0.78 ], dtype = float32 ) Calling tf.identity on a variable will make a Tensor that represents the
value of that variable at the time it is called. This is equivalent to calling <variable>.read_value() . a = tf . Variable ( 5 ) a_identity = tf . identity ( a ) a . assign_add ( 1 ) < tf . Variable ... shape = () dtype = int32 , numpy = 6 > a . numpy () 6 a_identity . numpy () 5 This function can also be used to explicitly transfer tensors between devices.
For example, to transfer a tensor in GPU memory back to host memory, one can
use: with tf . device ( "/gpu:0" ): x_on_gpu = tf . constant ( 1 ) with tf . device ( "/cpu:0" ): x_on_cpu = tf . identity ( x_on_gpu ) x_on_cpu . device '/job:localhost/replica:0/task:0/device:CPU:0' Args input A Tensor , a Variable , a CompositeTensor or anything that can be
converted to a tensor using tf.convert_to_tensor . name A name for the operation (optional). Returns A Tensor or CompositeTensor. Has the same type and contents as input .


Page: https://www.tensorflow.org/api_docs/python/tf/identity_n
Returns a list of tensors with the same shapes and contents as the input View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.identity_n tf . identity_n ( input , name = None ) tensors. This op can be used to override the gradient for complicated functions. For
example, suppose y = f(x) and we wish to apply a custom function g for backprop
such that dx = g(dy). In Python, with tf . get_default_graph () . gradient_override_map ( { 'IdentityN' : 'OverrideGradientWithG' }): y , _ = identity_n ([ f ( x ), x ]) @tf . RegisterGradient ( 'OverrideGradientWithG' ) def ApplyG ( op , dy , _ ): return [ None , g ( dy )] # Do not backprop to f(x). Args input A list of Tensor objects. name A name for the operation (optional). Returns A list of Tensor objects. Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/ifftnd
ND inverse fast Fourier transform. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.ifftnd tf . ifftnd ( input : Annotated [ Any , TV_IFFTND_Tcomplex ], fft_length : Annotated [ Any , _atypes . Int32 ], axes : Annotated [ Any , _atypes . Int32 ], name = None ) -> Annotated [ Any , TV_IFFTND_Tcomplex ] Computes the n-dimensional inverse discrete Fourier transform over designated
dimensions of input . The designated dimensions of input are assumed to be
the result of IFFTND . If fft_length[i] shape(input)[i], the input is padded with zeros. If fft_length
is not given, the default shape(input) is used. Axes mean the dimensions to perform the transform on. Default is to perform on
all axes. Args input A Tensor . Must be one of the following types: complex64 , complex128 .
A complex tensor. fft_length A Tensor of type int32 .
An int32 tensor. The FFT length for each dimension. axes A Tensor of type int32 .
An int32 tensor with a same shape as fft_length. Axes to perform the transform. name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/graph_util/import_graph_def
View source on GitHub Imports the graph from graph_def into the current default Graph . (deprecated arguments) View aliases Main aliases tf.import_graph_def Compat aliases for migration See Migration guide for
more details. tf.compat.v1.graph_util.import_graph_def , tf.compat.v1.import_graph_def tf . graph_util . import_graph_def ( graph_def , input_map = None , return_elements = None , name = None , op_dict = None , producer_op_list = None ) Used in the notebooks Used in the guide Migrating your TFLite code to TF2 Deprecated: SOME ARGUMENTS ARE DEPRECATED: (op_dict) . They will be removed in a future version.
Instructions for updating:
Please file an issue at https://github.com/tensorflow/tensorflow/issues if you depend on this feature. This function provides a way to import a serialized TensorFlow GraphDef protocol buffer, and extract individual objects in the GraphDef as tf.Tensor and tf.Operation objects. Once extracted,
these objects are placed into the current default Graph . See tf.Graph.as_graph_def for a way to create a GraphDef proto. Args graph_def A GraphDef proto containing operations to be imported into
the default graph. input_map A dictionary mapping input names (as strings) in graph_def to Tensor objects. The values of the named input tensors in the
imported graph will be re-mapped to the respective Tensor values. return_elements A list of strings containing operation names in graph_def that will be returned as Operation objects; and/or
tensor names in graph_def that will be returned as Tensor objects. name (Optional.) A prefix that will be prepended to the names in graph_def . Note that this does not apply to imported function names.
Defaults to "import" . op_dict (Optional.) Deprecated, do not use. producer_op_list (Optional.) An OpList proto with the (possibly stripped)
list of OpDef s used by the producer of the graph. If provided,
unrecognized attrs for ops in graph_def that have their default value
according to producer_op_list will be removed. This will allow some more GraphDef s produced by later binaries to be accepted by earlier binaries. Returns A list of Operation and/or Tensor objects from the imported graph,
corresponding to the names in return_elements ,
and None if returns_elements is None. Raises TypeError If graph_def is not a GraphDef proto, input_map is not a dictionary mapping strings to Tensor objects,
or return_elements is not a list of strings. ValueError If input_map , or return_elements contains names that
do not appear in graph_def , or graph_def is not well-formed (e.g.
it refers to an unknown tensor).


Page: https://www.tensorflow.org/api_docs/python/tf/init_scope
View source on GitHub A context manager that lifts ops out of control-flow scopes and function-building graphs. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.init_scope @tf_contextlib . contextmanager tf . init_scope () -> Iterator [ None ] Used in the notebooks Used in the guide Used in the tutorials Better performance with tf.function Preprocessing data with TensorFlow Transform There is often a need to lift variable initialization ops out of control-flow
scopes, function-building graphs, and gradient tapes. Entering an init_scope is a mechanism for satisfying these desiderata. In particular,
entering an init_scope has three effects: (1) All control dependencies are cleared the moment the scope is entered;
      this is equivalent to entering the context manager returned from control_dependencies(None) , which has the side-effect of exiting
      control-flow scopes like tf.cond and tf.while_loop . (2) All operations that are created while the scope is active are lifted
      into the lowest context on the context_stack that is not building a
      graph function. Here, a context is defined as either a graph or an eager
      context. Every context switch, i.e., every installation of a graph as
      the default graph and every switch into eager mode, is logged in a
      thread-local stack called context_switches ; the log entry for a
      context switch is popped from the stack when the context is exited.
      Entering an init_scope is equivalent to crawling up context_switches , finding the first context that is not building a
      graph function, and entering it. A caveat is that if graph mode is
      enabled but the default graph stack is empty, then entering an init_scope will simply install a fresh graph as the default one. (3) The gradient tape is paused while the scope is active. When eager execution is enabled, code inside an init_scope block runs with
eager execution enabled even when tracing a tf.function . For example: tf . compat . v1 . enable_eager_execution () @tf . function def func (): # A function constructs TensorFlow graphs, # it does not execute eagerly. assert not tf . executing_eagerly () with tf . init_scope (): # Initialization runs with eager execution enabled assert tf . executing_eagerly () Raises RuntimeError if graph state is incompatible with this initialization.


Page: https://www.tensorflow.org/api_docs/python/tf/inside_function
View source on GitHub Indicates whether the caller code is executing inside a tf.function . tf . inside_function () -> bool Returns Boolean, True if the caller code is executing inside a tf.function rather than eagerly. Example: tf . inside_function () False @tf . function def f (): print ( tf . inside_function ()) f () True


Page: https://www.tensorflow.org/api_docs/python/tf/irfftnd
ND inverse real fast Fourier transform. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.irfftnd tf . irfftnd ( input : Annotated [ Any , TV_IRFFTND_Tcomplex ], fft_length : Annotated [ Any , _atypes . Int32 ], axes : Annotated [ Any , _atypes . Int32 ], Treal : TV_IRFFTND_Treal = tf . dtypes . float32 , name = None ) -> Annotated [ Any , TV_IRFFTND_Treal ] Computes the n-dimensional inverse real discrete Fourier transform over
designated dimensions of input . The designated dimensions of input are
assumed to be the result of IRFFTND . The inner-most dimension contains the fft_length / 2 + 1 unique components of the DFT of a real-valued signal. If fft_length[i] shape(input)[i], the input is padded with zeros. If fft_length
is not given, the default shape(input) is used. Axes mean the dimensions to perform the transform on. Default is to perform on
all axes. Args input A Tensor . Must be one of the following types: complex64 , complex128 .
A complex tensor. fft_length A Tensor of type int32 .
An int32 tensor. The FFT length for each dimension. axes A Tensor of type int32 .
An int32 tensor with a same shape as fft_length. Axes to perform the transform. Treal An optional tf.DType from: tf.float32, tf.float64 . Defaults to tf.float32 . name A name for the operation (optional). Returns A Tensor of type Treal .


Page: https://www.tensorflow.org/api_docs/python/tf/is_symbolic_tensor
View source on GitHub Test if tensor is a symbolic Tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.is_symbolic_tensor tf . is_symbolic_tensor ( tensor ) -> bool Args tensor a tensor-like object Returns True if tensor is a symbolic tensor (not an eager tensor).


Page: https://www.tensorflow.org/api_docs/python/tf/is_tensor
View source on GitHub Checks whether x is a TF-native type that can be passed to many TF ops. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.is_tensor tf . is_tensor ( x ) Use is_tensor to differentiate types that can ingested by TensorFlow ops
without any conversion (e.g., tf.Tensor , tf.SparseTensor , and tf.RaggedTensor ) from types that need to be converted into tensors before
they are ingested (e.g., numpy ndarray and Python scalars). For example, in the following code block: if not tf . is_tensor ( t ): t = tf . convert_to_tensor ( t ) return t . shape , t . dtype we check to make sure that t is a tensor (and convert it if not) before
accessing its shape and dtype .  (But note that not all TensorFlow native
types have shapes or dtypes; tf.data.Dataset is an example of a TensorFlow
native type that has neither shape nor dtype.) Args x A python object to check. Returns True if x is a TensorFlow-native type.


Page: https://www.tensorflow.org/api_docs/python/tf/math/less
Returns the truth value of (x < y) element-wise. View aliases Main aliases tf.less Compat aliases for migration See Migration guide for
more details. tf.compat.v1.less , tf.compat.v1.math.less tf . math . less ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the guide Used in the tutorials Extension types Intro to Autoencoders Note: math.less supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . less ( x , y ) == > [ False , True , False ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 6 , 7 ]) tf . math . less ( x , y ) == > [ False , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool .


Page: https://www.tensorflow.org/api_docs/python/tf/math/less_equal
Returns the truth value of (x <= y) element-wise. View aliases Main aliases tf.less_equal Compat aliases for migration See Migration guide for
more details. tf.compat.v1.less_equal , tf.compat.v1.math.less_equal tf . math . less_equal ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the tutorials Federated Reconstruction for Matrix Factorization Note: math.less_equal supports broadcasting. More about broadcasting here Example: x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 ]) tf . math . less_equal ( x , y ) == > [ True , True , False ] x = tf . constant ([ 5 , 4 , 6 ]) y = tf . constant ([ 5 , 6 , 6 ]) tf . math . less_equal ( x , y ) == > [ True , True , True ] Args x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor of type bool .


Page: https://www.tensorflow.org/api_docs/python/tf/linspace
View source on GitHub Generates evenly-spaced values in an interval along a given axis. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.lin_space , tf.compat.v1.linspace tf . linspace ( start , stop , num , name = None , axis = 0 ) Used in the notebooks Used in the guide Used in the tutorials Multilayer perceptrons for digit recognition with Core APIs Advanced automatic differentiation Introduction to gradients and automatic differentiation Basic training loops TensorFlow basics Integrated gradients Learned data compression Basic regression: Predict fuel efficiency Scalable model compression TFP Release Notes notebook (0.12.1) A sequence of num evenly-spaced values are generated beginning at start along a given axis .
If num > 1 , the values in the sequence increase by (stop - start) / (num - 1) , so that the last one is exactly stop .
If num <= 0 , ValueError is raised. Matches np.linspace 's
behaviour
except when num == 0 . For example: tf . linspace ( 10.0 , 12.0 , 3 , name = "linspace" ) = > [ 10.0 11.0 12.0 ] Start and stop can be tensors of arbitrary size: tf . linspace ([ 0. , 5. ], [ 10. , 40. ], 5 , axis = 0 ) < tf . Tensor : shape = ( 5 , 2 ), dtype = float32 , numpy = array ([[ 0. , 5. ], [ 2.5 , 13.75 ], [ 5. , 22.5 ], [ 7.5 , 31.25 ], [ 10. , 40. ]], dtype = float32 ) > Axis is where the values will be generated (the dimension in the
returned tensor which corresponds to the axis will be equal to num ) tf . linspace ([ 0. , 5. ], [ 10. , 40. ], 5 , axis =- 1 ) < tf . Tensor : shape = ( 2 , 5 ), dtype = float32 , numpy = array ([[ 0. , 2.5 , 5. , 7.5 , 10. ], [ 5. , 13.75 , 22.5 , 31.25 , 40. ]], dtype = float32 ) > Args start A Tensor . Must be one of the following types: bfloat16 , float32 , float64 . N-D tensor. First entry in the range. stop A Tensor . Must have the same type and shape as start . N-D tensor.
Last entry in the range. num A Tensor . Must be one of the following types: int32 , int64 . 0-D
tensor. Number of values to generate. name A name for the operation (optional). axis Axis along which the operation is performed (used only when N-D
tensors are provided). Returns A Tensor . Has the same type as start .


Page: https://www.tensorflow.org/api_docs/python/tf/load_library
View source on GitHub Loads a TensorFlow plugin. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.load_library tf . load_library ( library_location ) "library_location" can be a path to a specific shared object, or a folder.
If it is a folder, all shared objects that are named "libtfkernel*" will be
loaded. When the library is loaded, kernels registered in the library via the REGISTER_* macros are made available in the TensorFlow process. Args library_location Path to the plugin or the folder of plugins.
Relative or absolute filesystem path to a dynamic library file or folder. Returns None Raises OSError When the file to be loaded is not found. RuntimeError when unable to load the library.


Page: https://www.tensorflow.org/api_docs/python/tf/load_op_library
View source on GitHub Loads a TensorFlow plugin, containing custom ops and kernels. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.load_op_library tf . load_op_library ( library_filename ) Pass "library_filename" to a platform-specific mechanism for dynamically
loading a library. The rules for determining the exact location of the
library are platform-specific and are not documented here. When the
library is loaded, ops and kernels registered in the library via the REGISTER_* macros are made available in the TensorFlow process. Note
that ops with the same name as an existing op are rejected and not
registered with the process. Args library_filename Path to the plugin.
Relative or absolute filesystem path to a dynamic library file. Returns A python module containing the Python wrappers for Ops defined in
the plugin. Raises RuntimeError when unable to load the library or get the python wrappers.


Page: https://www.tensorflow.org/api_docs/python/tf/math/logical_and
Returns the truth value of x AND y element-wise. View aliases Main aliases tf.logical_and Compat aliases for migration See Migration guide for
more details. tf.compat.v1.logical_and , tf.compat.v1.math.logical_and tf . math . logical_and ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the tutorials Tutorial on Multi Armed Bandits in TF-Agents Logical AND function. Requires that x and y have the same shape or have broadcast-compatible shapes. For example, x and y can be: Two single elements of type bool . One tf.Tensor of type bool and one single bool , where the result will
be calculated by applying logical AND with the single element to each
element in the larger Tensor. Two tf.Tensor objects of type bool of the same shape. In this case,
the result will be the element-wise logical AND of the two input tensors. You can also use the & operator instead. Usage >>> a = tf . constant ([ True ]) >>> b = tf . constant ([ False ]) >>> tf . math . logical_and ( a , b ) < tf . Tensor : shape = ( 1 ,), dtype = bool , numpy = array ([ False ]) >
>>> a & b < tf . Tensor : shape = ( 1 ,), dtype = bool , numpy = array ([ False ]) > c = tf . constant ([ True ]) x = tf . constant ([ False , True , True , False ]) tf . math . logical_and ( c , x ) < tf . Tensor : shape = ( 4 ,), dtype = bool , numpy = array ([ False , True , True , False ]) > c & x < tf . Tensor : shape = ( 4 ,), dtype = bool , numpy = array ([ False , True , True , False ]) > y = tf . constant ([ False , False , True , True ]) z = tf . constant ([ False , True , False , True ]) tf . math . logical_and ( y , z ) < tf . Tensor : shape = ( 4 ,), dtype = bool , numpy = array ([ False , False , False , True ]) > y & z < tf . Tensor : shape = ( 4 ,), dtype = bool , numpy = array ([ False , False , False , True ]) > This op also supports broadcasting tf . logical_and ([[ True , False ]], [[ True ], [ False ]]) < tf . Tensor : shape = ( 2 , 2 ), dtype = bool , numpy = array ([[ True , False ], [ False , False ]]) > The reduction version of this elementwise operation is tf.math.reduce_all . Args x A tf.Tensor of type bool. y A tf.Tensor of type bool. name A name for the operation (optional). Returns A tf.Tensor of type bool with the shape that x and y broadcast to. Args x A Tensor of type bool . y A Tensor of type bool . name A name for the operation (optional). Returns A Tensor of type bool .


Page: https://www.tensorflow.org/api_docs/python/tf/math/logical_not
Returns the truth value of NOT x element-wise. View aliases Main aliases tf.logical_not Compat aliases for migration See Migration guide for
more details. tf.compat.v1.logical_not , tf.compat.v1.math.logical_not tf . math . logical_not ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the tutorials Private Heavy Hitters Client-efficient large-model federated learning via `federated_select` and sparse aggregation Example: tf . math . logical_not ( tf . constant ([ True , False ])) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ False , True ]) > Args x A Tensor of type bool . A Tensor of type bool . name A name for the operation (optional). Returns A Tensor of type bool .


Page: https://www.tensorflow.org/api_docs/python/tf/math/logical_or
Returns the truth value of x OR y element-wise. View aliases Main aliases tf.logical_or Compat aliases for migration See Migration guide for
more details. tf.compat.v1.logical_or , tf.compat.v1.math.logical_or tf . math . logical_or ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Logical OR function. Requires that x and y have the same shape or have broadcast-compatible shapes. For example, x and y can be: Two single elements of type bool . One tf.Tensor of type bool and one single bool , where the result will
be calculated by applying logical OR with the single element to each
element in the larger Tensor. Two tf.Tensor objects of type bool of the same shape. In this case,
the result will be the element-wise logical OR of the two input tensors. You can also use the | operator instead. Usage >>> a = tf . constant ([ True ]) >>> b = tf . constant ([ False ]) >>> tf . math . logical_or ( a , b ) < tf . Tensor : shape = ( 1 ,), dtype = bool , numpy = array ([ True ]) >
>>> a | b < tf . Tensor : shape = ( 1 ,), dtype = bool , numpy = array ([ True ]) > c = tf . constant ([ False ]) x = tf . constant ([ False , True , True , False ]) tf . math . logical_or ( c , x ) < tf . Tensor : shape = ( 4 ,), dtype = bool , numpy = array ([ False , True , True , False ]) > c | x < tf . Tensor : shape = ( 4 ,), dtype = bool , numpy = array ([ False , True , True , False ]) > y = tf . constant ([ False , False , True , True ]) z = tf . constant ([ False , True , False , True ]) tf . math . logical_or ( y , z ) < tf . Tensor : shape = ( 4 ,), dtype = bool , numpy = array ([ False , True , True , True ]) > y | z < tf . Tensor : shape = ( 4 ,), dtype = bool , numpy = array ([ False , True , True , True ]) > This op also supports broadcasting tf . logical_or ([[ True , False ]], [[ True ], [ False ]]) < tf . Tensor : shape = ( 2 , 2 ), dtype = bool , numpy = array ([[ True , True ], [ True , False ]]) > The reduction version of this elementwise operation is tf.math.reduce_any . Args x A tf.Tensor of type bool. y A tf.Tensor of type bool. name A name for the operation (optional). Returns A tf.Tensor of type bool with the shape that x and y broadcast to. Args x A Tensor of type bool . y A Tensor of type bool . name A name for the operation (optional). Returns A Tensor of type bool .


Page: https://www.tensorflow.org/api_docs/python/tf/make_ndarray
View source on GitHub Create a numpy ndarray from a tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.make_ndarray tf . make_ndarray ( tensor ) Create a numpy ndarray with the same shape and data as the tensor. For example: # Tensor a has shape (2,3) a = tf . constant ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]]) proto_tensor = tf . make_tensor_proto ( a ) # convert `tensor a` to a proto tensor tf . make_ndarray ( proto_tensor ) # output: array([[1, 2, 3], #                                              [4, 5, 6]], dtype=int32) # output has shape (2,3) Args tensor A TensorProto. Returns A numpy array with the tensor contents. Raises TypeError if tensor has unsupported type.


Page: https://www.tensorflow.org/api_docs/python/tf/make_tensor_proto
View source on GitHub Create a TensorProto. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.make_tensor_proto tf . make_tensor_proto ( values , dtype = None , shape = None , verify_shape = False , allow_broadcast = False ) In TensorFlow 2.0, representing tensors as protos should no longer be a
common workflow. That said, this utility function is still useful for
generating TF Serving request protos: request = tensorflow_serving . apis . predict_pb2 . PredictRequest () request . model_spec . name = "my_model" request . model_spec . signature_name = "serving_default" request . inputs [ "images" ] . CopyFrom ( tf . make_tensor_proto ( X_new )) make_tensor_proto accepts "values" of a python scalar, a python list, a
numpy ndarray, or a numpy scalar. If "values" is a python scalar or a python list, make_tensor_proto
first convert it to numpy ndarray. If dtype is None, the
conversion tries its best to infer the right numpy data
type. Otherwise, the resulting numpy array has a compatible data
type with the given dtype. In either case above, the numpy ndarray (either the caller provided
or the auto-converted) must have the compatible type with dtype. make_tensor_proto then converts the numpy array to a tensor proto. If "shape" is None, the resulting tensor proto represents the numpy
array precisely. Otherwise, "shape" specifies the tensor's shape and the numpy array
can not have more elements than what "shape" specifies. Args values Values to put in the TensorProto. dtype Optional tensor_pb2 DataType value. shape List of integers representing the dimensions of tensor. verify_shape Boolean that enables verification of a shape of values. allow_broadcast Boolean that enables allowing scalars and 1 length vector
broadcasting. Cannot be true when verify_shape is true. Returns A TensorProto . Depending on the type, it may contain data in the
"tensor_content" attribute, which is not directly useful to Python programs.
To access the values you should convert the proto back to a numpy ndarray
with tf.make_ndarray(proto) . If values is a TensorProto , it is immediately returned; dtype and shape are ignored. Raises TypeError if unsupported types are provided. ValueError if arguments have inappropriate values or if verify_shape is
True and shape of values is not equals to a shape from the argument.


Page: https://www.tensorflow.org/api_docs/python/tf/map_fn
View source on GitHub Transforms elems by applying fn to each element unstacked on axis 0. (deprecated arguments) tf . map_fn ( fn , elems , dtype = None , parallel_iterations = None , back_prop = True , swap_memory = False , infer_shape = True , name = None , fn_output_signature = None ) Used in the notebooks Used in the guide Used in the tutorials Matrix approximation with Core APIs Extension types Ragged tensors NumPy API on TensorFlow Federated Learning for Text Generation Client-efficient large-model federated learning via `federated_select` and sparse aggregation Quantum Convolutional Neural Network Deprecated: SOME ARGUMENTS ARE DEPRECATED: (dtype) . They will be removed in a future version.
Instructions for updating:
Use fn_output_signature instead See also tf.scan . map_fn unstacks elems on axis 0 to obtain a sequence of elements;
calls fn to transform each element; and then stacks the transformed
values back together. Mapping functions with single-Tensor inputs and outputs If elems is a single tensor and fn 's signature is tf.Tensor->tf.Tensor ,
then map_fn(fn, elems) is equivalent to tf.stack([fn(elem) for elem in tf.unstack(elems)]) .  E.g.: tf . map_fn ( fn = lambda t : tf . range ( t , t + 3 ), elems = tf . constant ([ 3 , 5 , 2 ])) < tf . Tensor : shape = ( 3 , 3 ), dtype = int32 , numpy = array ([[ 3 , 4 , 5 ], [ 5 , 6 , 7 ], [ 2 , 3 , 4 ]], dtype = int32 ) > map_fn(fn, elems).shape = [elems.shape[0]] + fn(elems[0]).shape . Mapping functions with multi-arity inputs and outputs map_fn also supports functions with multi-arity inputs and outputs: If elems is a tuple (or nested structure) of tensors, then those tensors
must all have the same outer-dimension size ( num_elems ); and fn is
used to transform each tuple (or structure) of corresponding slices from elems .  E.g., if elems is a tuple (t1, t2, t3) , then fn is used to
transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 <= i < num_elems ). If fn returns a tuple (or nested structure) of tensors, then the
result is formed by stacking corresponding elements from those structures. Specifying fn 's output signature If fn 's input and output signatures are different, then the output
signature must be specified using fn_output_signature .  (The input and
output signatures are differ if their structures, dtypes, or tensor types do
not match).  E.g.: tf . map_fn ( fn = tf . strings . length , # input & output have different dtypes elems = tf . constant ([ "hello" , "moon" ]), fn_output_signature = tf . int32 ) < tf . Tensor : shape = ( 2 ,), dtype = int32 , numpy = array ([ 5 , 4 ], dtype = int32 ) > tf . map_fn ( fn = tf . strings . join , # input & output have different structures elems = [ tf . constant ([ 'The' , 'A' ]), tf . constant ([ 'Dog' , 'Cat' ])], fn_output_signature = tf . string ) < tf . Tensor : shape = ( 2 ,), dtype = string , numpy = array ([ b 'TheDog' , b 'ACat' ], dtype = object ) > fn_output_signature can be specified using any of the following: A tf.DType or tf.TensorSpec (to describe a tf.Tensor ) A tf.RaggedTensorSpec (to describe a tf.RaggedTensor ) A tf.SparseTensorSpec (to describe a tf.sparse.SparseTensor ) A (possibly nested) tuple, list, or dict containing the above types. RaggedTensors map_fn supports tf.RaggedTensor inputs and outputs.  In particular: If elems is a RaggedTensor , then fn will be called with each
row of that ragged tensor. If elems has only one ragged dimension, then the values passed to fn will be tf.Tensor s. If elems has multiple ragged dimensions, then the values passed to fn will be tf.RaggedTensor s with one fewer ragged dimension. If the result of map_fn should be a RaggedTensor , then use a tf.RaggedTensorSpec to specify fn_output_signature . If fn returns tf.Tensor s with varying sizes, then use a tf.RaggedTensorSpec with ragged_rank=0 to combine them into a
single ragged tensor (which will have ragged_rank=1). If fn returns tf.RaggedTensor s, then use a tf.RaggedTensorSpec with the same ragged_rank . # Example: RaggedTensor input rt = tf . ragged . constant ([[ 1 , 2 , 3 ], [], [ 4 , 5 ], [ 6 ]]) tf . map_fn ( tf . reduce_sum , rt , fn_output_signature = tf . int32 ) < tf . Tensor : shape = ( 4 ,), dtype = int32 , numpy = array ([ 6 , 0 , 9 , 6 ], dtype = int32 ) > # Example: RaggedTensor output elems = tf . constant ([ 3 , 5 , 0 , 2 ]) tf . map_fn ( tf . range , elems , fn_output_signature = tf . RaggedTensorSpec ( shape = [ None ], dtype = tf . int32 )) < tf . RaggedTensor [[ 0 , 1 , 2 ], [ 0 , 1 , 2 , 3 , 4 ], [], [ 0 , 1 ]] > Note: map_fn should only be used if you need to map a function over the rows of a RaggedTensor .  If you wish to map a function over the
individual values, then you should use: tf.ragged.map_flat_values(fn, rt) (if fn is expressible as TensorFlow ops) rt.with_flat_values(map_fn(fn, rt.flat_values)) (otherwise) E.g.: rt = tf . ragged . constant ([[ 1 , 2 , 3 ], [], [ 4 , 5 ], [ 6 ]]) tf . ragged . map_flat_values ( lambda x : x + 2 , rt ) < tf . RaggedTensor [[ 3 , 4 , 5 ], [], [ 6 , 7 ], [ 8 ]] > SparseTensors map_fn supports tf.sparse.SparseTensor inputs and outputs.  In particular: If elems is a SparseTensor , then fn will be called with each row
of that sparse tensor. In particular, the value passed to fn will be a tf.sparse.SparseTensor with one fewer dimension than elems . If the result of map_fn should be a SparseTensor , then use a tf.SparseTensorSpec to specify fn_output_signature .  The individual SparseTensor s returned by fn will be stacked into a single SparseTensor with one more dimension. # Example: SparseTensor input st = tf . sparse . SparseTensor ([[ 0 , 0 ], [ 2 , 0 ], [ 2 , 1 ]], [ 2 , 3 , 4 ], [ 4 , 4 ]) tf . map_fn ( tf . sparse . reduce_sum , st , fn_output_signature = tf . int32 ) < tf . Tensor : shape = ( 4 ,), dtype = int32 , numpy = array ([ 2 , 0 , 7 , 0 ], dtype = int32 ) > # Example: SparseTensor output tf . sparse . to_dense ( tf . map_fn ( tf . sparse . eye , tf . constant ([ 2 , 3 ]), fn_output_signature = tf . SparseTensorSpec ( None , tf . float32 ))) < tf . Tensor : shape = ( 2 , 3 , 3 ), dtype = float32 , numpy = array ([[[ 1. , 0. , 0. ], [ 0. , 1. , 0. ], [ 0. , 0. , 0. ]], [[ 1. , 0. , 0. ], [ 0. , 1. , 0. ], [ 0. , 0. , 1. ]]], dtype = float32 ) > Note: map_fn should only be used if you need to map a function over the rows of a SparseTensor .  If you wish to map a function over the nonzero
values, then you should use: If the function is expressible as TensorFlow ops, use: tf . sparse . SparseTensor ( st . indices , fn ( st . values ), st . dense_shape ) Otherwise, use: tf . sparse . SparseTensor ( st . indices , tf . map_fn ( fn , st . values ), st . dense_shape ) map_fn vs. vectorized operations map_fn will apply the operations used by fn to each element of elems ,
resulting in O(elems.shape[0]) total operations.  This is somewhat
mitigated by the fact that map_fn can process elements in parallel.
However, a transform expressed using map_fn is still typically less
efficient than an equivalent transform expressed using vectorized operations. map_fn should typically only be used if one of the following is true: It is difficult or expensive to express the desired transform with
vectorized operations. fn creates large intermediate values, so an equivalent vectorized
transform would take too much memory. Processing elements in parallel is more efficient than an equivalent
vectorized transform. Efficiency of the transform is not critical, and using map_fn is
more readable. E.g., the example given above that maps fn=lambda t: tf.range(t, t + 3) across elems could be rewritten more efficiently using vectorized ops: elems = tf . constant ([ 3 , 5 , 2 ]) tf . range ( 3 ) + tf . expand_dims ( elems , 1 ) < tf . Tensor : shape = ( 3 , 3 ), dtype = int32 , numpy = array ([[ 3 , 4 , 5 ], [ 5 , 6 , 7 ], [ 2 , 3 , 4 ]], dtype = int32 ) > In some cases, tf.vectorized_map can be used to automatically convert a
function to a vectorized equivalent. Eager execution When executing eagerly, map_fn does not execute in parallel even if parallel_iterations is set to a value > 1. You can still get the
performance benefits of running a function in parallel by using the tf.function decorator: fn = lambda t : tf . range ( t , t + 3 ) @tf . function def func ( elems ): return tf . map_fn ( fn , elems , parallel_iterations = 3 ) func ( tf . constant ([ 3 , 5 , 2 ])) < tf . Tensor : shape = ( 3 , 3 ), dtype = int32 , numpy = array ([[ 3 , 4 , 5 ], [ 5 , 6 , 7 ], [ 2 , 3 , 4 ]], dtype = int32 ) > Note: if you use the tf.function decorator, any non-TensorFlow Python
code that you may have written in your function won't get executed. See tf.function for more  details. The recommendation would be to debug without tf.function but switch to it to get performance benefits of running map_fn in parallel. Args fn The callable to be performed.  It accepts one argument, which will have
the same (possibly nested) structure as elems .  Its output must have the
same structure as fn_output_signature if one is provided; otherwise it
must have the same structure as elems . elems A tensor or (possibly nested) sequence of tensors, each of which will
be unstacked along their first dimension. fn will be applied to the
nested sequence of the resulting slices. elems may include ragged and
sparse tensors. elems must consist of at least one tensor. dtype Deprecated: Equivalent to fn_output_signature . parallel_iterations (optional) The number of iterations allowed to run in
parallel. When graph building, the default value is 10. While executing
eagerly, the default value is set to 1. back_prop (optional) Deprecated: prefer using tf.stop_gradient instead.  False disables support for back propagation. swap_memory (optional) True enables GPU-CPU memory swapping. infer_shape (optional) False disables tests for consistent output shapes. name (optional) Name prefix for the returned tensors. fn_output_signature The output signature of fn . Must be specified if fn 's input and output signatures are different (i.e., if their
structures, dtypes, or tensor types do not match). fn_output_signature can be specified using any of the following: A tf.DType or tf.TensorSpec (to describe a tf.Tensor ) A tf.RaggedTensorSpec (to describe a tf.RaggedTensor ) A tf.SparseTensorSpec (to describe a tf.sparse.SparseTensor ) A (possibly nested) tuple, list, or dict containing the above types. Returns A tensor or (possibly nested) sequence of tensors.  Each tensor stacks the
results of applying fn to tensors unstacked from elems along the first
dimension, from first to last.  The result may include ragged and sparse
tensors. Raises TypeError if fn is not callable or the structure of the output of fn and fn_output_signature do not match. ValueError if the lengths of the output of fn and fn_output_signature do not match, or if the elems does not contain any tensor. Examples >>> elems = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) >>> tf . map_fn ( lambda x : x * x , elems ) < tf . Tensor : shape = ( 6 ,), dtype = int64 , numpy = array ([ 1 , 4 , 9 , 16 , 25 , 36 ]) > elems = ( np . array ([ 1 , 2 , 3 ]), np . array ([ - 1 , 1 , - 1 ])) tf . map_fn ( lambda x : x [ 0 ] * x [ 1 ], elems , fn_output_signature = tf . int64 ) < tf . Tensor : shape = ( 3 ,), dtype = int64 , numpy = array ([ - 1 , 2 , - 3 ]) > elems = np . array ([ 1 , 2 , 3 ]) tf . map_fn ( lambda x : ( x , - x ), elems , fn_output_signature = ( tf . int64 , tf . int64 )) ( < tf . Tensor : shape = ( 3 ,), dtype = int64 , numpy = array ([ 1 , 2 , 3 ])>, < tf . Tensor : shape = ( 3 ,), dtype = int64 , numpy = array ([ - 1 , - 2 , - 3 ])>)


Page: https://www.tensorflow.org/api_docs/python/tf/linalg/matmul
View source on GitHub Multiplies matrix a by matrix b , producing a * b . View aliases Main aliases tf.matmul Compat aliases for migration See Migration guide for
more details. tf.compat.v1.matmul tf . linalg . matmul ( a , b , transpose_a = False , transpose_b = False , adjoint_a = False , adjoint_b = False , a_is_sparse = False , b_is_sparse = False , output_type = None , grad_a = False , grad_b = False , name = None ) Used in the notebooks Used in the guide Used in the tutorials Use a GPU Introduction to modules, layers, and models DTensor concepts Introduction to graphs and tf.function Use TF1.x models in TF2 workflows Customization basics: tensors and operations Custom layers Distributed training with DTensors Bayesian Gaussian Mixture Model and Hamiltonian MCMC Optimizers in TensorFlow Probability The inputs must, following any transpositions, be tensors of rank >= 2
where the inner 2 dimensions specify valid matrix multiplication dimensions,
and any further outer dimensions specify matching batch size. Both matrices must be of the same type. The supported types are: bfloat16 , float16 , float32 , float64 , int32 , int64 , complex64 , complex128 . Either matrix can be transposed or adjointed (conjugated and transposed) on
the fly by setting one of the corresponding flag to True . These are False by default. If one or both of the matrices contain a lot of zeros, a more efficient
multiplication algorithm can be used by setting the corresponding a_is_sparse or b_is_sparse flag to True . These are False by default.
This optimization is only available for plain matrices (rank-2 tensors) with
datatypes bfloat16 or float32 . A simple 2-D tensor matrix multiplication: a = tf . constant ([ 1 , 2 , 3 , 4 , 5 , 6 ], shape = [ 2 , 3 ]) a # 2-D tensor < tf . Tensor : shape = ( 2 , 3 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], dtype = int32 ) > b = tf . constant ([ 7 , 8 , 9 , 10 , 11 , 12 ], shape = [ 3 , 2 ]) b # 2-D tensor < tf . Tensor : shape = ( 3 , 2 ), dtype = int32 , numpy = array ([[ 7 , 8 ], [ 9 , 10 ], [ 11 , 12 ]], dtype = int32 ) > c = tf . matmul ( a , b ) c # `a` * `b` < tf . Tensor : shape = ( 2 , 2 ), dtype = int32 , numpy = array ([[ 58 , 64 ], [ 139 , 154 ]], dtype = int32 ) > A batch matrix multiplication with batch shape [2]: a = tf . constant ( np . arange ( 1 , 13 , dtype = np . int32 ), shape = [ 2 , 2 , 3 ]) a # 3-D tensor < tf . Tensor : shape = ( 2 , 2 , 3 ), dtype = int32 , numpy = array ([[[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], [[ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]], dtype = int32 ) > b = tf . constant ( np . arange ( 13 , 25 , dtype = np . int32 ), shape = [ 2 , 3 , 2 ]) b # 3-D tensor < tf . Tensor : shape = ( 2 , 3 , 2 ), dtype = int32 , numpy = array ([[[ 13 , 14 ], [ 15 , 16 ], [ 17 , 18 ]], [[ 19 , 20 ], [ 21 , 22 ], [ 23 , 24 ]]], dtype = int32 ) > c = tf . matmul ( a , b ) c # `a` * `b` < tf . Tensor : shape = ( 2 , 2 , 2 ), dtype = int32 , numpy = array ([[[ 94 , 100 ], [ 229 , 244 ]], [[ 508 , 532 ], [ 697 , 730 ]]], dtype = int32 ) > Since python >= 3.5 the @ operator is supported
(see PEP 465 ). In TensorFlow,
it simply calls the tf.matmul() function, so the following lines are
equivalent: d = a @ b @ [[ 10 ], [ 11 ]] d = tf . matmul ( tf . matmul ( a , b ), [[ 10 ], [ 11 ]]) Args a tf.Tensor of type float16 , float32 , float64 , int32 , complex64 , complex128 and rank > 1. b tf.Tensor with same type and rank as a . transpose_a If True , a is transposed before multiplication. transpose_b If True , b is transposed before multiplication. adjoint_a If True , a is conjugated and transposed before
multiplication. adjoint_b If True , b is conjugated and transposed before
multiplication. a_is_sparse If True , a is treated as a sparse matrix. Notice, this does not support tf.sparse.SparseTensor , it just makes optimizations
that assume most values in a are zero. See tf.sparse.sparse_dense_matmul for some support for tf.sparse.SparseTensor multiplication. b_is_sparse If True , b is treated as a sparse matrix. Notice, this does not support tf.sparse.SparseTensor , it just makes optimizations
that assume most values in b are zero. See tf.sparse.sparse_dense_matmul for some support for tf.sparse.SparseTensor multiplication. output_type The output datatype if needed. Defaults to None in which case
the output_type is the same as input type. Currently only works when input
tensors are type (u)int8 and output_type can be int32. grad_a Set it to True to hint that Tensor a is for the backward pass. grad_b Set it to True to hint that Tensor b is for the backward pass. name Name for the operation (optional). Returns A tf.Tensor of the same type as a and b where each inner-most matrix
is the product of the corresponding matrices in a and b , e.g. if all
transpose or adjoint attributes are False : output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j]) ,
for all indices i , j . Note This is matrix product, not element-wise product. Raises ValueError If transpose_a and adjoint_a , or transpose_b and adjoint_b are both set to True . TypeError If output_type is specified but the types of a , b and output_type is not (u)int8, (u)int8 and int32.


Page: https://www.tensorflow.org/api_docs/python/tf/linalg/sqrtm
Computes the matrix square root of one or more square matrices: View aliases Main aliases tf.matrix_square_root Compat aliases for migration See Migration guide for
more details. tf.compat.v1.linalg.sqrtm , tf.compat.v1.matrix_square_root tf . linalg . sqrtm ( input : Annotated [ Any , TV_MatrixSquareRoot_T ], name = None ) -> Annotated [ Any , TV_MatrixSquareRoot_T ] matmul(sqrtm(A), sqrtm(A)) = A The input matrix should be invertible. If the input matrix is real, it should
have no eigenvalues which are real and negative (pairs of complex conjugate
eigenvalues are allowed). The matrix square root is computed by first reducing the matrix to
quasi-triangular form with the real Schur decomposition. The square root
of the quasi-triangular matrix is then computed directly. Details of
the algorithm can be found in: Nicholas J. Higham, "Computing real
square roots of a real matrix", Linear Algebra Appl., 1987. The input is a tensor of shape [..., M, M] whose inner-most 2 dimensions
form square matrices. The output is a tensor of the same shape as the input
containing the matrix square root for all input submatrices [..., :, :] . Args input A Tensor . Must be one of the following types: float64 , float32 , half , complex64 , complex128 .
Shape is [..., M, M] . name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/math/maximum
Returns the max of x and y (i.e. x > y ? x : y) element-wise. View aliases Main aliases tf.maximum Compat aliases for migration See Migration guide for
more details. tf.compat.v1.maximum tf . math . maximum ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the guide Used in the tutorials Extension types Introduction to graphs and tf.function Generate music with an RNN Instance Segmentation with Model Garden Graph regularization for sentiment classification using synthesized graphs Substantial Undocumented Infection Facilitates the Rapid Dissemination of Novel Coronavirus (SARS-CoV2) Human Pose Classification with MoveNet and TensorFlow Lite Example: x = tf . constant ([ 0. , 0. , 0. , 0. ]) y = tf . constant ([ - 2. , 0. , 2. , 5. ]) tf . math . maximum ( x , y ) < tf . Tensor : shape = ( 4 ,), dtype = float32 , numpy = array ([ 0. , 0. , 2. , 5. ], dtype = float32 ) > Note that maximum supports broadcast semantics for x and y . x = tf . constant ([ - 5. , 0. , 0. , 0. ]) y = tf . constant ([ - 3. ]) tf . math . maximum ( x , y ) < tf . Tensor : shape = ( 4 ,), dtype = float32 , numpy = array ([ - 3. , 0. , 0. , 0. ], dtype = float32 ) > The reduction version of this elementwise operation is tf.math.reduce_max Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , int8 , uint8 , int16 , uint16 , int32 , uint32 , int64 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/meshgrid
View source on GitHub Broadcasts parameters for evaluation on an N-D grid. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.meshgrid tf . meshgrid ( * args , ** kwargs ) Used in the notebooks Used in the guide Optimizers with Core APIs Given N one-dimensional coordinate arrays *args , returns a list outputs of N-D coordinate arrays for evaluating expressions on an N-D grid. Notes: meshgrid supports cartesian ('xy') and matrix ('ij') indexing conventions.
When the indexing argument is set to 'xy' (the default), the broadcasting
instructions for the first two dimensions are swapped. Examples: Calling X, Y = meshgrid(x, y) with the tensors x = [ 1 , 2 , 3 ] y = [ 4 , 5 , 6 ] X , Y = tf . meshgrid ( x , y ) # X = [[1, 2, 3], #      [1, 2, 3], #      [1, 2, 3]] # Y = [[4, 4, 4], #      [5, 5, 5], #      [6, 6, 6]] Args *args Tensor s with rank 1. **kwargs indexing: Either 'xy' or 'ij' (optional, default: 'xy'). name: A name for the operation (optional). Returns outputs A list of N Tensor s with rank N. Raises TypeError When no keyword arguments (kwargs) are passed. ValueError When indexing keyword argument is not one of xy or ij .


Page: https://www.tensorflow.org/api_docs/python/tf/math/minimum
Returns the min of x and y (i.e. x < y ? x : y) element-wise. View aliases Main aliases tf.minimum Compat aliases for migration See Migration guide for
more details. tf.compat.v1.minimum tf . math . minimum ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the guide Used in the tutorials Extension types Integrated gradients Client-efficient large-model federated learning via `federated_select` and sparse aggregation Neural machine translation with a Transformer and Keras Both inputs are number-type tensors (except complex). minimum expects that
both tensors have the same dtype . Examples: x = tf . constant ([ 0. , 0. , 0. , 0. ]) y = tf . constant ([ - 5. , - 2. , 0. , 3. ]) tf . math . minimum ( x , y ) < tf . Tensor : shape = ( 4 ,), dtype = float32 , numpy = array ([ - 5. , - 2. , 0. , 0. ], dtype = float32 ) > Note that minimum supports broadcast semantics for x and y . x = tf . constant ([ - 5. , 0. , 0. , 0. ]) y = tf . constant ([ - 3. ]) tf . math . minimum ( x , y ) < tf . Tensor : shape = ( 4 ,), dtype = float32 , numpy = array ([ - 5. , - 3. , - 3. , - 3. ], dtype = float32 ) > The reduction version of this elementwise operation is tf.math.reduce_min Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , int8 , uint8 , int16 , uint16 , int32 , uint32 , int64 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/multiply
View source on GitHub Returns an element-wise x * y. View aliases Main aliases tf.multiply Compat aliases for migration See Migration guide for
more details. tf.compat.v1.multiply tf . math . multiply ( x , y , name = None ) Used in the notebooks Used in the guide Used in the tutorials Introduction to Tensors Migrate the SavedModel workflow Training & evaluation with the built-in methods Customization basics: tensors and operations Parametrized Quantum Circuits for Reinforcement Learning Universal Sentence Encoder Universal Sentence Encoder-Lite demo TFX Estimator Component Tutorial For example: x = tf . constant (([ 1 , 2 , 3 , 4 ])) tf . math . multiply ( x , x ) < tf . Tensor : shape = ( 4 ,), dtype =... , numpy = array ([ 1 , 4 , 9 , 16 ], dtype = int32 ) > Since tf.math.multiply will convert its arguments to Tensor s, you can also
pass in non- Tensor arguments: tf . math . multiply ( 7 , 6 ) < tf . Tensor : shape = (), dtype = int32 , numpy = 42 > If x.shape is not the same as y.shape , they will be broadcast to a
compatible shape. (More about broadcasting here .) For example: x = tf . ones ([ 1 , 2 ]); y = tf . ones ([ 2 , 1 ]); x * y # Taking advantage of operator overriding < tf . Tensor : shape = ( 2 , 2 ), dtype = float32 , numpy = array ([[ 1. , 1. ], [ 1. , 1. ]], dtype = float32 ) > The reduction version of this elementwise operation is tf.math.reduce_prod Args x A Tensor. Must be one of the following types: bfloat16 , half , float32 , float64 , uint8 , int8 , uint16 , int16 , int32 , int64 , complex64 , complex128 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor .  Has the same type as x . Raises InvalidArgumentError: When x and y have incompatible shapes or types.


Page: https://www.tensorflow.org/api_docs/python/tf/math/negative
Computes numerical negative value element-wise. View aliases Main aliases tf.negative Compat aliases for migration See Migration guide for
more details. tf.compat.v1.negative tf . math . negative ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] I.e., \(y = -x\). Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , int8 , int16 , int32 , int64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x . If x is a SparseTensor , returns SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)


Page: https://www.tensorflow.org/api_docs/python/tf/no_gradient
View source on GitHub Specifies that ops of type op_type is not differentiable. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.NoGradient , tf.compat.v1.NotDifferentiable , tf.compat.v1.no_gradient tf . no_gradient ( op_type : str ) -> None This function should not be used for operations that have a
well-defined gradient that is not yet implemented. This function is only used when defining a new op type. It may be
used for ops such as tf.size() that are not differentiable.  For
example: tf . no_gradient ( "Size" ) The gradient computed for 'op_type' will then propagate zeros. For ops that have a well-defined gradient but are not yet implemented,
no declaration should be made, and an error must be thrown if
an attempt to request its gradient is made. Args op_type The string type of an operation. This corresponds to the OpDef.name field for the proto that defines the operation. Raises TypeError If op_type is not a string.


Page: https://www.tensorflow.org/api_docs/python/tf/no_op
Does nothing. Only useful as a placeholder for control edges. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.no_op tf . no_op ( name = None ) Args name A name for the operation (optional). Returns The created Operation.


Page: https://www.tensorflow.org/api_docs/python/tf/nondifferentiable_batch_function
View source on GitHub Batches the computation done by the decorated function. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.nondifferentiable_batch_function tf . nondifferentiable_batch_function ( num_batch_threads , max_batch_size , batch_timeout_micros , allowed_batch_sizes = None , max_enqueued_batches = 10 , autograph = True , enable_large_batch_splitting = True ) So, for example, in the following code @batch_function ( 1 , 2 , 3 ) def layer ( a ): return tf . matmul ( a , a ) b = layer ( w ) if more than one session.run call is simultaneously trying to compute b the values of w will be gathered, non-deterministically concatenated
along the first axis, and only one thread will run the computation. See the
documentation of the Batch op for more details. Assumes that all arguments of the decorated function are Tensors which will
be batched along their first dimension. SparseTensor is not supported. The return value of the decorated function
must be a Tensor or a list/tuple of Tensors. Args num_batch_threads Number of scheduling threads for processing batches
of work. Determines the number of batches processed in parallel. max_batch_size Batch sizes will never be bigger than this. batch_timeout_micros Maximum number of microseconds to wait before
outputting an incomplete batch. allowed_batch_sizes Optional list of allowed batch sizes. If left empty,
does nothing. Otherwise, supplies a list of batch sizes, causing the op
to pad batches up to one of those sizes. The entries must increase
monotonically, and the final entry must equal max_batch_size. max_enqueued_batches The maximum depth of the batch queue. Defaults to 10. autograph Whether to use autograph to compile python and eager style code
for efficient graph-mode execution. enable_large_batch_splitting The value of this option doesn't affect
processing output given the same input; it affects implementation details
as stated below: 1. Improve batching efficiency by eliminating unnecessary
adding. 2. max_batch_size specifies the limit of input and allowed_batch_sizes specifies the limit of a task to be processed. API
user can give an input of size 128 when 'max_execution_batch_size'
is 32 -> implementation can split input of 128 into 4 x 32, schedule
concurrent processing, and then return concatenated results corresponding
to 128. Returns The decorated function will return the unbatched computation output Tensors.


Page: https://www.tensorflow.org/api_docs/python/tf/norm
View source on GitHub Computes the norm of vectors, matrices, and tensors. View aliases Main aliases tf.linalg.norm tf . norm ( tensor , ord = 'euclidean' , axis = None , keepdims = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Advanced automatic differentiation Matrix approximation with Core APIs Generate Artificial Faces with CelebA Progressive GAN Model Human Pose Classification with MoveNet and TensorFlow Lite Generalized Linear Models Neural machine translation with a Transformer and Keras This function can compute several different vector norms (the 1-norm, the
Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and
matrix norms (Frobenius, 1-norm, 2-norm and inf-norm). Args tensor Tensor of types float32 , float64 , complex64 , complex128 ord Order of the norm. Supported values are 'fro' , 'euclidean' , 1 , 2 , np.inf and any positive real number yielding the corresponding
p-norm. Default is 'euclidean' which is equivalent to Frobenius norm if tensor is a matrix and equivalent to 2-norm for vectors.
Some restrictions apply:
  a) The Frobenius norm 'fro' is not defined for vectors,
  b) If axis is a 2-tuple (matrix norm), only 'euclidean' , ' fro' , 1 , 2 , np.inf are supported.
See the description of axis on how to compute norms for a batch of
vectors or matrices stored in a tensor. axis If axis is None (the default), the input is considered a vector
and a single vector norm is computed over the entire set of values in the
tensor, i.e. norm(tensor, ord=ord) is equivalent to norm(reshape(tensor, [-1]), ord=ord) .
If axis is a Python integer, the input is considered a batch of vectors,
and axis determines the axis in tensor over which to compute vector
norms.
If axis is a 2-tuple of Python integers it is considered a batch of
matrices and axis determines the axes in tensor over which to compute
a matrix norm.
Negative indices are supported. Example: If you are passing a tensor that
can be either a matrix or a batch of matrices at runtime, pass axis=[-2,-1] instead of axis=None to make sure that matrix norms are
computed. keepdims If True, the axis indicated in axis are kept with size 1.
Otherwise, the dimensions in axis are removed from the output shape. name The name of the op. Returns output A Tensor of the same type as tensor, containing the vector or
matrix norms. If keepdims is True then the rank of output is equal to
the rank of tensor . Otherwise, if axis is none the output is a scalar,
if axis is an integer, the rank of output is one less than the rank
of tensor , if axis is a 2-tuple the rank of output is two less
than the rank of tensor . Raises ValueError If ord or axis is invalid. numpy compatibility Mostly equivalent to numpy.linalg.norm.
Not supported: ord <= 0, 2-norm for matrices, nuclear norm.
Other differences:
  a) If axis is None , treats the flattened tensor as a vector
   regardless of rank.
  b) Explicitly supports 'euclidean' norm as the default, including for
   higher order tensors.


Page: https://www.tensorflow.org/api_docs/python/tf/math/not_equal
View source on GitHub Returns the truth value of (x != y) element-wise. View aliases Main aliases tf.not_equal Compat aliases for migration See Migration guide for
more details. tf.compat.v1.math.not_equal , tf.compat.v1.not_equal tf . math . not_equal ( x , y , name = None ) Used in the notebooks Used in the guide Used in the tutorials tf.data: Build TensorFlow input pipelines Understanding masking & padding Unicode strings Tutorial on Multi Armed Bandits in TF-Agents Performs a broadcast with the
arguments and then an element-wise inequality comparison, returning a Tensor
of boolean values. For example: x = tf . constant ([ 2 , 4 ]) y = tf . constant ( 2 ) tf . math . not_equal ( x , y ) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ False , True ]) > x = tf . constant ([ 2 , 4 ]) y = tf . constant ([ 2 , 4 ]) tf . math . not_equal ( x , y ) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ False , False ]) > Args x A tf.Tensor . y A tf.Tensor . name A name for the operation (optional). Returns A tf.Tensor of type bool with the same size as that of x or y. Raises tf.errors.InvalidArgumentError : If shapes of arguments are incompatible


Page: https://www.tensorflow.org/api_docs/python/tf/numpy_function
View source on GitHub Wraps a python function and uses it as a TensorFlow op. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.numpy_function tf . numpy_function ( func = None , inp = None , Tout = None , stateful = True , name = None ) Used in the notebooks Used in the tutorials Playing CartPole with the Actor-Critic method Given a python function func wrap this function as an operation in a tf.function . func must take numpy arrays as its arguments and
return numpy arrays as its outputs. There are two ways to use tf.numpy_function . As a decorator When using tf.numpy_function as a decorator: you must set Tout you may set name you must not set func or inp @tf . numpy_function ( Tout = tf . float32 ) def my_numpy_func ( x ): # x will be a numpy array with the contents of the input to the # tf.function print ( f 'executing eagerly, { x =} ' ) return np . sinh ( x ) The function runs eagerly: my_numpy_func ( 1.0 ) . numpy () executing eagerly , x = 1.0 1.17520 The behavior doesn't change inside a tf.function : @tf . function ( input_signature = [ tf . TensorSpec ( None , tf . float32 )]) def tf_function ( input ): y = tf . numpy_function ( my_numpy_func , [ input ], tf . float32 ) return y tf_function ( tf . constant ( 1. )) . numpy () executing eagerly , x = array ( 1. ) 1.17520 Inplace This form can be useful if you don't control the function's source,
but it is harder to read. Here is the same function with no decorator: def my_func ( x ): # x will be a numpy array with the contents of the input to the # tf.function print ( f 'executing eagerly, { x =} ' ) return np . sinh ( x ) To run tf.numpy_function in-place, pass the function, its inputs, and the
output type in a single call to tf.numpy_function : tf . numpy_function ( my_func , [ tf . constant ( 1.0 )], tf . float32 ) executing eagerly , x = array ( 1. ) 1.17520 More info Comparison to tf.py_function : tf.py_function and tf.numpy_function are very similar, except that tf.numpy_function takes numpy arrays, and not tf.Tensor s. If you want the
function to contain tf.Tensors , and have any TensorFlow operations executed
in the function be differentiable, please use tf.py_function . Note: We recommend to avoid using tf.numpy_function outside of
prototyping and experimentation due to the following known limitations: Calling tf.numpy_function will acquire the Python Global Interpreter Lock
(GIL) that allows only one thread to run at any point in time. This will
preclude efficient parallelization and distribution of the execution of the
program. Therefore, you are discouraged to use tf.numpy_function outside
of prototyping and experimentation. The body of the function (i.e. func ) will not be serialized in a tf.SavedModel . Therefore, you should not use this function if you need to
serialize your model and restore it in a different environment. The operation must run in the same address space as the Python program
that calls tf.numpy_function() . If you are using distributed
TensorFlow, you must run a tf.distribute.Server in the same process as the
program that calls tf.numpy_function you must pin the created
operation to a device in that server (e.g. using with tf.device(): ). Currently tf.numpy_function is not compatible with XLA. Calling tf.numpy_function inside tf.function(jit_compile=True) will raise an
error. Since the function takes numpy arrays, you cannot take gradients
through a numpy_function. If you require something that is differentiable,
please consider using tf.py_function. Args func A Python function, which accepts numpy.ndarray objects as arguments
and returns a list of numpy.ndarray objects (or a single numpy.ndarray ). This function must accept as many arguments as there are
tensors in inp , and these argument types will match the corresponding tf.Tensor objects in inp . The returns numpy.ndarray s must match the
number and types defined Tout . Important Note: Input and output numpy.ndarray s of func are not guaranteed to be copies. In some cases
their underlying memory will be shared with the corresponding TensorFlow
tensors. In-place modification or storing func input or return values in
python datastructures without explicit (np.)copy can have
non-deterministic consequences. inp A list of tf.Tensor objects. Tout A list or tuple of tensorflow data types or a single tensorflow data
type if there is only one, indicating what func returns. stateful (Boolean.) Setting this argument to False tells the runtime to
treat the function as stateless, which enables certain optimizations. A
function is stateless when given the same input it will return the same
output and have no side effects; its only purpose is to have a return
value. The behavior for a stateful function with the stateful argument
False is undefined. In particular, caution should be taken when mutating
the input arguments as this is a stateful operation. name (Optional) A name for the operation. Returns If func is None this returns a decorator that will ensure the
decorated function will always run with eager execution even if called
from a tf.function / tf.Graph . If used func is not None this executes func with eager execution
and returns the result: A single or list of tf.Tensor which func computes.


Page: https://www.tensorflow.org/api_docs/python/tf/one_hot
View source on GitHub Returns a one-hot tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.one_hot tf . one_hot ( indices , depth , on_value = None , off_value = None , axis = None , dtype = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Import a JAX model using JAX2TF Quickstart for the TensorFlow Core APIs Training & evaluation with the built-in methods Adversarial example using FGSM Custom Federated Algorithms, Part 2: Implementing Federated Averaging Federated Learning for Image Classification Client-efficient large-model federated learning via `federated_select` and sparse aggregation Training with Orbit See also tf.fill , tf.eye . The locations represented by indices in indices take value on_value ,
while all other locations take value off_value . on_value and off_value must have matching data types. If dtype is also
provided, they must be the same data type as specified by dtype . If on_value is not provided, it will default to the value 1 with type dtype If off_value is not provided, it will default to the value 0 with type dtype If the input indices is rank N , the output will have rank N+1 . The
new axis is created at dimension axis (default: the new axis is appended
at the end). If indices is a scalar the output shape will be a vector of length depth If indices is a vector of length features , the output shape will be: features x depth if axis == - 1 depth x features if axis == 0 If indices is a matrix (batch) with shape [batch, features] , the output
shape will be: batch x features x depth if axis == - 1 batch x depth x features if axis == 1 depth x batch x features if axis == 0 If indices is a RaggedTensor, the 'axis' argument must be positive and refer
to a non-ragged axis. The output will be equivalent to applying 'one_hot' on
the values of the RaggedTensor, and creating a new RaggedTensor from the
result. If dtype is not provided, it will attempt to assume the data type of on_value or off_value , if one or both are passed in. If none of on_value , off_value , or dtype are provided, dtype will default to the
value tf.float32 . Note: If a non-numeric data type output is desired ( tf.string , tf.bool ,
etc.), both on_value and off_value must be provided to one_hot . For example: indices = [ 0 , 1 , 2 ] depth = 3 tf . one_hot ( indices , depth ) # output: [3 x 3] # [[1., 0., 0.], #  [0., 1., 0.], #  [0., 0., 1.]] indices = [ 0 , 2 , - 1 , 1 ] depth = 3 tf . one_hot ( indices , depth , on_value = 5.0 , off_value = 0.0 , axis =- 1 ) # output: [4 x 3] # [[5.0, 0.0, 0.0],  # one_hot(0) #  [0.0, 0.0, 5.0],  # one_hot(2) #  [0.0, 0.0, 0.0],  # one_hot(-1) #  [0.0, 5.0, 0.0]]  # one_hot(1) indices = [[ 0 , 2 ], [ 1 , - 1 ]] depth = 3 tf . one_hot ( indices , depth , on_value = 1.0 , off_value = 0.0 , axis =- 1 ) # output: [2 x 2 x 3] # [[[1.0, 0.0, 0.0],   # one_hot(0) #   [0.0, 0.0, 1.0]],  # one_hot(2) #  [[0.0, 1.0, 0.0],   # one_hot(1) #   [0.0, 0.0, 0.0]]]  # one_hot(-1) indices = tf . ragged . constant ([[ 0 , 1 ], [ 2 ]]) depth = 3 tf . one_hot ( indices , depth ) # output: [2 x None x 3] # [[[1., 0., 0.], #   [0., 1., 0.]], #  [[0., 0., 1.]]] Args indices A Tensor of indices. depth A scalar defining the depth of the one hot dimension. on_value A scalar defining the value to fill in output when indices[j]
= i . (default: 1) off_value A scalar defining the value to fill in output when indices[j]
!= i . (default: 0) axis The axis to fill (default: -1, a new inner-most axis). dtype The data type of the output tensor. name A name for the operation (optional). Returns output The one-hot tensor. Raises TypeError If dtype of either on_value or off_value don't match dtype TypeError If dtype of on_value and off_value don't match one another


Page: https://www.tensorflow.org/api_docs/python/tf/ones
View source on GitHub Creates a tensor with all elements set to one (1). View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.ones tf . ones ( shape , dtype = tf . dtypes . float32 , name = None , layout = None ) Used in the notebooks Used in the guide Used in the tutorials Validating correctness & numerical equivalence Use TF1.x models in TF2 workflows Better performance with tf.function Extension types Ragged tensors Multilevel Modeling Primer in TensorFlow Probability Learnable Distributions Zoo Probabilistic PCA Policies Bayesian Switchpoint Analysis See also tf.ones_like , tf.zeros , tf.fill , tf.eye . This operation returns a tensor of type dtype with shape shape and
all elements set to one. tf . ones ([ 3 , 4 ], tf . int32 ) < tf . Tensor : shape = ( 3 , 4 ), dtype = int32 , numpy = array ([[ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ]], dtype = int32 ) > Args shape A list of integers, a tuple of integers, or a 1-D Tensor of
type int32 . dtype Optional DType of an element in the resulting Tensor . Default is tf.float32 . name Optional string. A name for the operation. layout Optional, tf.experimental.dtensor.Layout . If provided, the result
is a DTensor with the
provided layout. Returns A Tensor with all elements set to one (1).


Page: https://www.tensorflow.org/api_docs/python/tf/ones_like
View source on GitHub Creates a tensor of all ones that has the same shape as the input. tf . ones_like ( input , dtype = None , name = None , layout = None ) Used in the notebooks Used in the guide Used in the tutorials Extension types Validating correctness & numerical equivalence CycleGAN Deep Convolutional Generative Adversarial Network pix2pix: Image-to-image translation with a conditional GAN Multilevel Modeling Primer in TensorFlow Probability Research tools See also tf.ones . Given a single tensor ( tensor ), this operation returns a tensor of the
same type and shape as tensor with all elements set to 1. Optionally,
you can use dtype to specify a new type for the returned tensor. For example: tensor = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) tf . ones_like ( tensor ) < tf . Tensor : shape = ( 2 , 3 ), dtype = int32 , numpy = array ([[ 1 , 1 , 1 ], [ 1 , 1 , 1 ]], dtype = int32 ) > Note that the layout of the input tensor is not preserved if the op
is used inside tf.function. To obtain a tensor with the same layout as the
input, chain the returned value to a dtensor.relayout_like . Args input A Tensor . dtype A type for the returned Tensor . Must be float16 , float32 , float64 , int8 , uint8 , int16 , uint16 , int32 , int64 , complex64 , complex128 , bool or string . name A name for the operation (optional). layout Optional, tf.experimental.dtensor.Layout . If provided, the result
is a DTensor with the
provided layout. Returns A Tensor with all elements set to one.


Page: https://www.tensorflow.org/api_docs/python/tf/pad
View source on GitHub Pads a tensor. tf . pad ( tensor , paddings , mode = 'CONSTANT' , constant_values = 0 , name = None ) Used in the notebooks Used in the guide Used in the tutorials Advanced automatic differentiation Client-efficient large-model federated learning via `federated_select` and sparse aggregation A Tour of TensorFlow Probability Multiple changepoint detection and Bayesian model selection Graph-based Neural Structured Learning in TFX This operation pads a tensor according to the paddings you specify. paddings is an integer tensor with shape [n, 2] , where n is the rank of tensor . For each dimension D of input , paddings[D, 0] indicates how
many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension. If mode is "REFLECT" then both paddings[D, 0] and paddings[D, 1] must be no greater than tensor.dim_size(D) - 1 . If mode is "SYMMETRIC" then both paddings[D, 0] and paddings[D, 1] must be
no greater than tensor.dim_size(D) . The padded size of each dimension D of the output is: paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1] For example: t = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) paddings = tf . constant ([[ 1 , 1 ,], [ 2 , 2 ]]) # 'constant_values' is 0. # rank of 't' is 2. tf . pad ( t , paddings , "CONSTANT" ) # [[0, 0, 0, 0, 0, 0, 0], #  [0, 0, 1, 2, 3, 0, 0], #  [0, 0, 4, 5, 6, 0, 0], #  [0, 0, 0, 0, 0, 0, 0]] tf . pad ( t , paddings , "REFLECT" ) # [[6, 5, 4, 5, 6, 5, 4], #  [3, 2, 1, 2, 3, 2, 1], #  [6, 5, 4, 5, 6, 5, 4], #  [3, 2, 1, 2, 3, 2, 1]] tf . pad ( t , paddings , "SYMMETRIC" ) # [[2, 1, 1, 2, 3, 3, 2], #  [2, 1, 1, 2, 3, 3, 2], #  [5, 4, 4, 5, 6, 6, 5], #  [5, 4, 4, 5, 6, 6, 5]] Args tensor A Tensor . paddings A Tensor of type int32 . mode One of "CONSTANT", "REFLECT", or "SYMMETRIC" (case-insensitive) constant_values In "CONSTANT" mode, the scalar pad value to use. Must be
same type as tensor . name A name for the operation (optional). Returns A Tensor . Has the same type as tensor . Raises ValueError When mode is not one of "CONSTANT", "REFLECT", or "SYMMETRIC".


Page: https://www.tensorflow.org/api_docs/python/tf/parallel_stack
View source on GitHub Stacks a list of rank- R tensors into one rank- (R+1) tensor in parallel. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.parallel_stack tf . parallel_stack ( values , name = 'parallel_stack' ) Requires that the shape of inputs be known at graph construction time. Packs the list of tensors in values into a tensor with rank one higher than
each tensor in values , by packing them along the first dimension.
Given a list of length N of tensors of shape (A, B, C) ; the output tensor will have the shape (N, A, B, C) . For example: x = tf . constant ([ 1 , 4 ]) y = tf . constant ([ 2 , 5 ]) z = tf . constant ([ 3 , 6 ]) tf . parallel_stack ([ x , y , z ]) # [[1, 4], [2, 5], [3, 6]] The difference between stack and parallel_stack is that stack requires
all the inputs be computed before the operation will begin but doesn't require
that the input shapes be known during graph construction. parallel_stack will copy pieces of the input into the output as they become
available, in some situations this can provide a performance benefit. Unlike stack , parallel_stack does NOT support backpropagation. This is the opposite of unstack.  The numpy equivalent is tf . parallel_stack ([ x , y , z ]) = np . asarray ([ x , y , z ]) Args values A list of Tensor objects with the same shape and type. name A name for this operation (optional). Returns output A stacked Tensor with the same type as values . Raises RuntimeError if executed in eager mode. eager compatibility parallel_stack is not compatible with eager execution.


Page: https://www.tensorflow.org/api_docs/python/tf/math/pow
View source on GitHub Computes the power of one value to another. View aliases Main aliases tf.pow Compat aliases for migration See Migration guide for
more details. tf.compat.v1.math.pow , tf.compat.v1.pow tf . math . pow ( x , y , name = None ) Used in the notebooks Used in the guide Introduction to graphs and tf.function Given a tensor x and a tensor y , this operation computes \(x^y\) for
corresponding elements in x and y . For example: x = tf . constant ([[ 2 , 2 ], [ 3 , 3 ]]) y = tf . constant ([[ 8 , 16 ], [ 2 , 3 ]]) tf . pow ( x , y ) # [[256, 65536], [9, 27]] Args x A Tensor of type float16 , float32 , float64 , int32 , int64 , complex64 , or complex128 . y A Tensor of type float16 , float32 , float64 , int32 , int64 , complex64 , or complex128 . name A name for the operation (optional). Returns A Tensor .


Page: https://www.tensorflow.org/api_docs/python/tf/print
View source on GitHub Print the specified inputs. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.print tf . print ( * inputs , ** kwargs ) Used in the notebooks Used in the guide Used in the tutorials Better performance with tf.function Better performance with the tf.data API Debug a TensorFlow 2 migrated training pipeline Distributed Input A TensorFlow operator that prints the specified inputs to a desired
output stream or logging level. The inputs may be dense or sparse Tensors,
primitive python objects, data structures that contain tensors, and printable
Python objects. Printed tensors will recursively show the first and last
elements of each dimension to summarize. Example Single-input usage: tensor = tf . range ( 10 ) tf . print ( tensor , output_stream = sys . stderr ) (This prints "[0 1 2 ... 7 8 9]" to sys.stderr) Multi-input usage: tensor = tf . range ( 10 ) tf . print ( "tensors:" , tensor , { 2 : tensor * 2 }, output_stream = sys . stdout ) (This prints "tensors: [0 1 2 ... 7 8 9] {2: [0 2 4 ... 14 16 18]}" to
sys.stdout) Changing the input separator: tensor_a = tf . range ( 2 ) tensor_b = tensor_a * 2 tf . print ( tensor_a , tensor_b , output_stream = sys . stderr , sep = ',' ) (This prints "[0 1],[0 2]" to sys.stderr) Usage in a tf.function : @tf . function def f (): tensor = tf . range ( 10 ) tf . print ( tensor , output_stream = sys . stderr ) return tensor range_tensor = f () (This prints "[0 1 2 ... 7 8 9]" to sys.stderr) Compatibility usage in TF 1.x graphs : In graphs manually created outside of tf.function , this method returns
  the created TF operator that prints the data. To make sure the
  operator runs, users need to pass the produced op to tf.compat.v1.Session 's run method, or to use the op as a control
  dependency for executed ops by specifying with tf.compat.v1.control_dependencies([print_op]) . tf . compat . v1 . disable_v2_behavior () # for TF1 compatibility only sess = tf . compat . v1 . Session () with sess . as_default (): tensor = tf . range ( 10 ) print_op = tf . print ( "tensors:" , tensor , { 2 : tensor * 2 }, output_stream = sys . stdout ) with tf . control_dependencies ([ print_op ]): tripled_tensor = tensor * 3 sess . run ( tripled_tensor ) (This prints "tensors: [0 1 2 ... 7 8 9] {2: [0 2 4 ... 14 16 18]}" to
  sys.stdout) Note: In Jupyter notebooks and colabs, tf.print prints to the notebook
  cell outputs. It will not write to the notebook kernel's console logs. Args *inputs Positional arguments that are the inputs to print. Inputs in the
printed output will be separated by spaces. Inputs may be python
primitives, tensors, data structures such as dicts and lists that may
contain tensors (with the data structures possibly nested in arbitrary
ways), and printable python objects. output_stream The output stream, logging level, or file to print to.
Defaults to sys.stderr, but sys.stdout, tf.compat.v1.logging.info,
tf.compat.v1.logging.warning, tf.compat.v1.logging.error,
absl.logging.info, absl.logging.warning and absl.logging.error are also
supported. To print to a file, pass a string started with "file://"
followed by the file path, e.g., "file:///tmp/foo.out". summarize The first and last summarize elements within each dimension are
recursively printed per Tensor. If None, then the first 3 and last 3
elements of each dimension are printed for each tensor. If set to -1, it
will print all elements of every tensor. sep The string to use to separate the inputs. Defaults to " ". end End character that is appended at the end the printed string. Defaults
to the newline character. name A name for the operation (optional). Returns None when executing eagerly. During graph tracing this returns
a TF operator that prints the specified inputs in the specified output
stream or logging level. This operator will be automatically executed
except inside of tf.compat.v1 graphs and sessions. Raises ValueError If an unsupported output stream is specified.


Page: https://www.tensorflow.org/api_docs/python/tf/py_function
View source on GitHub Wraps a python function into a TensorFlow op that executes it eagerly. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.py_function tf . py_function ( func = None , inp = None , Tout = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Better performance with the tf.data API tf.data: Build TensorFlow input pipelines Better performance with tf.function TFRecord and tf.train.Example Using tf.py_function inside a tf.function allows you to run a python
function using eager execution, inside the tf.function 's graph.
This has two main effects: This allows you to use nofunc=None, inp=None, Tout=None tensorflow code
inside your tf.function . It allows you to run python control logic in a tf.function without
relying on tf.autograph to convert the code to use tensorflow control logic
(tf.cond, tf.while_loop). Both of these features can be useful for debugging. Since tf.py_function operates on Tensor s it is still
differentiable (once). There are two ways to use this function: As a decorator Use tf.py_function as a decorator to ensure the function always runs
eagerly. When using tf.py_function as a decorator: you must set Tout you may set name you must not set func or inp For example, you might use tf.py_function to
implement the log huber function. @tf . py_function ( Tout = tf . float32 ) def py_log_huber ( x , m ): print ( 'Running with eager execution.' ) if tf . abs ( x ) < = m : return x ** 2 else : return m ** 2 * ( 1 - 2 * tf . math . log ( m ) + tf . math . log ( x ** 2 )) Under eager execution the function operates normally: x = tf . constant ( 1.0 ) m = tf . constant ( 2.0 ) print ( py_log_huber ( x , m ) . numpy ()) Running with eager execution . 1.0 Inside a tf.function the tf.py_function is not converted to a tf.Graph .: @tf . function def tf_wrapper ( x ): print ( 'Tracing.' ) m = tf . constant ( 2.0 ) return py_log_huber ( x , m ) The tf.py_function only executes eagerly, and only when the tf.function is called: print ( tf_wrapper ( x ) . numpy ()) Tracing . Running with eager execution . 1.0 print ( tf_wrapper ( x ) . numpy ()) Running with eager execution . 1.0 Gradients work as expected: with tf . GradientTape () as t : t . watch ( x ) y = tf_wrapper ( x ) Running with eager execution . t . gradient ( y , x ) . numpy () 2.0 Inplace You can also skip the decorator and use tf.py_function in-place.
This form is a useful shortcut if you don't control the function's source,
but it is harder to read. # No decorator def log_huber ( x , m ): if tf . abs ( x ) < = m : return x ** 2 else : return m ** 2 * ( 1 - 2 * tf . math . log ( m ) + tf . math . log ( x ** 2 )) x = tf . constant ( 1.0 ) m = tf . constant ( 2.0 ) tf . py_function ( func = log_huber , inp = [ x , m ], Tout = tf . float32 ) . numpy () 1.0 More info You can also use tf.py_function to debug your models at runtime
using Python tools, i.e., you can isolate portions of your code that
you want to debug, wrap them in Python functions and insert pdb tracepoints
or print statements as desired, and wrap those functions in tf.py_function . For more information on eager execution, see the Eager guide . tf.py_function is similar in spirit to tf.numpy_function , but unlike
the latter, the former lets you use TensorFlow operations in the wrapped
Python function. In particular, while tf.compat.v1.py_func only runs on CPUs
and wraps functions that take NumPy arrays as inputs and return NumPy arrays
as outputs, tf.py_function can be placed on GPUs and wraps functions
that take Tensors as inputs, execute TensorFlow operations in their bodies,
and return Tensors as outputs. Note: We recommend to avoid using tf.py_function outside of prototyping
and experimentation due to the following known limitations: Calling tf.py_function will acquire the Python Global Interpreter Lock
(GIL) that allows only one thread to run at any point in time. This will
preclude efficient parallelization and distribution of the execution of the
program. The body of the function (i.e. func ) will not be serialized in a GraphDef . Therefore, you should not use this function if you need to
serialize your model and restore it in a different environment. The operation must run in the same address space as the Python program
that calls tf.py_function() . If you are using distributed
TensorFlow, you must run a tf.distribute.Server in the same process as the
program that calls tf.py_function() and you must pin the created
operation to a device in that server (e.g. using with tf.device(): ). Currently tf.py_function is not compatible with XLA. Calling tf.py_function inside tf.function(jit_compile=True) will raise an
error. Args func A Python function that accepts inp as arguments, and returns a value
(or list of values) whose type is described by Tout . Do not set func when using tf.py_function as a decorator. inp Input arguments for func .  A list whose elements are Tensor s or CompositeTensors (such as tf.RaggedTensor ); or a single Tensor or CompositeTensor . Do not set inp when using tf.py_function as a
decorator. Tout The type(s) of the value(s) returned by func .  One of the following. If func returns a Tensor (or a value that can be converted to a
Tensor): the tf.DType for that value. * If func returns a CompositeTensor : The tf.TypeSpec for that value. * If func returns None : the empty list ( [] ). * If func returns a list of Tensor and CompositeTensor values: a corresponding list of tf.DType s and tf.TypeSpec s for each value. name A name for the operation (optional). Returns If func is None this returns a decorator that will ensure the
decorated function will always run with eager execution even if called
from a tf.function / tf.Graph . If used func is not None this executes func with eager execution
and returns the result: a Tensor , CompositeTensor , or list of Tensor and CompositeTensor ; or an empty list if func returns None .


Page: https://www.tensorflow.org/api_docs/python/tf/ragged_fill_empty_rows
View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.ragged_fill_empty_rows tf . ragged_fill_empty_rows ( value_rowids : Annotated [ Any , _atypes . Int64 ], values : Annotated [ Any , TV_RaggedFillEmptyRows_T ], nrows : Annotated [ Any , _atypes . Int64 ], default_value : Annotated [ Any , TV_RaggedFillEmptyRows_T ], name = None ) Args value_rowids A Tensor of type int64 . values A Tensor . nrows A Tensor of type int64 . default_value A Tensor . Must have the same type as values . name A name for the operation (optional). Returns A tuple of Tensor objects (output_value_rowids, output_values, empty_row_indicator, reverse_index_map). output_value_rowids A Tensor of type int64 . output_values A Tensor . Has the same type as values . empty_row_indicator A Tensor of type bool . reverse_index_map A Tensor of type int64 .


Page: https://www.tensorflow.org/api_docs/python/tf/ragged_fill_empty_rows_grad
View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.ragged_fill_empty_rows_grad tf . ragged_fill_empty_rows_grad ( reverse_index_map : Annotated [ Any , _atypes . Int64 ], grad_values : Annotated [ Any , TV_RaggedFillEmptyRowsGrad_T ], name = None ) Args reverse_index_map A Tensor of type int64 . grad_values A Tensor . name A name for the operation (optional). Returns A tuple of Tensor objects (d_values, d_default_value). d_values A Tensor . Has the same type as grad_values . d_default_value A Tensor . Has the same type as grad_values .


Page: https://www.tensorflow.org/api_docs/python/tf/random_index_shuffle
Outputs the position of value in a permutation of [0, ..., max_index]. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.random_index_shuffle tf . random_index_shuffle ( index : Annotated [ Any , TV_RandomIndexShuffle_dtype ], seed : Annotated [ Any , TV_RandomIndexShuffle_Tseed ], max_index : Annotated [ Any , TV_RandomIndexShuffle_dtype ], rounds : int = 4 , name = None ) -> Annotated [ Any , TV_RandomIndexShuffle_dtype ] Output values are a bijection of the index for any combination and seed and max_index . If multiple inputs are vectors (matrix in case of seed) then the size of the
first dimension must match. The outputs are deterministic. Args index A Tensor . Must be one of the following types: int32 , uint32 , int64 , uint64 .
A scalar tensor or a vector of dtype dtype . The index (or indices) to be shuffled. Must be within [0, max_index]. seed A Tensor . Must be one of the following types: int32 , uint32 , int64 , uint64 .
A tensor of dtype Tseed and shape [3] or [n, 3]. The random seed. max_index A Tensor . Must have the same type as index .
A scalar tensor or vector of dtype dtype . The upper bound(s) of the interval (inclusive). rounds An optional int . Defaults to 4 .
The number of rounds to use the in block cipher. name A name for the operation (optional). Returns A Tensor . Has the same type as index .


Page: https://www.tensorflow.org/api_docs/python/tf/range
View source on GitHub Creates a sequence of numbers. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.range tf . range ( limit , delta = 1 , dtype = None , name = 'range' ) tf . range ( start , limit , delta = 1 , dtype = None , name = 'range' ) Used in the notebooks Used in the guide Used in the tutorials DTensor concepts Better performance with tf.function Training checkpoints Estimators Optimizers with Core APIs Integrated gradients DeepDream Save and load a model using a distribution strategy Playing CartPole with the Actor-Critic method Distributed Input Creates a sequence of numbers that begins at start and extends by
increments of delta up to but not including limit . The dtype of the resulting tensor is inferred from the inputs unless
it is provided explicitly. Like the Python builtin range , start defaults to 0, so that range(n) = range(0, n) . For example: start = 3 limit = 18 delta = 3 tf . range ( start , limit , delta ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 3 , 6 , 9 , 12 , 15 ], dtype = int32 ) > start = 3 limit = 1 delta = - 0.5 tf . range ( start , limit , delta ) < tf . Tensor : shape = ( 4 ,), dtype = float32 , numpy = array ([ 3. , 2.5 , 2. , 1.5 ], dtype = float32 ) > limit = 5 tf . range ( limit ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 0 , 1 , 2 , 3 , 4 ], dtype = int32 ) > Args start A 0-D Tensor (scalar). Acts as first entry in the range if limit is not None; otherwise, acts as range limit and first entry defaults to 0. limit A 0-D Tensor (scalar). Upper limit of sequence, exclusive. If None,
defaults to the value of start while the first entry of the range
defaults to 0. delta A 0-D Tensor (scalar). Number that increments start . Defaults to
1. dtype The type of the elements of the resulting tensor. name A name for the operation. Defaults to "range". Returns An 1-D Tensor of type dtype . numpy compatibility Equivalent to np.arange


Page: https://www.tensorflow.org/api_docs/python/tf/rank
View source on GitHub Returns the rank of a tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.rank tf . rank ( input , name = None ) Used in the notebooks Used in the guide Introduction to Tensors See also tf.shape . Returns a 0-D int32 Tensor representing the rank of input . For example: # shape of tensor 't' is [2, 2, 3] t = tf . constant ([[[ 1 , 1 , 1 ], [ 2 , 2 , 2 ]], [[ 3 , 3 , 3 ], [ 4 , 4 , 4 ]]]) tf . rank ( t ) # 3 Note: The rank of a tensor is not the same as the rank of a matrix. The
rank of a tensor is the number of indices required to uniquely select each
element of the tensor. Rank is also known as "order", "degree", or "ndims." Args input A Tensor or SparseTensor . name A name for the operation (optional). Returns A Tensor of type int32 . numpy compatibility Equivalent to np.ndim


Page: https://www.tensorflow.org/api_docs/python/tf/realdiv
Returns x / y element-wise for real types. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.realdiv tf . realdiv ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] If x and y are reals, this will return the floating-point division. Note: Div supports broadcasting. More about broadcasting here Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , uint8 , int8 , uint16 , int16 , int32 , uint32 , uint64 , int64 , complex64 , complex128 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/recompute_grad
View source on GitHub Defines a function as a recompute-checkpoint for the tape auto-diff. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.recompute_grad tf . recompute_grad ( f ) Tape checkpointing is a technique to reduce the memory consumption of the
auto-diff tape: Without tape checkpointing operations and intermediate values are
recorded to the tape for use in the backward pass. With tape checkpointing, only the function call and its inputs are
recorded. During back-propagation the recompute_grad custom gradient
( tf.custom_gradient ) recomputes the function under a localized Tape object.
This recomputation of the function during backpropagation performs redundant
calculation, but reduces the overall memory usage of the Tape. y = tf . Variable ( 1.0 ) def my_function ( x ): tf . print ( 'running' ) z = x * y return z my_function_recompute = tf . recompute_grad ( my_function ) with tf . GradientTape () as tape : r = tf . constant ( 1.0 ) for i in range ( 4 ): r = my_function_recompute ( r ) running running running running grad = tape . gradient ( r , [ y ]) running running running running Without recompute_grad , the tape contains all intermitate steps, and no
recomputation is performed. with tf . GradientTape () as tape : r = tf . constant ( 1.0 ) for i in range ( 4 ): r = my_function ( r ) running running running running grad = tape . gradient ( r , [ y ]) If f was a tf.keras Model or Layer object, methods and attributes
such as f.variables are not available on the returned function g .
Either keep a reference of f , or use g.__wrapped__ for accessing
these variables and methods. def print_running_and_return ( x ): tf . print ( "running" ) return x model = tf . keras . Sequential ([ tf . keras . layers . Lambda ( print_running_and_return ), tf . keras . layers . Dense ( 2 ) ]) model_recompute = tf . recompute_grad ( model ) with tf . GradientTape ( persistent = True ) as tape : r = tf . constant ([[ 1 , 2 ]]) for i in range ( 4 ): r = model_recompute ( r ) running running running running grad = tape . gradient ( r , model . variables ) running running running running Alternatively, use the __wrapped__ attribute to access the original
model object. grad = tape . gradient ( r , model_recompute . __wrapped__ . variables ) running running running running Args f function f(*x) that returns a Tensor or sequence of Tensor outputs. Returns A function g wrapping f that defines a custom gradient, which recomputes f on the backwards pass of a gradient call.


Page: https://www.tensorflow.org/api_docs/python/tf/math/reduce_all
View source on GitHub Computes tf.math.logical_and of elements across dimensions of a tensor. View aliases Main aliases tf.reduce_all tf . math . reduce_all ( input_tensor , axis = None , keepdims = False , name = None ) Used in the notebooks Used in the guide Used in the tutorials Extension types Neural machine translation with attention This is the reduction operation for the elementwise tf.math.logical_and op. Reduces input_tensor along the dimensions given in axis .
Unless keepdims is true, the rank of the tensor is reduced by 1 for each
of the entries in axis , which must be unique. If keepdims is true, the
reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a
tensor with a single element is returned. For example >>> x = tf . constant ([[ True , True ], [ False , False ]]) >>> tf . math . reduce_all ( x ) < tf . Tensor : shape = (), dtype = bool , numpy = False >
>>> tf . math . reduce_all ( x , 0 ) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ False , False ]) >
>>> tf . math . reduce_all ( x , 1 ) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ True , False ]) > Args input_tensor The boolean tensor to reduce. axis The dimensions to reduce. If None (the default), reduces all
dimensions. Must be in the range [-rank(input_tensor),
rank(input_tensor)) . keepdims If true, retains reduced dimensions with length 1. name A name for the operation (optional). Returns The reduced tensor. numpy compatibility Equivalent to np.all


Page: https://www.tensorflow.org/api_docs/python/tf/math/reduce_any
View source on GitHub Computes tf.math.logical_or of elements across dimensions of a tensor. View aliases Main aliases tf.reduce_any tf . math . reduce_any ( input_tensor , axis = None , keepdims = False , name = None ) This is the reduction operation for the elementwise tf.math.logical_or op. Reduces input_tensor along the dimensions given in axis .
Unless keepdims is true, the rank of the tensor is reduced by 1 for each
of the entries in axis , which must be unique. If keepdims is true, the
reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a
tensor with a single element is returned. For example >>> x = tf . constant ([[ True , True ], [ False , False ]]) >>> tf . reduce_any ( x ) < tf . Tensor : shape = (), dtype = bool , numpy = True >
>>> tf . reduce_any ( x , 0 ) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ True , True ]) >
>>> tf . reduce_any ( x , 1 ) < tf . Tensor : shape = ( 2 ,), dtype = bool , numpy = array ([ True , False ]) > Args input_tensor The boolean tensor to reduce. axis The dimensions to reduce. If None (the default), reduces all
dimensions. Must be in the range [-rank(input_tensor),
rank(input_tensor)) . keepdims If true, retains reduced dimensions with length 1. name A name for the operation (optional). Returns The reduced tensor. numpy compatibility Equivalent to np.any


Page: https://www.tensorflow.org/api_docs/python/tf/math/reduce_logsumexp
View source on GitHub Computes log(sum(exp(elements across dimensions of a tensor))). View aliases Main aliases tf.reduce_logsumexp tf . math . reduce_logsumexp ( input_tensor , axis = None , keepdims = False , name = None ) Reduces input_tensor along the dimensions given in axis .
Unless keepdims is true, the rank of the tensor is reduced by 1 for each
of the entries in axis , which must be unique. If keepdims is true, the
reduced dimensions are retained with length 1. If axis has no entries, all dimensions are reduced, and a
tensor with a single element is returned. This function is more numerically stable than log(sum(exp(input))). It avoids
overflows caused by taking the exp of large inputs and underflows caused by
taking the log of small inputs. For example: x = tf . constant ([[ 0. , 0. , 0. ], [ 0. , 0. , 0. ]]) tf . reduce_logsumexp ( x ) # log(6) tf . reduce_logsumexp ( x , 0 ) # [log(2), log(2), log(2)] tf . reduce_logsumexp ( x , 1 ) # [log(3), log(3)] tf . reduce_logsumexp ( x , 1 , keepdims = True ) # [[log(3)], [log(3)]] tf . reduce_logsumexp ( x , [ 0 , 1 ]) # log(6) Args input_tensor The tensor to reduce. Should have numeric type. axis The dimensions to reduce. If None (the default), reduces all
dimensions. Must be in the range [-rank(input_tensor),
rank(input_tensor)) . keepdims If true, retains reduced dimensions with length 1. name A name for the operation (optional). Returns The reduced tensor.


Page: https://www.tensorflow.org/api_docs/python/tf/math/reduce_max
View source on GitHub Computes tf.math.maximum of elements across dimensions of a tensor. View aliases Main aliases tf.reduce_max tf . math . reduce_max ( input_tensor , axis = None , keepdims = False , name = None ) Used in the notebooks Used in the guide Used in the tutorials Advanced automatic differentiation Introduction to Tensors Matrix approximation with Core APIs Ragged tensors Working with sparse tensors Intro to Autoencoders Integrated gradients Client-efficient large-model federated learning via `federated_select` and sparse aggregation Preprocess data with TensorFlow Transform Tutorial on Multi Armed Bandits in TF-Agents This is the reduction operation for the elementwise tf.math.maximum op. Reduces input_tensor along the dimensions given in axis .
Unless keepdims is true, the rank of the tensor is reduced by 1 for each
of the entries in axis , which must be unique. If keepdims is true, the
reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a
tensor with a single element is returned. Usage example >>> x = tf . constant ([ 5 , 1 , 2 , 4 ]) >>> tf . reduce_max ( x ) < tf . Tensor : shape = (), dtype = int32 , numpy = 5 >
>>> x = tf . constant ([ - 5 , - 1 , - 2 , - 4 ]) >>> tf . reduce_max ( x ) < tf . Tensor : shape = (), dtype = int32 , numpy =- 1 >
>>> x = tf . constant ([ 4 , float ( 'nan' )]) >>> tf . reduce_max ( x ) < tf . Tensor : shape = (), dtype = float32 , numpy = nan >
>>> x = tf . constant ([ float ( 'nan' ), float ( 'nan' )]) >>> tf . reduce_max ( x ) < tf . Tensor : shape = (), dtype = float32 , numpy = nan >
>>> x = tf . constant ([ float ( '-inf' ), float ( 'inf' )]) >>> tf . reduce_max ( x ) < tf . Tensor : shape = (), dtype = float32 , numpy = inf > See the numpy docs for np.amax and np.nanmax behavior. Args input_tensor The tensor to reduce. Should have real numeric type. axis The dimensions to reduce. If None (the default), reduces all
dimensions. Must be in the range [-rank(input_tensor),
rank(input_tensor)) . keepdims If true, retains reduced dimensions with length 1. name A name for the operation (optional). Returns The reduced tensor.


Page: https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean
View source on GitHub Computes the mean of elements across dimensions of a tensor. View aliases Main aliases tf.reduce_mean tf . math . reduce_mean ( input_tensor , axis = None , keepdims = False , name = None ) Used in the notebooks Used in the guide Used in the tutorials Logistic regression for binary classification with Core APIs Multilayer perceptrons for digit recognition with Core APIs Distributed training with Core APIs and DTensor Quickstart for the TensorFlow Core APIs Introduction to gradients and automatic differentiation Transfer learning with YAMNet for environmental sound classification Learned data compression Uncertainty-aware Deep Learning with SNGP CycleGAN Neural style transfer Reduces input_tensor along the dimensions given in axis by computing the
mean of elements across the dimensions in axis .
Unless keepdims is true, the rank of the tensor is reduced by 1 for each
of the entries in axis , which must be unique. If keepdims is true, the
reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a tensor with a single
element is returned. For example: x = tf . constant ([[ 1. , 1. ], [ 2. , 2. ]]) tf . reduce_mean ( x ) < tf . Tensor : shape = (), dtype = float32 , numpy = 1.5 > tf . reduce_mean ( x , 0 ) < tf . Tensor : shape = ( 2 ,), dtype = float32 , numpy = array ([ 1.5 , 1.5 ], dtype = float32 ) > tf . reduce_mean ( x , 1 ) < tf . Tensor : shape = ( 2 ,), dtype = float32 , numpy = array ([ 1. , 2. ], dtype = float32 ) > Args input_tensor The tensor to reduce. Should have numeric type. axis The dimensions to reduce. If None (the default), reduces all
dimensions. Must be in the range [-rank(input_tensor),
rank(input_tensor)) . keepdims If true, retains reduced dimensions with length 1. name A name for the operation (optional). Returns The reduced tensor. numpy compatibility Equivalent to np.mean Please note that np.mean has a dtype parameter that could be used to
specify the output type. By default this is dtype=float64 . On the other
hand, tf.reduce_mean has an aggressive type inference from input_tensor ,
for example: x = tf . constant ([ 1 , 0 , 1 , 0 ]) tf . reduce_mean ( x ) < tf . Tensor : shape = (), dtype = int32 , numpy = 0 > y = tf . constant ([ 1. , 0. , 1. , 0. ]) tf . reduce_mean ( y ) < tf . Tensor : shape = (), dtype = float32 , numpy = 0.5 >


Page: https://www.tensorflow.org/api_docs/python/tf/math/reduce_min
View source on GitHub Computes the tf.math.minimum of elements across dimensions of a tensor. View aliases Main aliases tf.reduce_min tf . math . reduce_min ( input_tensor , axis = None , keepdims = False , name = None ) Used in the notebooks Used in the guide Used in the tutorials Ragged tensors Integrated gradients Intro to Autoencoders MoViNet for streaming action recognition TensorFlow Ranking Keras pipeline for distributed training This is the reduction operation for the elementwise tf.math.minimum op. Reduces input_tensor along the dimensions given in axis .
Unless keepdims is true, the rank of the tensor is reduced by 1 for each
of the entries in axis , which must be unique. If keepdims is true, the
reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a
tensor with a single element is returned. For example: a = tf . constant ([ [[ 1 , 2 ], [ 3 , 4 ]], [[ 1 , 2 ], [ 3 , 4 ]] ]) tf . reduce_min ( a ) < tf . Tensor : shape = (), dtype = int32 , numpy = 1 > Choosing a specific axis returns minimum element in the given axis: b = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) tf . reduce_min ( b , axis = 0 ) < tf . Tensor : shape = ( 3 ,), dtype = int32 , numpy = array ([ 1 , 2 , 3 ], dtype = int32 ) > tf . reduce_min ( b , axis = 1 ) < tf . Tensor : shape = ( 2 ,), dtype = int32 , numpy = array ([ 1 , 4 ], dtype = int32 ) > Setting keepdims to True retains the dimension of input_tensor : tf . reduce_min ( a , keepdims = True ) < tf . Tensor : shape = ( 1 , 1 , 1 ), dtype = int32 , numpy = array ([[[ 1 ]]], dtype = int32 ) > tf . math . reduce_min ( a , axis = 0 , keepdims = True ) < tf . Tensor : shape = ( 1 , 2 , 2 ), dtype = int32 , numpy = array ([[[ 1 , 2 ], [ 3 , 4 ]]], dtype = int32 ) > Args input_tensor The tensor to reduce. Should have real numeric type. axis The dimensions to reduce. If None (the default), reduces all
dimensions. Must be in the range [-rank(input_tensor),
rank(input_tensor)) . keepdims If true, retains reduced dimensions with length 1. name A name for the operation (optional). Returns The reduced tensor. numpy compatibility Equivalent to np.min


Page: https://www.tensorflow.org/api_docs/python/tf/math/reduce_prod
View source on GitHub Computes tf.math.multiply of elements across dimensions of a tensor. View aliases Main aliases tf.reduce_prod tf . math . reduce_prod ( input_tensor , axis = None , keepdims = False , name = None ) Used in the notebooks Used in the guide Used in the tutorials Advanced automatic differentiation Sparse weights using structural pruning Intro to Autoencoders TFP Release Notes notebook (0.12.1) TensorFlow Distributions: A Gentle Introduction This is the reduction operation for the elementwise tf.math.multiply op. Reduces input_tensor along the dimensions given in axis .
Unless keepdims is true, the rank of the tensor is reduced by 1 for each
entry in axis . If keepdims is true, the reduced dimensions
are retained with length 1. If axis is None, all dimensions are reduced, and a
tensor with a single element is returned. For example >>> x = tf . constant ([[ 1. , 2. ], [ 3. , 4. ]]) >>> tf . math . reduce_prod ( x ) < tf . Tensor : shape = (), dtype = float32 , numpy = 24. >
>>> tf . math . reduce_prod ( x , 0 ) < tf . Tensor : shape = ( 2 ,), dtype = float32 , numpy = array ([ 3. , 8. ], dtype = float32 ) >
>>> tf . math . reduce_prod ( x , 1 ) < tf . Tensor : shape = ( 2 ,), dtype = float32 , numpy = array ([ 2. , 12. ], dtype = float32 ) > Args input_tensor The tensor to reduce. Should have numeric type. axis The dimensions to reduce. If None (the default), reduces all
dimensions. Must be in the range [-rank(input_tensor),
rank(input_tensor)) . keepdims If true, retains reduced dimensions with length 1. name A name for the operation (optional). Returns The reduced tensor. numpy compatibility Equivalent to np.prod


Page: https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum
View source on GitHub Computes the sum of elements across dimensions of a tensor. View aliases Main aliases tf.reduce_sum tf . math . reduce_sum ( input_tensor , axis = None , keepdims = False , name = None ) Used in the notebooks Used in the guide Used in the tutorials Advanced automatic differentiation Better performance with tf.function TensorFlow basics Matrix approximation with Core APIs Extension types Using DTensors with Keras Distributed training with DTensors Convolutional Variational Autoencoder Neural style transfer Playing CartPole with the Actor-Critic method This is the reduction operation for the elementwise tf.math.add op. Reduces input_tensor along the dimensions given in axis .
Unless keepdims is true, the rank of the tensor is reduced by 1 for each
of the entries in axis , which must be unique. If keepdims is true, the
reduced dimensions are retained with length 1. If axis is None, all dimensions are reduced, and a
tensor with a single element is returned. For example >>> # x has a shape of (2, 3) (two rows and three columns): >>> x = tf . constant ([[ 1 , 1 , 1 ], [ 1 , 1 , 1 ]]) >>> x . numpy () array ([[ 1 , 1 , 1 ], [ 1 , 1 , 1 ]], dtype = int32 ) >>> # sum all the elements >>> # 1 + 1 + 1 + 1 + 1+ 1 = 6 >>> tf . reduce_sum ( x ) . numpy () 6 >>> # reduce along the first dimension >>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2] >>> tf . reduce_sum ( x , 0 ) . numpy () array ([ 2 , 2 , 2 ], dtype = int32 ) >>> # reduce along the second dimension >>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3] >>> tf . reduce_sum ( x , 1 ) . numpy () array ([ 3 , 3 ], dtype = int32 ) >>> # keep the original dimensions >>> tf . reduce_sum ( x , 1 , keepdims = True ) . numpy () array ([[ 3 ], [ 3 ]], dtype = int32 ) >>> # reduce along both dimensions >>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6 >>> # or, equivalently, reduce along rows, then reduce the resultant array >>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2] >>> # 2 + 2 + 2 = 6 >>> tf . reduce_sum ( x , [ 0 , 1 ]) . numpy () 6 Args input_tensor The tensor to reduce. Should have numeric type. axis The dimensions to reduce. If None (the default), reduces all
dimensions. Must be in the range [-rank(input_tensor),
rank(input_tensor)] . keepdims If true, retains reduced dimensions with length 1. name A name for the operation (optional). Returns The reduced tensor, of the same dtype as the input_tensor. numpy compatibility Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to
int64 while tensorflow returns the same dtype as the input.


Page: https://www.tensorflow.org/api_docs/python/tf/register_tensor_conversion_function
View source on GitHub Registers a function for converting objects of base_type to Tensor . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.register_tensor_conversion_function tf . register_tensor_conversion_function ( base_type , conversion_func , priority = 100 ) The conversion function must have the following signature: def conversion_func ( value , dtype = None , name = None , as_ref = False ): # ... It must return a Tensor with the given dtype if specified. If the
conversion function creates a new Tensor , it should use the given name if specified. All exceptions will be propagated to the caller. The conversion function may return NotImplemented for some
inputs. In this case, the conversion process will continue to try
subsequent conversion functions. If as_ref is true, the function must return a Tensor reference,
such as a Variable . Note: The conversion functions will execute in order of priority,
followed by order of registration. To ensure that a conversion function F runs before another conversion function G , ensure that F is
registered with a smaller priority than G . Args base_type The base type or tuple of base types for all objects that conversion_func accepts. conversion_func A function that converts instances of base_type to Tensor . priority Optional integer that indicates the priority for applying this
conversion function. Conversion functions with smaller priority values run
earlier than conversion functions with larger priority values. Defaults to
100. Raises TypeError If the arguments do not have the appropriate type.


Page: https://www.tensorflow.org/api_docs/python/tf/repeat
View source on GitHub Repeat elements of input . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.repeat tf . repeat ( input , repeats , axis = None , name = None ) Used in the notebooks Used in the tutorials Transfer learning with YAMNet for environmental sound classification Parametrized Quantum Circuits for Reinforcement Learning Recommend movies for users with TensorFlow Ranking Listwise ranking See also tf.concat , tf.stack , tf.tile . Args input An N -dimensional Tensor. repeats An 1-D int Tensor. The number of repetitions for each element.
repeats is broadcasted to fit the shape of the given axis. len(repeats) must equal input.shape[axis] if axis is not None. axis An int. The axis along which to repeat values. By default, (axis=None),
use the flattened input array, and return a flat output array. name A name for the operation. Returns A Tensor which has the same shape as input , except along the given axis.
If axis is None then the output array is flattened to match the flattened
input array. Example usage: repeat ([ 'a' , 'b' , 'c' ], repeats = [ 3 , 0 , 2 ], axis = 0 ) < tf . Tensor : shape = ( 5 ,), dtype = string , numpy = array ([ b 'a' , b 'a' , b 'a' , b 'c' , b 'c' ], dtype = object ) > repeat ([[ 1 , 2 ], [ 3 , 4 ]], repeats = [ 2 , 3 ], axis = 0 ) < tf . Tensor : shape = ( 5 , 2 ), dtype = int32 , numpy = array ([[ 1 , 2 ], [ 1 , 2 ], [ 3 , 4 ], [ 3 , 4 ], [ 3 , 4 ]], dtype = int32 ) > repeat ([[ 1 , 2 ], [ 3 , 4 ]], repeats = [ 2 , 3 ], axis = 1 ) < tf . Tensor : shape = ( 2 , 5 ), dtype = int32 , numpy = array ([[ 1 , 1 , 2 , 2 , 2 ], [ 3 , 3 , 4 , 4 , 4 ]], dtype = int32 ) > repeat ( 3 , repeats = 4 ) < tf . Tensor : shape = ( 4 ,), dtype = int32 , numpy = array ([ 3 , 3 , 3 , 3 ], dtype = int32 ) > repeat ([[ 1 , 2 ], [ 3 , 4 ]], repeats = 2 ) < tf . Tensor : shape = ( 8 ,), dtype = int32 , numpy = array ([ 1 , 1 , 2 , 2 , 3 , 3 , 4 , 4 ], dtype = int32 ) >


Page: https://www.tensorflow.org/api_docs/python/tf/required_space_to_batch_paddings
View source on GitHub Calculate padding required to make block_shape divide input_shape. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.required_space_to_batch_paddings tf . required_space_to_batch_paddings ( input_shape , block_shape , base_paddings = None , name = None ) This function can be used to calculate a suitable paddings argument for use
with space_to_batch_nd and batch_to_space_nd. Args input_shape int32 Tensor of shape [N]. block_shape int32 Tensor of shape [N]. base_paddings Optional int32 Tensor of shape [N, 2].  Specifies the minimum
amount of padding to use.  All elements must be >= 0.  If not specified,
defaults to 0. name string.  Optional name prefix. Returns (paddings, crops), where: paddings and crops are int32 Tensors of rank 2 and shape [N, 2] satisfying paddings[i, 0] = base_paddings[i, 0].
0 <= paddings[i, 1] - base_paddings[i, 1] < block_shape i % block_shape[i] == 0 crops[i, 0] = 0
crops[i, 1] = paddings[i, 1] - base_paddings[i, 1] Raises: ValueError if called with incompatible shapes.


Page: https://www.tensorflow.org/api_docs/python/tf/reshape
View source on GitHub Reshapes a tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.reshape tf . reshape ( tensor , shape , name = None ) Used in the notebooks Used in the guide Used in the tutorials Introduction to Tensors Advanced automatic differentiation DTensor concepts Multilayer perceptrons for digit recognition with Core APIs Distributed training with Core APIs and DTensor Learned data compression Adversarial example using FGSM Convolutional Variational Autoencoder Load CSV data Load text Given tensor , this operation returns a new tf.Tensor that has the same
values as tensor in the same order, except with a new shape given by shape . t1 = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]] print ( tf . shape ( t1 ) . numpy ()) [ 2 3 ] t2 = tf . reshape ( t1 , [ 6 ]) t2 < tf . Tensor : shape = ( 6 ,), dtype = int32 , numpy = array ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = int32 ) > tf . reshape ( t2 , [ 3 , 2 ]) < tf . Tensor : shape = ( 3 , 2 ), dtype = int32 , numpy = array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], dtype = int32 ) > The tf.reshape does not change the order of or the total number of elements
in the tensor, and so it can reuse the underlying data buffer. This makes it
a fast operation independent of how big of a tensor it is operating on. tf . reshape ([ 1 , 2 , 3 ], [ 2 , 2 ]) Traceback ( most recent call last ): InvalidArgumentError : Input to reshape is a tensor with 3 values , but the requested shape has 4 To instead reorder the data to rearrange the dimensions of a tensor, see tf.transpose . t = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]] tf . reshape ( t , [ 3 , 2 ]) . numpy () array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], dtype = int32 ) tf . transpose ( t , perm = [ 1 , 0 ]) . numpy () array ([[ 1 , 4 ], [ 2 , 5 ], [ 3 , 6 ]], dtype = int32 ) If one component of shape is the special value -1, the size of that
dimension is computed so that the total size remains constant.  In particular,
a shape of [-1] flattens into 1-D.  At most one component of shape can
be -1. t = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]] tf . reshape ( t , [ - 1 ]) < tf . Tensor : shape = ( 6 ,), dtype = int32 , numpy = array ([ 1 , 2 , 3 , 4 , 5 , 6 ], dtype = int32 ) > tf . reshape ( t , [ 3 , - 1 ]) < tf . Tensor : shape = ( 3 , 2 ), dtype = int32 , numpy = array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], dtype = int32 ) > tf . reshape ( t , [ - 1 , 2 ]) < tf . Tensor : shape = ( 3 , 2 ), dtype = int32 , numpy = array ([[ 1 , 2 ], [ 3 , 4 ], [ 5 , 6 ]], dtype = int32 ) > tf.reshape(t, []) reshapes a tensor t with one element to a scalar. tf . reshape ([ 7 ], []) . numpy () 7 More examples: t = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ] print ( tf . shape ( t ) . numpy ()) [ 9 ] tf . reshape ( t , [ 3 , 3 ]) < tf . Tensor : shape = ( 3 , 3 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]], dtype = int32 ) > t = [[[ 1 , 1 ], [ 2 , 2 ]], [[ 3 , 3 ], [ 4 , 4 ]]] print ( tf . shape ( t ) . numpy ()) [ 2 2 2 ] tf . reshape ( t , [ 2 , 4 ]) < tf . Tensor : shape = ( 2 , 4 ), dtype = int32 , numpy = array ([[ 1 , 1 , 2 , 2 ], [ 3 , 3 , 4 , 4 ]], dtype = int32 ) > t = [[[ 1 , 1 , 1 ], [ 2 , 2 , 2 ]], [[ 3 , 3 , 3 ], [ 4 , 4 , 4 ]], [[ 5 , 5 , 5 ], [ 6 , 6 , 6 ]]] print ( tf . shape ( t ) . numpy ()) [ 3 2 3 ] # Pass '[-1]' to flatten 't'. tf . reshape ( t , [ - 1 ]) < tf . Tensor : shape = ( 18 ,), dtype = int32 , numpy = array ([ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 , 4 , 4 , 4 , 5 , 5 , 5 , 6 , 6 , 6 ], dtype = int32 ) > # -- Using -1 to infer the shape -- # Here -1 is inferred to be 9: tf . reshape ( t , [ 2 , - 1 ]) < tf . Tensor : shape = ( 2 , 9 ), dtype = int32 , numpy = array ([[ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ], [ 4 , 4 , 4 , 5 , 5 , 5 , 6 , 6 , 6 ]], dtype = int32 ) > # -1 is inferred to be 2: tf . reshape ( t , [ - 1 , 9 ]) < tf . Tensor : shape = ( 2 , 9 ), dtype = int32 , numpy = array ([[ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ], [ 4 , 4 , 4 , 5 , 5 , 5 , 6 , 6 , 6 ]], dtype = int32 ) > # -1 is inferred to be 3: tf . reshape ( t , [ 2 , - 1 , 3 ]) < tf . Tensor : shape = ( 2 , 3 , 3 ), dtype = int32 , numpy = array ([[[ 1 , 1 , 1 ], [ 2 , 2 , 2 ], [ 3 , 3 , 3 ]], [[ 4 , 4 , 4 ], [ 5 , 5 , 5 ], [ 6 , 6 , 6 ]]], dtype = int32 ) > Args tensor A Tensor . shape A Tensor . Must be one of the following types: int32 , int64 .
Defines the shape of the output tensor. name Optional string. A name for the operation. Returns A Tensor . Has the same type as tensor .


Page: https://www.tensorflow.org/api_docs/python/tf/reverse
Reverses specific dimensions of a tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.manip.reverse , tf.compat.v1.reverse , tf.compat.v1.reverse_v2 tf . reverse ( tensor : Annotated [ Any , TV_ReverseV2_T ], axis : Annotated [ Any , TV_ReverseV2_Tidx ], name = None ) -> Annotated [ Any , TV_ReverseV2_T ] Used in the notebooks Used in the guide Ragged tensors Given a tensor , and a int32 tensor axis representing the set of
dimensions of tensor to reverse. This operation reverses each dimension i for which there exists j s.t. axis[j] == i . tensor can have up to 8 dimensions. The number of dimensions specified
in axis may be 0 or more entries. If an index is specified more than
once, a InvalidArgument error is raised. For example: # tensor 't' is [[[[ 0,  1,  2,  3], #                  [ 4,  5,  6,  7], #                  [ 8,  9, 10, 11]], #                 [[12, 13, 14, 15], #                  [16, 17, 18, 19], #                  [20, 21, 22, 23]]]] # tensor 't' shape is [1, 2, 3, 4] # 'dims' is [3] or 'dims' is [-1] reverse ( t , dims ) == > [[[[ 3 , 2 , 1 , 0 ], [ 7 , 6 , 5 , 4 ], [ 11 , 10 , 9 , 8 ]], [[ 15 , 14 , 13 , 12 ], [ 19 , 18 , 17 , 16 ], [ 23 , 22 , 21 , 20 ]]]] # 'dims' is '[1]' (or 'dims' is '[-3]') reverse ( t , dims ) == > [[[[ 12 , 13 , 14 , 15 ], [ 16 , 17 , 18 , 19 ], [ 20 , 21 , 22 , 23 ] [[ 0 , 1 , 2 , 3 ], [ 4 , 5 , 6 , 7 ], [ 8 , 9 , 10 , 11 ]]]] # 'dims' is '[2]' (or 'dims' is '[-2]') reverse ( t , dims ) == > [[[[ 8 , 9 , 10 , 11 ], [ 4 , 5 , 6 , 7 ], [ 0 , 1 , 2 , 3 ]] [[ 20 , 21 , 22 , 23 ], [ 16 , 17 , 18 , 19 ], [ 12 , 13 , 14 , 15 ]]]] Args tensor A Tensor . Must be one of the following types: uint8 , int8 , uint16 , int16 , int32 , uint32 , int64 , uint64 , bool , bfloat16 , half , float32 , float64 , complex64 , complex128 , string .
Up to 8-D. axis A Tensor . Must be one of the following types: int32 , int64 .
1-D. The indices of the dimensions to reverse. Must be in the range [-rank(tensor), rank(tensor)) . name A name for the operation (optional). Returns A Tensor . Has the same type as tensor .


Page: https://www.tensorflow.org/api_docs/python/tf/reverse_sequence
View source on GitHub Reverses variable length slices. tf . reverse_sequence ( input , seq_lengths , seq_axis = None , batch_axis = None , name = None ) This op first slices input along the dimension batch_axis , and for
each slice i , reverses the first seq_lengths[i] elements along the
dimension seq_axis . The elements of seq_lengths must obey seq_lengths[i] <=
input.dims[seq_axis] , and seq_lengths must be a vector of length input.dims[batch_axis] . The output slice i along dimension batch_axis is then given by
input slice i , with the first seq_lengths[i] slices along
dimension seq_axis reversed. Example usage: seq_lengths = [ 7 , 2 , 3 , 5 ] input = [[ 1 , 2 , 3 , 4 , 5 , 0 , 0 , 0 ], [ 1 , 2 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 1 , 2 , 3 , 4 , 0 , 0 , 0 , 0 ], [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ]] output = tf . reverse_sequence ( input , seq_lengths , seq_axis = 1 , batch_axis = 0 ) output < tf . Tensor : shape = ( 4 , 8 ), dtype = int32 , numpy = array ([[ 0 , 0 , 5 , 4 , 3 , 2 , 1 , 0 ], [ 2 , 1 , 0 , 0 , 0 , 0 , 0 , 0 ], [ 3 , 2 , 1 , 4 , 0 , 0 , 0 , 0 ], [ 5 , 4 , 3 , 2 , 1 , 6 , 7 , 8 ]], dtype = int32 ) > Args input A Tensor . The input to reverse. seq_lengths A Tensor . Must be one of the following types: int32 , int64 . 1-D with length input.dims(batch_axis) and max(seq_lengths) <=
input.dims(seq_axis) seq_axis An int . The dimension which is partially reversed. batch_axis An optional int . Defaults to 0 . The dimension along which
reversal is performed. name A name for the operation (optional). Returns A Tensor. Has the same type as input.


Page: https://www.tensorflow.org/api_docs/python/tf/rfftnd
ND fast real Fourier transform. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.rfftnd tf . rfftnd ( input : Annotated [ Any , TV_RFFTND_Treal ], fft_length : Annotated [ Any , _atypes . Int32 ], axes : Annotated [ Any , _atypes . Int32 ], Tcomplex : TV_RFFTND_Tcomplex = tf . dtypes . complex64 , name = None ) -> Annotated [ Any , TV_RFFTND_Tcomplex ] Computes the n-dimensional real discrete Fourier transform over designated
dimensions of input . The designated dimensions of input are assumed to be
the result of RFFTND . The length of the last axis transformed will be
fft_length[-1]//2+1. If fft_length[i] shape(input)[i], the input is padded with zeros. If fft_length
is not given, the default shape(input) is used. Axes mean the dimensions to perform the transform on. Default is to perform on
all axes. Args input A Tensor . Must be one of the following types: float32 , float64 .
A complex tensor. fft_length A Tensor of type int32 .
An int32 tensor. The FFT length for each dimension. axes A Tensor of type int32 .
An int32 tensor with a same shape as fft_length. Axes to perform the transform. Tcomplex An optional tf.DType from: tf.complex64, tf.complex128 . Defaults to tf.complex64 . name A name for the operation (optional). Returns A Tensor of type Tcomplex .


Page: https://www.tensorflow.org/api_docs/python/tf/roll
View source on GitHub Rolls the elements of a tensor along an axis. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.manip.roll , tf.compat.v1.roll tf . roll ( input , shift , axis , name = None ) Used in the notebooks Used in the tutorials DeepDream The elements are shifted positively (towards larger indices) by the offset of shift along the dimension of axis . Negative shift values will shift
elements in the opposite direction. Elements that roll passed the last position
will wrap around to the first and vice versa. Multiple shifts along multiple
axes may be specified. For example: # 't' is [0, 1, 2, 3, 4] roll ( t , shift = 2 , axis = 0 ) == > [ 3 , 4 , 0 , 1 , 2 ] # shifting along multiple dimensions # 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]] roll ( t , shift = [ 1 , - 2 ], axis = [ 0 , 1 ]) == > [[ 7 , 8 , 9 , 5 , 6 ], [ 2 , 3 , 4 , 0 , 1 ]] # shifting along the same axis multiple times # 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]] roll ( t , shift = [ 2 , - 3 ], axis = [ 1 , 1 ]) == > [[ 1 , 2 , 3 , 4 , 0 ], [ 6 , 7 , 8 , 9 , 5 ]] Args input A Tensor . shift A Tensor . Must be one of the following types: int32 , int64 .
Dimension must be 0-D or 1-D. shift[i] specifies the number of places by which
elements are shifted positively (towards larger indices) along the dimension
specified by axis[i] . Negative shifts will roll the elements in the opposite
direction. axis A Tensor . Must be one of the following types: int32 , int64 .
Dimension must be 0-D or 1-D. axis[i] specifies the dimension that the shift shift[i] should occur. If the same axis is referenced more than once, the
total shift for that axis will be the sum of all the shifts that belong to that
axis. name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/math/round
View source on GitHub Rounds the values of a tensor to the nearest integer, element-wise. View aliases Main aliases tf.round Compat aliases for migration See Migration guide for
more details. tf.compat.v1.round tf . math . round ( x , name = None ) Used in the notebooks Used in the tutorials Learned data compression Substantial Undocumented Infection Facilitates the Rapid Dissemination of Novel Coronavirus (SARS-CoV2) Research tools Super resolution with TensorFlow Lite Graph-based Neural Structured Learning in TFX Rounds half to even.  Also known as bankers rounding. If you want to round
according to the current system rounding mode use tf::cint.
For example: x = tf . constant ([ 0.9 , 2.5 , 2.3 , 1.5 , - 4.5 ]) tf . round ( x ) # [ 1.0, 2.0, 2.0, 2.0, -4.0 ] Args x A Tensor of type float16 , float32 , float64 , int32 , or int64 . name A name for the operation (optional). Returns A Tensor of same shape and type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/dtypes/saturate_cast
View source on GitHub Performs a safe saturating cast of value to dtype . View aliases Main aliases tf.saturate_cast Compat aliases for migration See Migration guide for
more details. tf.compat.v1.dtypes.saturate_cast , tf.compat.v1.saturate_cast tf . dtypes . saturate_cast ( value , dtype , name = None ) Used in the notebooks Used in the tutorials Learned data compression This function casts the input to dtype without overflow.  If
there is a danger that values would over or underflow in the cast, this op
applies the appropriate clamping before the cast.  See tf.cast for more
details. Args value A Tensor . dtype The desired output DType . name A name for the operation (optional). Returns value safely cast to dtype .


Page: https://www.tensorflow.org/api_docs/python/tf/math/scalar_mul
View source on GitHub Multiplies a scalar times a Tensor or IndexedSlices object. View aliases Main aliases tf.scalar_mul tf . math . scalar_mul ( scalar , x , name = None ) This is a special case of tf.math.multiply , where the first value must be a scalar . Unlike the general form of tf.math.multiply , this is operation is
guaranteed to be efficient for tf.IndexedSlices . x = tf . reshape ( tf . range ( 30 , dtype = tf . float32 ), [ 10 , 3 ]) with tf . GradientTape () as g : g . watch ( x ) y = tf . gather ( x , [ 1 , 2 ]) # IndexedSlices z = tf . math . scalar_mul ( 10.0 , y ) Args scalar A 0-D scalar Tensor . Must have known shape. x A Tensor or IndexedSlices to be scaled. name A name for the operation (optional). Returns scalar * x of the same type ( Tensor or IndexedSlices ) as x . Raises ValueError if scalar is not a 0-D scalar .


Page: https://www.tensorflow.org/api_docs/python/tf/scan
View source on GitHub scan on the list of tensors unpacked from elems on dimension 0. (deprecated argument values) tf . scan ( fn , elems , initializer = None , parallel_iterations = 10 , back_prop = True , swap_memory = False , infer_shape = True , reverse = False , name = None ) Deprecated: SOME ARGUMENT VALUES ARE DEPRECATED: (back_prop=False) . They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.scan(fn, elems, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.scan(fn, elems)) The simplest version of scan repeatedly applies the callable fn to a
sequence of elements from first to last. The elements are made of the tensors
unpacked from elems on dimension 0. The callable fn takes two tensors as
arguments. The first argument is the accumulated value computed from the
preceding invocation of fn, and the second is the value at the current
position of elems . If initializer is None, elems must contain at least
one element, and its first element is used as the initializer. Suppose that elems is unpacked into values , a list of tensors. The shape
of the result tensor is [len(values)] + fn(initializer, values[0]).shape .
If reverse=True, it's fn(initializer, values[-1]).shape. This method also allows multi-arity elems and accumulator.  If elems is a (possibly nested) list or tuple of tensors, then each of these tensors
must have a matching first (unpack) dimension.  The second argument of fn must match the structure of elems . If no initializer is provided, the output structure and dtypes of fn are assumed to be the same as its input; and in this case, the first
argument of fn must match the structure of elems . If an initializer is provided, then the output of fn must have the same
structure as initializer ; and the first argument of fn must match
this structure. For example, if elems is (t1, [t2, t3]) and initializer is [i1, i2] then an appropriate signature for fn in python2 is: fn = lambda (acc_p1, acc_p2), (t1, [t2, t3]): and fn must return a list, [acc_n1, acc_n2] .  An alternative correct signature for fn , and the
 one that works in python3 , is: fn = lambda a, t: , where a and t correspond to the input tuples. Args fn The callable to be performed.  It accepts two arguments.  The first will
have the same structure as initializer if one is provided, otherwise it
will have the same structure as elems .  The second will have the same
(possibly nested) structure as elems .  Its output must have the same
structure as initializer if one is provided, otherwise it must have the
same structure as elems . elems A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension.  The nested sequence of the
resulting slices will be the first argument to fn . initializer (optional) A tensor or (possibly nested) sequence of tensors,
initial value for the accumulator, and the expected output type of fn . parallel_iterations (optional) The number of iterations allowed to run in
parallel. back_prop (optional) Deprecated. False disables support for back
propagation. Prefer using tf.stop_gradient instead. swap_memory (optional) True enables GPU-CPU memory swapping. infer_shape (optional) False disables tests for consistent output shapes. reverse (optional) True scans the tensor last to first (instead of first to
last). name (optional) Name prefix for the returned tensors. Returns A tensor or (possibly nested) sequence of tensors.  Each tensor packs the
results of applying fn to tensors unpacked from elems along the first
dimension, and the previous accumulator value(s), from first to last (or
last to first, if reverse=True ). Raises TypeError if fn is not callable or the structure of the output of fn and initializer do not match. ValueError if the lengths of the output of fn and initializer do not match. Examples elems = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) sum = scan ( lambda a , x : a + x , elems ) # sum == [1, 3, 6, 10, 15, 21] sum = scan ( lambda a , x : a + x , elems , reverse = True ) # sum == [21, 20, 18, 15, 11, 6] elems = np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) initializer = np . array ( 0 ) sum_one = scan ( lambda a , x : x [ 0 ] - x [ 1 ] + a , ( elems + 1 , elems ), initializer ) # sum_one == [1, 2, 3, 4, 5, 6] elems = np . array ([ 1 , 0 , 0 , 0 , 0 , 0 ]) initializer = ( np . array ( 0 ), np . array ( 1 )) fibonaccis = scan ( lambda a , _ : ( a [ 1 ], a [ 0 ] + a [ 1 ]), elems , initializer ) # fibonaccis == ([1, 1, 2, 3, 5, 8], [1, 2, 3, 5, 8, 13])


Page: https://www.tensorflow.org/api_docs/python/tf/scatter_nd
Scatters updates into a tensor of shape shape according to indices . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.manip.scatter_nd , tf.compat.v1.scatter_nd tf . scatter_nd ( indices : Annotated [ Any , TV_ScatterNd_Tindices ], updates : Annotated [ Any , TV_ScatterNd_T ], shape : Annotated [ Any , TV_ScatterNd_Tindices ], name = None ) -> Annotated [ Any , TV_ScatterNd_T ] Used in the notebooks Used in the guide Introduction to tensor slicing Scatter sparse updates according to individual values at the specified indices . This op returns an output tensor with the shape you specify. This
op is the inverse of the tf.gather_nd operator which extracts values or slices
from a given tensor. This operation is similar to tf.tensor_scatter_nd_add , except that the tensor
is zero-initialized. Calling tf.scatter_nd(indices, updates, shape) is identical to calling tf.tensor_scatter_nd_add(tf.zeros(shape, updates.dtype), indices, updates) If indices contains duplicates, the associated updates are accumulated
(summed) into the output tensor. Warning: For floating-point data types, the output may be nondeterministic.
This is because the order in which the updates are applied is nondeterministic
and when floating-point numbers are added in different orders the resulting
numerical approximation error can be slightly different. However, the output
will be deterministic if op determinism is enabled via tf.config.experimental.enable_op_determinism . indices is an integer tensor containing indices into the output tensor. The
last dimension of indices can be at most the rank of shape : indices . shape [ - 1 ] < = shape . rank The last dimension of indices corresponds to indices of elements
(if indices.shape[-1] = shape.rank ) or slices
(if indices.shape[-1] < shape.rank ) along dimension indices.shape[-1] of shape . updates is a tensor with shape: indices . shape [: - 1 ] + shape [ indices . shape [ - 1 ]:] The simplest form of the scatter op is to insert individual elements in
a tensor by index. Consider an example where you want to insert 4 scattered
elements in a rank-1 tensor with 8 elements. In Python, this scatter operation would look like this: indices = tf . constant ([[ 4 ], [ 3 ], [ 1 ], [ 7 ]]) updates = tf . constant ([ 9 , 10 , 11 , 12 ]) shape = tf . constant ([ 8 ]) scatter = tf . scatter_nd ( indices , updates , shape ) print ( scatter ) The resulting tensor would look like this: [ 0 , 11 , 0 , 10 , 9 , 0 , 0 , 12 ] You can also insert entire slices of a higher rank tensor all at once. For
example, you can insert two slices in the first dimension of a rank-3 tensor
with two matrices of new values. In Python, this scatter operation would look like this: indices = tf . constant ([[ 1 ], [ 3 ]]) updates = tf . constant ([[[ 5 , 5 , 5 , 5 ], [ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ]], [[ 5 , 5 , 5 , 5 ], [ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ]]]) shape = tf . constant ([ 4 , 4 , 4 ]) scatter = tf . scatter_nd ( indices , updates , shape ) print ( scatter ) The resulting tensor would look like this: [[[ 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 ]], [[ 5 , 5 , 5 , 5 ], [ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ]], [[ 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 ]], [[ 5 , 5 , 5 , 5 ], [ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ]]] Note that on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, the index is ignored. Args indices A Tensor . Must be one of the following types: int16 , int32 , int64 .
Tensor of indices. updates A Tensor . Values to scatter into the output tensor. shape A Tensor . Must have the same type as indices .
1-D. The shape of the output tensor. name A name for the operation (optional). Returns A Tensor . Has the same type as updates .


Page: https://www.tensorflow.org/api_docs/python/tf/searchsorted
View source on GitHub Searches for where a value would go in a sorted sequence. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.searchsorted tf . searchsorted ( sorted_sequence , values , side = 'left' , out_type = tf . dtypes . int32 , name = None ) This is not a method for checking containment (like python in ). The typical use case for this operation is "binning", "bucketing", or
"discretizing". The values are assigned to bucket-indices based on the edges listed in sorted_sequence . This operation
returns the bucket-index for each value. edges = [ - 1 , 3.3 , 9.1 , 10.0 ] values = [ 0.0 , 4.1 , 12.0 ] tf . searchsorted ( edges , values ) . numpy () array ([ 1 , 2 , 4 ], dtype = int32 ) The side argument controls which index is returned if a value lands exactly
on an edge: seq = [ 0 , 3 , 9 , 10 , 10 ] values = [ 0 , 4 , 10 ] tf . searchsorted ( seq , values ) . numpy () array ([ 0 , 2 , 3 ], dtype = int32 ) tf . searchsorted ( seq , values , side = "right" ) . numpy () array ([ 1 , 2 , 5 ], dtype = int32 ) The axis is not settable for this operation. It always operates on the
innermost dimension ( axis=-1 ). The operation will accept any number of
outer dimensions. Here it is applied to the rows of a matrix: sorted_sequence = [[ 0. , 3. , 8. , 9. , 10. ], [ 1. , 2. , 3. , 4. , 5. ]] values = [[ 9.8 , 2.1 , 4.3 ], [ 0.1 , 6.6 , 4.5 , ]] tf . searchsorted ( sorted_sequence , values ) . numpy () array ([[ 4 , 1 , 2 ], [ 0 , 5 , 4 ]], dtype = int32 ) Note: This operation assumes that sorted_sequence is sorted along the
innermost axis, maybe using tf.sort(..., axis=-1) . If the sequence is not
sorted, no error is raised and the content of the returned tensor is not well
defined. Args sorted_sequence N-D Tensor containing a sorted sequence. values N-D Tensor containing the search values. side 'left' or 'right'; 'left' corresponds to lower_bound and 'right' to
upper_bound. out_type The output type ( int32 or int64 ).  Default is tf.int32 . name Optional name for the operation. Returns An N-D Tensor the size of values containing the result of applying
either lower_bound or upper_bound (depending on side) to each value.  The
result is not a global index to the entire Tensor , but the index in the
last dimension. Raises ValueError If the last dimension of sorted_sequence >= 2^31-1 elements.
If the total size of values exceeds 2^31 - 1 elements.
If the first N-1 dimensions of the two tensors don't match.


Page: https://www.tensorflow.org/api_docs/python/tf/sequence_mask
View source on GitHub Returns a mask tensor representing the first N positions of each cell. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.sequence_mask tf . sequence_mask ( lengths , maxlen = None , dtype = tf . dtypes . bool , name = None ) Used in the notebooks Used in the tutorials TFP Release Notes notebook (0.13.0) If lengths has shape [d_1, d_2, ..., d_n] the resulting tensor mask has
dtype dtype and shape [d_1, d_2, ..., d_n, maxlen] , with mask [ i_1 , i_2 , ... , i_n , j ] = ( j < lengths [ i_1 , i_2 , ... , i_n ]) Examples: tf . sequence_mask ([ 1 , 3 , 2 ], 5 ) # [[True, False, False, False, False], #  [True, True, True, False, False], #  [True, True, False, False, False]] tf . sequence_mask ([[ 1 , 3 ],[ 2 , 0 ]]) # [[[True, False, False], #   [True, True, True]], #  [[True, True, False], #   [False, False, False]]] Args lengths integer tensor, all its values <= maxlen. maxlen scalar integer tensor, size of last dimension of returned tensor.
Default is the maximum value in lengths . dtype output type of the resulting tensor. name name of the op. Returns A mask tensor of shape lengths.shape + (maxlen,) , cast to specified dtype. Raises ValueError if maxlen is not a scalar.


Page: https://www.tensorflow.org/api_docs/python/tf/shape
View source on GitHub Returns a tensor containing the shape of the input tensor. tf . shape ( input , out_type = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Effective Tensorflow 2 Ragged tensors Introduction to Tensors Customizing what happens in `fit()` with TensorFlow Customizing what happens in `fit()` DeepDream Neural style transfer Transfer learning with YAMNet for environmental sound classification pix2pix: Image-to-image translation with a conditional GAN Playing CartPole with the Actor-Critic method See also tf.size , tf.rank . tf.shape returns a 1-D integer tensor representing the shape of input .
For a scalar input, the tensor returned has a shape of (0,) and its value is
the empty vector (i.e. []). For example: tf . shape ( 1. ) < tf . Tensor : shape = ( 0 ,), dtype = int32 , numpy = array ([], dtype = int32 ) > t = tf . constant ([[[ 1 , 1 , 1 ], [ 2 , 2 , 2 ]], [[ 3 , 3 , 3 ], [ 4 , 4 , 4 ]]]) tf . shape ( t ) < tf . Tensor : shape = ( 3 ,), dtype = int32 , numpy = array ([ 2 , 2 , 3 ], dtype = int32 ) > Note: When using symbolic tensors, such as when using the Keras API,
tf.shape() will return the shape of the symbolic tensor. a = tf . keras . layers . Input (( None , 10 )) tf . shape ( a ) < ... shape = ( 3 ,) dtype = int32 ... > In these cases, using tf.Tensor.shape will return more informative results. a . shape TensorShape ([ None , None , 10 ]) (The first None represents the as yet unknown batch size.) tf.shape and Tensor.shape should be identical in eager mode.  Within tf.function or within a compat.v1 context, not all dimensions may be
known until execution time. Hence, when defining custom layers and models
for graph mode, prefer the dynamic tf.shape(x) over the static x.shape . Args input A Tensor or SparseTensor . out_type (Optional) The specified output type of the operation ( int32 or int64 ). Defaults to tf.int32 . (Note: there is an experimental
flag, tf_shape_default_int64 that changes the default to tf.int64 .
This is an unsupported, experimental setting that causes known breakages.) name A name for the operation (optional). Returns A Tensor of type out_type .


Page: https://www.tensorflow.org/api_docs/python/tf/shape_n
View source on GitHub Returns shape of a list of tensors. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.shape_n tf . shape_n ( input , out_type = tf . dtypes . int32 , name = None ) Given a list of tensors, tf.shape_n is much faster than applying tf.shape to each tensor individually. >>> a = tf . ones ([ 1 , 2 ]) >>> b = tf . ones ([ 2 , 3 ]) >>> c = tf . ones ([ 3 , 4 ]) >>> tf . shape_n ([ a , b , c ]) [ < tf . Tensor : shape = ( 2 ,), dtype = int32 , numpy = array ([ 1 , 2 ], dtype = int32 )>, < tf . Tensor : shape = ( 2 ,), dtype = int32 , numpy = array ([ 2 , 3 ], dtype = int32 )>, < tf . Tensor : shape = ( 2 ,), dtype = int32 , numpy = array ([ 3 , 4 ], dtype = int32 )>] Args input A list of at least 1 Tensor object with the same dtype. out_type The specified output type of the operation ( int32 or int64 ).
Defaults to tf.int32 (optional). name A name for the operation (optional). Returns A list of Tensor specifying the shape of each input tensor with type of out_type .


Page: https://www.tensorflow.org/api_docs/python/tf/math/sigmoid
View source on GitHub Computes sigmoid of x element-wise. View aliases Main aliases tf.nn.sigmoid , tf.sigmoid Compat aliases for migration See Migration guide for
more details. tf.compat.v1.sigmoid tf . math . sigmoid ( x , name = None ) Used in the notebooks Used in the guide Used in the tutorials Logistic regression for binary classification with Core APIs Advanced automatic differentiation Introduction to gradients and automatic differentiation Convolutional Variational Autoencoder Transfer learning and fine-tuning Classify structured data using Keras preprocessing layers Classify text with BERT Shape Constraints for Ethics with Tensorflow Lattice Formula for calculating \(\mathrm{sigmoid}(x) = y = 1 / (1 + \exp(-x))\). For \(x \in (-\infty, \infty)\), \(\mathrm{sigmoid}(x) \in (0, 1)\). Example Usage: If a positive number is large, then its sigmoid will approach to 1 since the
formula will be y = <large_num> / (1 + <large_num>) x = tf . constant ([ 0.0 , 1.0 , 50.0 , 100.0 ]) tf . math . sigmoid ( x ) < tf . Tensor : shape = ( 4 ,), dtype = float32 , numpy = array ([ 0.5 , 0.7310586 , 1.0 , 1.0 ], dtype = float32 ) > If a negative number is large, its sigmoid will approach to 0 since the
formula will be y = 1 / (1 + <large_num>) x = tf . constant ([ - 100.0 , - 50.0 , - 1.0 , 0.0 ]) tf . math . sigmoid ( x ) < tf . Tensor : shape = ( 4 ,), dtype = float32 , numpy = array ([ 0.0000000e+00 , 1.9287499e-22 , 2.6894143e-01 , 0.5 ], dtype = float32 ) > Args x A Tensor with type float16 , float32 , float64 , complex64 , or complex128 . name A name for the operation (optional). Returns A Tensor with the same type as x . Usage Example: x = tf . constant ([ - 128.0 , 0.0 , 128.0 ], dtype = tf . float32 ) tf . sigmoid ( x ) < tf . Tensor : shape = ( 3 ,), dtype = float32 , numpy = array ([ 0. , 0.5 , 1. ], dtype = float32 ) > scipy compatibility Equivalent to scipy.special.expit


Page: https://www.tensorflow.org/api_docs/python/tf/math/sign
View source on GitHub Returns an element-wise indication of the sign of a number. View aliases Main aliases tf.sign Compat aliases for migration See Migration guide for
more details. tf.compat.v1.sign tf . math . sign ( x , name = None ) Used in the notebooks Used in the tutorials Adversarial example using FGSM Tutorial on Multi Armed Bandits in TF-Agents y = sign(x) = -1 if x < 0; 0 if x == 0; 1 if x > 0 . For complex numbers, y = sign(x) = x / |x| if x != 0, otherwise y = 0 . Example usage: # real number tf . math . sign ([ 0. , 2. , - 3. ]) < tf . Tensor : shape = ( 3 ,), dtype = float32 , numpy = array ([ 0. , 1. , - 1. ], dtype = float32 ) > # complex number tf . math . sign ([ 1 + 1 j , 0 + 0 j ]) < tf . Tensor : shape = ( 2 ,), dtype = complex128 , numpy = array ([ 0.70710678 + 0.70710678 j , 0. + 0. j ]) > Args x A Tensor. Must be one of the following types: bfloat16, half, float32,
float64, int32, int64, complex64, complex128. name A name for the operation (optional). Returns A Tensor. Has the same type as x. If x is a SparseTensor, returns SparseTensor(x.indices,
  tf.math.sign(x.values, ...), x.dense_shape). If x is a SparseTensor , returns SparseTensor(x.indices, tf.math.sign(x.values, ...), x.dense_shape)


Page: https://www.tensorflow.org/api_docs/python/tf/math/sin
Computes sine of x element-wise. View aliases Main aliases tf.sin Compat aliases for migration See Migration guide for
more details. tf.compat.v1.sin tf . math . sin ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the guide Used in the tutorials Advanced automatic differentiation Introduction to gradients and automatic differentiation TFP Release Notes notebook (0.12.1) Given an input tensor, this function computes sine of every
  element in the tensor. Input range is (-inf, inf) and
  output range is [-1,1] . x = tf . constant ([ - float ( "inf" ), - 9 , - 0.5 , 1 , 1.2 , 200 , 10 , float ( "inf" )]) tf . math . sin ( x ) == > [ nan - 0.4121185 - 0.47942555 0.84147096 0.9320391 - 0.87329733 - 0.54402107 nan ] Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/sinh
Computes hyperbolic sine of x element-wise. View aliases Main aliases tf.sinh Compat aliases for migration See Migration guide for
more details. tf.compat.v1.sinh tf . math . sinh ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Given an input tensor, this function computes hyperbolic sine of every
  element in the tensor. Input range is [-inf,inf] and output range
  is [-inf,inf] . x = tf . constant ([ - float ( "inf" ), - 9 , - 0.5 , 1 , 1.2 , 2 , 10 , float ( "inf" )]) tf . math . sinh ( x ) == > [ - inf - 4.0515420e+03 - 5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf ] Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/size
View source on GitHub Returns the size of a tensor. tf . size ( input , out_type = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Matrix approximation with Core APIs Introduction to Tensors BERT Preprocessing with TF Text Scalable model compression Client-efficient large-model federated learning via `federated_select` and sparse aggregation Sending Different Data To Particular Clients With tff.federated_select Human Pose Classification with MoveNet and TensorFlow Lite Federated Learning for Image Classification See also tf.shape . Returns a 0-D Tensor representing the number of elements in input of type out_type . Defaults to tf.int32. For example: t = tf . constant ([[[ 1 , 1 , 1 ], [ 2 , 2 , 2 ]], [[ 3 , 3 , 3 ], [ 4 , 4 , 4 ]]]) tf . size ( t ) < tf . Tensor : shape = (), dtype = int32 , numpy = 12 > Args input A Tensor or SparseTensor . out_type (Optional) The specified non-quantized numeric output type of the
operation. Defaults to tf.int32 . (Note: there is an experimental
flag, tf_shape_default_int64 that changes the default to tf.int64 .
This is an unsupported, experimental setting that causes known breakages.) name A name for the operation (optional). Returns A Tensor of type out_type . Defaults to tf.int32 . numpy compatibility Equivalent to np.size()


Page: https://www.tensorflow.org/api_docs/python/tf/slice
View source on GitHub Extracts a slice from a tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.slice tf . slice ( input_ , begin , size , name = None ) Used in the notebooks Used in the guide Used in the tutorials Introduction to tensor slicing TFLite Authoring Tool TensorFlow Lite Model Analyzer Client-efficient large-model federated learning via `federated_select` and sparse aggregation Graph regularization for sentiment classification using synthesized graphs See also tf.strided_slice . This operation extracts a slice of size size from a tensor input_ starting
at the location specified by begin . The slice size is represented as a
tensor shape, where size[i] is the number of elements of the 'i'th dimension
of input_ that you want to slice. The starting location ( begin ) for the
slice is represented as an offset in each dimension of input_ . In other
words, begin[i] is the offset into the i'th dimension of input_ that you
want to slice from. Note that tf.Tensor. getitem is typically a more pythonic way to
perform slices, as it allows you to write foo[3:7, :-2] instead of tf.slice(foo, [3, 0], [4, foo.get_shape()[1]-2]) . begin is zero-based; size is one-based. If size[i] is -1,
all remaining elements in dimension i are included in the
slice. In other words, this is equivalent to setting: size[i] = input_.dim_size(i) - begin[i] This operation requires that: 0 <= begin[i] <= begin[i] + size[i] <= Di  for i in [0, n] For example: t = tf . constant ([[[ 1 , 1 , 1 ], [ 2 , 2 , 2 ]], [[ 3 , 3 , 3 ], [ 4 , 4 , 4 ]], [[ 5 , 5 , 5 ], [ 6 , 6 , 6 ]]]) tf . slice ( t , [ 1 , 0 , 0 ], [ 1 , 1 , 3 ]) # [[[3, 3, 3]]] tf . slice ( t , [ 1 , 0 , 0 ], [ 1 , 2 , 3 ]) # [[[3, 3, 3], #   [4, 4, 4]]] tf . slice ( t , [ 1 , 0 , 0 ], [ 2 , 1 , 3 ]) # [[[3, 3, 3]], #  [[5, 5, 5]]] Args input_ A Tensor . begin An int32 or int64 Tensor . size An int32 or int64 Tensor . name A name for the operation (optional). Returns A Tensor the same type as input_ .


Page: https://www.tensorflow.org/api_docs/python/tf/sort
View source on GitHub Sorts a tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.sort tf . sort ( values , axis =- 1 , direction = 'ASCENDING' , name = None ) Used in the notebooks Used in the tutorials TFP Release Notes notebook (0.11.0) Usage: a = [ 1 , 10 , 26.9 , 2.8 , 166.32 , 62.3 ] tf . sort ( a ) . numpy () array ([ 1. , 2.8 , 10. , 26.9 , 62.3 , 166.32 ], dtype = float32 ) tf . sort ( a , direction = 'DESCENDING' ) . numpy () array ([ 166.32 , 62.3 , 26.9 , 10. , 2.8 , 1. ], dtype = float32 ) For multidimensional inputs you can control which axis the sort is applied
along. The default axis=-1 sorts the innermost axis. mat = [[ 3 , 2 , 1 ], [ 2 , 1 , 3 ], [ 1 , 3 , 2 ]] tf . sort ( mat , axis =- 1 ) . numpy () array ([[ 1 , 2 , 3 ], [ 1 , 2 , 3 ], [ 1 , 2 , 3 ]], dtype = int32 ) tf . sort ( mat , axis = 0 ) . numpy () array ([[ 1 , 1 , 1 ], [ 2 , 2 , 2 ], [ 3 , 3 , 3 ]], dtype = int32 ) See also tf.argsort : Like sort, but it returns the sort indices. tf.math.top_k : A partial sort that returns a fixed number of top values
and corresponding indices. Args values 1-D or higher numeric Tensor . axis The axis along which to sort. The default is -1, which sorts the last
axis. direction The direction in which to sort the values ( 'ASCENDING' or 'DESCENDING' ). name Optional name for the operation. Returns A Tensor with the same dtype and shape as values , with the elements
sorted along the given axis . Raises tf.errors.InvalidArgumentError If the values.dtype is not a float or int type. ValueError If axis is not a constant scalar, or the direction is invalid.


Page: https://www.tensorflow.org/api_docs/python/tf/space_to_batch
View source on GitHub SpaceToBatch for N-D tensors of type T. View aliases Main aliases tf.nn.space_to_batch tf . space_to_batch ( input , block_shape , paddings , name = None ) This operation divides "spatial" dimensions [1, ..., M] of the input into a
grid of blocks of shape block_shape , and interleaves these blocks with the
"batch" dimension (0) such that in the output, the spatial dimensions [1, ..., M] correspond to the position within the grid, and the batch
dimension combines both the position within a spatial block and the original
batch position.  Prior to division into blocks, the spatial dimensions of the
input are optionally zero padded according to paddings . See below for a
precise description. This operation is equivalent to the following steps: Zero-pad the start and end of dimensions [1, ..., M] of the
input according to paddings to produce padded of shape padded_shape . Reshape padded to reshaped_padded of shape: [batch] +
 [padded_shape[1] / block_shape[0],
   block_shape[0],
  ...,
  padded_shape[M] / block_shape[M-1],
  block_shape[M-1]] +
 remaining_shape Permute dimensions of reshaped_padded to produce permuted_reshaped_padded of shape: block_shape +
 [batch] +
 [padded_shape[1] / block_shape[0],
  ...,
  padded_shape[M] / block_shape[M-1]] +
 remaining_shape Reshape permuted_reshaped_padded to flatten block_shape into the batch
dimension, producing an output tensor of shape: [batch * prod(block_shape)] +
 [padded_shape[1] / block_shape[0],
  ...,
  padded_shape[M] / block_shape[M-1]] +
 remaining_shape Some examples: (1) For the following input of shape [1, 2, 2, 1] , block_shape = [2, 2] , and paddings = [[0, 0], [0, 0]] : x = [[[[ 1 ], [ 2 ]], [[ 3 ], [ 4 ]]]] The output tensor has shape [4, 1, 1, 1] and value: [[[[ 1 ]]], [[[ 2 ]]], [[[ 3 ]]], [[[ 4 ]]]] (2) For the following input of shape [1, 2, 2, 3] , block_shape = [2, 2] , and paddings = [[0, 0], [0, 0]] : x = [[[[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], [[ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]]] The output tensor has shape [4, 1, 1, 3] and value: [[[[ 1 , 2 , 3 ]]], [[[ 4 , 5 , 6 ]]], [[[ 7 , 8 , 9 ]]], [[[ 10 , 11 , 12 ]]]] (3) For the following input of shape [1, 4, 4, 1] , block_shape = [2, 2] , and paddings = [[0, 0], [0, 0]] : x = [[[[ 1 ], [ 2 ], [ 3 ], [ 4 ]], [[ 5 ], [ 6 ], [ 7 ], [ 8 ]], [[ 9 ], [ 10 ], [ 11 ], [ 12 ]], [[ 13 ], [ 14 ], [ 15 ], [ 16 ]]]] The output tensor has shape [4, 2, 2, 1] and value: x = [[[[ 1 ], [ 3 ]], [[ 9 ], [ 11 ]]], [[[ 2 ], [ 4 ]], [[ 10 ], [ 12 ]]], [[[ 5 ], [ 7 ]], [[ 13 ], [ 15 ]]], [[[ 6 ], [ 8 ]], [[ 14 ], [ 16 ]]]] (4) For the following input of shape [2, 2, 4, 1] , block_shape = [2, 2] , and
    paddings = [[0, 0], [2, 0]] : x = [[[[ 1 ], [ 2 ], [ 3 ], [ 4 ]], [[ 5 ], [ 6 ], [ 7 ], [ 8 ]]], [[[ 9 ], [ 10 ], [ 11 ], [ 12 ]], [[ 13 ], [ 14 ], [ 15 ], [ 16 ]]]] The output tensor has shape [8, 1, 3, 1] and value: x = [[[[ 0 ], [ 1 ], [ 3 ]]], [[[ 0 ], [ 9 ], [ 11 ]]], [[[ 0 ], [ 2 ], [ 4 ]]], [[[ 0 ], [ 10 ], [ 12 ]]], [[[ 0 ], [ 5 ], [ 7 ]]], [[[ 0 ], [ 13 ], [ 15 ]]], [[[ 0 ], [ 6 ], [ 8 ]]], [[[ 0 ], [ 14 ], [ 16 ]]]] Among others, this operation is useful for reducing atrous convolution into
regular convolution. Args input A Tensor .
N-D with shape input_shape = [batch] + spatial_shape + remaining_shape ,
where spatial_shape has M dimensions. block_shape A Tensor . Must be one of the following types: int32 , int64 .
1-D with shape [M] , all values must be >= 1. paddings A Tensor . Must be one of the following types: int32 , int64 .
2-D with shape [M, 2] , all values must be >= 0. paddings[i] = [pad_start, pad_end] specifies the padding for input dimension i + 1 , which corresponds to spatial dimension i .  It is required that block_shape[i] divides input_shape[i + 1] + pad_start + pad_end . name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd
SpaceToBatch for N-D tensors of type T. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.manip.space_to_batch_nd , tf.compat.v1.space_to_batch_nd tf . space_to_batch_nd ( input : Annotated [ Any , TV_SpaceToBatchND_T ], block_shape : Annotated [ Any , TV_SpaceToBatchND_Tblock_shape ], paddings : Annotated [ Any , TV_SpaceToBatchND_Tpaddings ], name = None ) -> Annotated [ Any , TV_SpaceToBatchND_T ] This operation divides "spatial" dimensions [1, ..., M] of the input into a
grid of blocks of shape block_shape , and interleaves these blocks with the
"batch" dimension (0) such that in the output, the spatial dimensions [1, ..., M] correspond to the position within the grid, and the batch
dimension combines both the position within a spatial block and the original
batch position.  Prior to division into blocks, the spatial dimensions of the
input are optionally zero padded according to paddings . See below for a
precise description. This operation is equivalent to the following steps: Zero-pad the start and end of dimensions [1, ..., M] of the
input according to paddings to produce padded of shape padded_shape . Reshape padded to reshaped_padded of shape: [batch] +
 [padded_shape[1] / block_shape[0],
   block_shape[0],
  ...,
  padded_shape[M] / block_shape[M-1],
  block_shape[M-1]] +
 remaining_shape Permute dimensions of reshaped_padded to produce permuted_reshaped_padded of shape: block_shape +
 [batch] +
 [padded_shape[1] / block_shape[0],
  ...,
  padded_shape[M] / block_shape[M-1]] +
 remaining_shape Reshape permuted_reshaped_padded to flatten block_shape into the batch
dimension, producing an output tensor of shape: [batch * prod(block_shape)] +
 [padded_shape[1] / block_shape[0],
  ...,
  padded_shape[M] / block_shape[M-1]] +
 remaining_shape Some examples: (1) For the following input of shape [1, 2, 2, 1] , block_shape = [2, 2] , and paddings = [[0, 0], [0, 0]] : x = [[[[ 1 ], [ 2 ]], [[ 3 ], [ 4 ]]]] The output tensor has shape [4, 1, 1, 1] and value: [[[[ 1 ]]], [[[ 2 ]]], [[[ 3 ]]], [[[ 4 ]]]] (2) For the following input of shape [1, 2, 2, 3] , block_shape = [2, 2] , and paddings = [[0, 0], [0, 0]] : x = [[[[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], [[ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]]] The output tensor has shape [4, 1, 1, 3] and value: [[[[ 1 , 2 , 3 ]]], [[[ 4 , 5 , 6 ]]], [[[ 7 , 8 , 9 ]]], [[[ 10 , 11 , 12 ]]]] (3) For the following input of shape [1, 4, 4, 1] , block_shape = [2, 2] , and paddings = [[0, 0], [0, 0]] : x = [[[[ 1 ], [ 2 ], [ 3 ], [ 4 ]], [[ 5 ], [ 6 ], [ 7 ], [ 8 ]], [[ 9 ], [ 10 ], [ 11 ], [ 12 ]], [[ 13 ], [ 14 ], [ 15 ], [ 16 ]]]] The output tensor has shape [4, 2, 2, 1] and value: x = [[[[ 1 ], [ 3 ]], [[ 9 ], [ 11 ]]], [[[ 2 ], [ 4 ]], [[ 10 ], [ 12 ]]], [[[ 5 ], [ 7 ]], [[ 13 ], [ 15 ]]], [[[ 6 ], [ 8 ]], [[ 14 ], [ 16 ]]]] (4) For the following input of shape [2, 2, 4, 1] , block_shape = [2, 2] , and
    paddings = [[0, 0], [2, 0]] : x = [[[[ 1 ], [ 2 ], [ 3 ], [ 4 ]], [[ 5 ], [ 6 ], [ 7 ], [ 8 ]]], [[[ 9 ], [ 10 ], [ 11 ], [ 12 ]], [[ 13 ], [ 14 ], [ 15 ], [ 16 ]]]] The output tensor has shape [8, 1, 3, 1] and value: x = [[[[ 0 ], [ 1 ], [ 3 ]]], [[[ 0 ], [ 9 ], [ 11 ]]], [[[ 0 ], [ 2 ], [ 4 ]]], [[[ 0 ], [ 10 ], [ 12 ]]], [[[ 0 ], [ 5 ], [ 7 ]]], [[[ 0 ], [ 13 ], [ 15 ]]], [[[ 0 ], [ 6 ], [ 8 ]]], [[[ 0 ], [ 14 ], [ 16 ]]]] Among others, this operation is useful for reducing atrous convolution into
regular convolution. Args input A Tensor .
N-D with shape input_shape = [batch] + spatial_shape + remaining_shape ,
where spatial_shape has M dimensions. block_shape A Tensor . Must be one of the following types: int32 , int64 .
1-D with shape [M] , all values must be >= 1. paddings A Tensor . Must be one of the following types: int32 , int64 .
2-D with shape [M, 2] , all values must be >= 0. paddings[i] = [pad_start, pad_end] specifies the padding for input dimension i + 1 , which corresponds to spatial dimension i .  It is required that block_shape[i] divides input_shape[i + 1] + pad_start + pad_end . name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/split
View source on GitHub Splits a tensor value into a list of sub tensors. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.split tf . split ( value , num_or_size_splits , axis = 0 , num = None , name = 'split' ) Used in the notebooks Used in the guide Used in the tutorials Distributed training with Core APIs and DTensor Understanding masking & padding Using DTensors with Keras Distributed training with DTensors Convolutional Variational Autoencoder MoViNet for streaming action recognition Bayesian Modeling with Joint Distribution See also tf.unstack . If num_or_size_splits is an int ,  then it splits value along the
dimension axis into num_or_size_splits smaller tensors. This requires that value.shape[axis] is divisible by num_or_size_splits . If num_or_size_splits is a 1-D Tensor (or list), then value is split into len(num_or_size_splits) elements. The shape of the i -th
element has the same size as the value except along dimension axis where
the size is num_or_size_splits[i] . For example: x = tf . Variable ( tf . random . uniform ([ 5 , 30 ], - 1 , 1 )) # Split `x` into 3 tensors along dimension 1 s0 , s1 , s2 = tf . split ( x , num_or_size_splits = 3 , axis = 1 ) tf . shape ( s0 ) . numpy () array ([ 5 , 10 ], dtype = int32 ) # Split `x` into 3 tensors with sizes [4, 15, 11] along dimension 1 split0 , split1 , split2 = tf . split ( x , [ 4 , 15 , 11 ], 1 ) tf . shape ( split0 ) . numpy () array ([ 5 , 4 ], dtype = int32 ) tf . shape ( split1 ) . numpy () array ([ 5 , 15 ], dtype = int32 ) tf . shape ( split2 ) . numpy () array ([ 5 , 11 ], dtype = int32 ) Args value The Tensor to split. num_or_size_splits Either an int indicating the number of splits
along axis or a 1-D integer Tensor or Python list containing the sizes
of each output tensor along axis . If an int , then it must evenly
divide value.shape[axis] ; otherwise the sum of sizes along the split
axis must match that of the value . axis An int or scalar int32 Tensor . The dimension along which
to split. Must be in the range [-rank(value), rank(value)) . Defaults to
0. num Optional, an int , used to specify the number of outputs when it
cannot be inferred from the shape of size_splits . name A name for the operation (optional). Returns if num_or_size_splits is an int returns a list of num_or_size_splits Tensor objects; if num_or_size_splits is a 1-D
list or 1-D Tensor returns num_or_size_splits.get_shape[0] Tensor objects resulting from splitting value . Raises ValueError If num is unspecified and cannot be inferred. ValueError If num_or_size_splits is a scalar Tensor .


Page: https://www.tensorflow.org/api_docs/python/tf/math/sqrt
View source on GitHub Computes element-wise square root of the input tensor. View aliases Main aliases tf.sqrt Compat aliases for migration See Migration guide for
more details. tf.compat.v1.sqrt tf . math . sqrt ( x , name = None ) Used in the notebooks Used in the guide Used in the tutorials Distributed training with Core APIs and DTensor Multilayer perceptrons for digit recognition with Core APIs Optimizers with Core APIs Uncertainty-aware Deep Learning with SNGP TFP Release Notes notebook (0.12.1) Copulas Primer Substantial Undocumented Infection Facilitates the Rapid Dissemination of Novel Coronavirus (SARS-CoV2) Neural machine translation with a Transformer and Keras Note: This operation does not support integer types. x = tf . constant ([[ 4.0 ], [ 16.0 ]]) tf . sqrt ( x ) < tf . Tensor : shape = ( 2 , 1 ), dtype = float32 , numpy = array ([[ 2. ], [ 4. ]], dtype = float32 ) > y = tf . constant ([[ - 4.0 ], [ 16.0 ]]) tf . sqrt ( y ) < tf . Tensor : shape = ( 2 , 1 ), dtype = float32 , numpy = array ([[ nan ], [ 4. ]], dtype = float32 ) > z = tf . constant ([[ - 1.0 ], [ 16.0 ]], dtype = tf . complex128 ) tf . sqrt ( z ) < tf . Tensor : shape = ( 2 , 1 ), dtype = complex128 , numpy = array ([[ 0.0 + 1. j ], [ 4.0 + 0. j ]]) > Note: In order to support complex type, please provide an input tensor
of complex64 or complex128 . Args x A tf.Tensor of type bfloat16 , half , float32 , float64 , complex64 , complex128 name A name for the operation (optional). Returns A tf.Tensor of same size, type and sparsity as x . If x is a SparseTensor , returns SparseTensor(x.indices, tf.math.sqrt(x.values, ...), x.dense_shape)


Page: https://www.tensorflow.org/api_docs/python/tf/math/square
Computes square of x element-wise. View aliases Main aliases tf.square Compat aliases for migration See Migration guide for
more details. tf.compat.v1.square tf . math . square ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the guide Used in the tutorials Better performance with tf.function Basic training loops TensorFlow basics Distributed training with Core APIs and DTensor Multilayer perceptrons for digit recognition with Core APIs Customization basics: tensors and operations I.e., \(y = x * x = x^2\). tf . math . square ([ - 2. , 0. , 3. ]) < tf . Tensor : shape = ( 3 ,), dtype = float32 , numpy = array ([ 4. , 0. , 9. ], dtype = float32 ) > Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , int8 , int16 , int32 , int64 , uint8 , uint16 , uint32 , uint64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x . If x is a SparseTensor , returns SparseTensor(x.indices, tf.math.square(x.values, ...), x.dense_shape)


Page: https://www.tensorflow.org/api_docs/python/tf/squeeze
View source on GitHub Removes dimensions of size 1 from the shape of a tensor. tf . squeeze ( input , axis = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Logistic regression for binary classification with Core APIs Multilayer perceptrons for digit recognition with Core APIs Quickstart for the TensorFlow Core APIs Unicode strings Learned data compression Generate music with an RNN Simple audio recognition: Recognizing keywords Intro to Autoencoders Transfer learning with YAMNet for environmental sound classification Given a tensor input , this operation returns a tensor of the same type with
all dimensions of size 1 removed. If you don't want to remove all size 1
dimensions, you can remove specific size 1 dimensions by specifying axis . For example: # 't' is a tensor of shape [1, 2, 1, 3, 1, 1] tf . shape ( tf . squeeze ( t )) # [2, 3] Or, to remove specific size 1 dimensions: # 't' is a tensor of shape [1, 2, 1, 3, 1, 1] tf . shape ( tf . squeeze ( t , [ 2 , 4 ])) # [1, 2, 3, 1] Unlike the older op tf.compat.v1.squeeze , this op does not accept a
deprecated squeeze_dims argument. Note: if input is a tf.RaggedTensor , then this operation takes O(N) time, where N is the number of elements in the squeezed dimensions. Note: If squeeze is performed on dimensions of unknown sizes, then the
returned Tensor will be of unknown shape. A common situation is when the
first (batch) dimension is of size None , tf.squeeze returns <unknown> shape which may be a surprise. Specify the axis= argument
to get the expected result, as illustrated in the following example: @tf . function def func ( x ): print ( 'x.shape:' , x . shape ) known_axes = [ i for i , size in enumerate ( x . shape ) if size == 1 ] y = tf . squeeze ( x , axis = known_axes ) print ( 'shape of tf.squeeze(x, axis=known_axes):' , y . shape ) y = tf . squeeze ( x ) print ( 'shape of tf.squeeze(x):' , y . shape ) return 0 _ = func . get_concrete_function ( tf . TensorSpec ([ None , 1 , 2 ], dtype = tf . int32 )) # Output is. # x.shape: (None, 1, 2) # shape of tf.squeeze(x, axis=known_axes): (None, 2) # shape of tf.squeeze(x): <unknown> Args input A Tensor . The input to squeeze. axis An optional list of ints . Defaults to [] . If specified, only
squeezes the dimensions listed. The dimension index starts at 0. It is an
error to squeeze a dimension that is not 1. Must be in the range [-rank(input), rank(input)) . Must be specified if input is a RaggedTensor . name A name for the operation (optional). Returns A Tensor . Has the same type as input .
Contains the same data as input , but has one or more dimensions of
size 1 removed. Raises ValueError The input cannot be converted to a tensor, or the specified
axis cannot be squeezed.


Page: https://www.tensorflow.org/api_docs/python/tf/stack
View source on GitHub Stacks a list of rank- R tensors into one rank- (R+1) tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.stack tf . stack ( values , axis = 0 , name = 'stack' ) Used in the notebooks Used in the guide Used in the tutorials Extension types tf.data: Build TensorFlow input pipelines TensorFlow basics Random number generation Time series forecasting Custom training: walkthrough pix2pix: Image-to-image translation with a conditional GAN Overfit and underfit Load CSV data See also tf.concat , tf.tile , tf.repeat . Packs the list of tensors in values into a tensor with rank one higher than
each tensor in values , by packing them along the axis dimension.
Given a list of length N of tensors of shape (A, B, C) ; if axis == 0 then the output tensor will have the shape (N, A, B, C) .
if axis == 1 then the output tensor will have the shape (A, N, B, C) .
Etc. For example: x = tf . constant ([ 1 , 4 ]) y = tf . constant ([ 2 , 5 ]) z = tf . constant ([ 3 , 6 ]) tf . stack ([ x , y , z ]) < tf . Tensor : shape = ( 3 , 2 ), dtype = int32 , numpy = array ([[ 1 , 4 ], [ 2 , 5 ], [ 3 , 6 ]], dtype = int32 ) > tf . stack ([ x , y , z ], axis = 1 ) < tf . Tensor : shape = ( 2 , 3 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], dtype = int32 ) > This is the opposite of unstack.  The numpy equivalent is np.stack np . array_equal ( np . stack ([ x , y , z ]), tf . stack ([ x , y , z ])) True Args values A list of Tensor objects with the same shape and type. axis An int . The axis to stack along. Defaults to the first dimension.
Negative values wrap around, so the valid range is [-(R+1), R+1) . name A name for this operation (optional). Returns output A stacked Tensor with the same type as values . Raises ValueError If axis is out of the range [-(R+1), R+1).


Page: https://www.tensorflow.org/api_docs/python/tf/stop_gradient
View source on GitHub Stops gradient computation. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.stop_gradient tf . stop_gradient ( input , name = None ) Used in the notebooks Used in the guide Advanced automatic differentiation When executed in a graph, this op outputs its input tensor as-is. When building ops to compute gradients, this op prevents the contribution of
its inputs to be taken into account.  Normally, the gradient generator adds ops
to a graph to compute the derivatives of a specified 'loss' by recursively
finding out inputs that contributed to its computation.  If you insert this op
in the graph it inputs are masked from the gradient generator.  They are not
taken into account for computing gradients. This is useful any time you want to compute a value with TensorFlow but need
to pretend that the value was a constant. For example, the softmax function
for a vector x can be written as def softmax ( x ): numerator = tf . exp ( x ) denominator = tf . reduce_sum ( numerator ) return numerator / denominator This however is susceptible to overflow if the values in x are large. An
alternative more stable way is to subtract the maximum of x from each of the
values. def stable_softmax ( x ): z = x - tf . reduce_max ( x ) numerator = tf . exp ( z ) denominator = tf . reduce_sum ( numerator ) return numerator / denominator However, when we backprop through the softmax to x, we dont want to backprop
through the tf.reduce_max(x) (if the max values are not unique then the
gradient could flow to the wrong input) calculation and treat that as a
constant. Therefore, we should write this out as def stable_softmax ( x ): z = x - tf . stop_gradient ( tf . reduce_max ( x )) numerator = tf . exp ( z ) denominator = tf . reduce_sum ( numerator ) return numerator / denominator Some other examples include: The EM algorithm where the M-step should not involve backpropagation
through the output of the E-step . Contrastive divergence training of Boltzmann machines where, when
differentiating the energy function, the training must not backpropagate
through the graph that generated the samples from the model. Adversarial training, where no backprop should happen through the adversarial
example generation process. Args input A Tensor . name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/strided_slice
View source on GitHub Extracts a strided slice of a tensor (generalized Python array indexing). View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.strided_slice tf . strided_slice ( input_ , begin , end , strides = None , begin_mask = 0 , end_mask = 0 , ellipsis_mask = 0 , new_axis_mask = 0 , shrink_axis_mask = 0 , var = None , name = None ) See also tf.slice . Instead of calling this op directly most users will want to use the
NumPy-style slicing syntax (e.g. tensor[..., 3:4:-1, tf.newaxis, 3] ), which
is supported via tf.Tensor. getitem and tf.Variable. getitem . The interface of this op is a low-level encoding of the slicing syntax. Roughly speaking, this op extracts a slice of size (end-begin)/stride from the given input_ tensor. Starting at the location specified by begin the slice continues by adding stride to the index until all dimensions are
not less than end .
Note that a stride can be negative, which causes a reverse slice. Given a Python slice input[spec0, spec1, ..., specn] ,
this function will be called as follows. begin , end , and strides will be vectors of length n.
n in general is not equal to the rank of the input_ tensor. In each mask field ( begin_mask , end_mask , ellipsis_mask , new_axis_mask , shrink_axis_mask ) the ith bit will correspond to
the ith spec. If the ith bit of begin_mask is set, begin[i] is ignored and
the fullest possible range in that dimension is used instead. end_mask works analogously, except with the end range. foo[5:,:,:3] on a 7x8x9 tensor is equivalent to foo[5:7,0:8,0:3] . foo[::-1] reverses a tensor with shape 8. If the ith bit of ellipsis_mask is set, as many unspecified dimensions
as needed will be inserted between other dimensions. Only one
non-zero bit is allowed in ellipsis_mask . For example foo[3:5,...,4:5] on a shape 10x3x3x10 tensor is
equivalent to foo[3:5,:,:,4:5] and foo[3:5,...] is equivalent to foo[3:5,:,:,:] . If the ith bit of new_axis_mask is set, then begin , end , and stride are ignored and a new length 1 dimension is
added at this point in the output tensor. For example, foo[:4, tf.newaxis, :2] would produce a shape (4, 1, 2) tensor. If the ith bit of shrink_axis_mask is set, it implies that the ith
specification shrinks the dimensionality by 1, taking on the value at index begin[i] . end[i] and strides[i] are ignored in this case. For example in
Python one might do foo[:, 3, :] which would result in shrink_axis_mask equal to 2. Note: begin and end are zero-indexed. strides entries must be non-zero. t = tf . constant ([[[ 1 , 1 , 1 ], [ 2 , 2 , 2 ]], [[ 3 , 3 , 3 ], [ 4 , 4 , 4 ]], [[ 5 , 5 , 5 ], [ 6 , 6 , 6 ]]]) tf . strided_slice ( t , [ 1 , 0 , 0 ], [ 2 , 1 , 3 ], [ 1 , 1 , 1 ]) # [[[3, 3, 3]]] tf . strided_slice ( t , [ 1 , 0 , 0 ], [ 2 , 2 , 3 ], [ 1 , 1 , 1 ]) # [[[3, 3, 3], #   [4, 4, 4]]] tf . strided_slice ( t , [ 1 , - 1 , 0 ], [ 2 , - 3 , 3 ], [ 1 , - 1 , 1 ]) # [[[4, 4, 4], #   [3, 3, 3]]] Args input_ A Tensor . begin An int32 or int64 Tensor . end An int32 or int64 Tensor . strides An int32 or int64 Tensor . begin_mask An int32 mask. end_mask An int32 mask. ellipsis_mask An int32 mask. new_axis_mask An int32 mask. shrink_axis_mask An int32 mask. var The variable corresponding to input_ or None name A name for the operation (optional). Returns A Tensor the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/math/subtract
View source on GitHub Returns x - y element-wise. View aliases Main aliases tf.subtract Compat aliases for migration See Migration guide for
more details. tf.compat.v1.math.subtract , tf.compat.v1.subtract tf . math . subtract ( x , y , name = None ) Note: tf.subtract supports broadcasting. More about broadcasting here Both input and output have a range (-inf, inf) . Example usages below. Subtract operation between an array and a scalar: x = [ 1 , 2 , 3 , 4 , 5 ] y = 1 tf . subtract ( x , y ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 0 , 1 , 2 , 3 , 4 ], dtype = int32 ) > tf . subtract ( y , x ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 0 , - 1 , - 2 , - 3 , - 4 ], dtype = int32 ) > Note that binary - operator can be used instead: x = tf . convert_to_tensor ([ 1 , 2 , 3 , 4 , 5 ]) y = tf . convert_to_tensor ( 1 ) x - y < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 0 , 1 , 2 , 3 , 4 ], dtype = int32 ) > Subtract operation between an array and a tensor of same shape: x = [ 1 , 2 , 3 , 4 , 5 ] y = tf . constant ([ 5 , 4 , 3 , 2 , 1 ]) tf . subtract ( y , x ) < tf . Tensor : shape = ( 5 ,), dtype = int32 , numpy = array ([ 4 , 2 , 0 , - 2 , - 4 ], dtype = int32 ) > Warning: If one of the inputs ( x or y ) is a tensor and the other is a
non-tensor, the non-tensor input will adopt (or get casted to) the data type
of the tensor input. This can potentially cause unwanted overflow or underflow
conversion. For example, x = tf . constant ([ 1 , 2 ], dtype = tf . int8 ) y = [ 2 ** 8 + 1 , 2 ** 8 + 2 ] tf . subtract ( x , y ) < tf . Tensor : shape = ( 2 ,), dtype = int8 , numpy = array ([ 0 , 0 ], dtype = int8 ) > When subtracting two input values of different shapes, tf.subtract follows the general broadcasting rules . The two input array shapes are compared element-wise. Starting with the
trailing dimensions, the two dimensions either have to be equal or one of them
needs to be 1 . For example, x = np . ones ( 6 ) . reshape ( 2 , 3 , 1 ) y = np . ones ( 6 ) . reshape ( 2 , 1 , 3 ) tf . subtract ( x , y ) < tf . Tensor : shape = ( 2 , 3 , 3 ), dtype = float64 , numpy = array ([[[ 0. , 0. , 0. ], [ 0. , 0. , 0. ], [ 0. , 0. , 0. ]], [[ 0. , 0. , 0. ], [ 0. , 0. , 0. ], [ 0. , 0. , 0. ]]]) > Example with inputs of different dimensions: x = np . ones ( 6 ) . reshape ( 2 , 3 , 1 ) y = np . ones ( 6 ) . reshape ( 1 , 6 ) tf . subtract ( x , y ) < tf . Tensor : shape = ( 2 , 3 , 6 ), dtype = float64 , numpy = array ([[[ 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. ]], [[ 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0. , 0. , 0. , 0. ]]]) > Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , uint8 , int8 , uint16 , int16 , int32 , int64 , complex64 , complex128 , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/switch_case
View source on GitHub Create a switch/case operation, i.e. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.switch_case tf . switch_case ( branch_index , branch_fns , default = None , name = 'switch_case' ) an integer-indexed conditional. See also tf.case . This op can be substantially more efficient than tf.case when exactly one
branch will be selected. tf.switch_case is more like a C++ switch/case
statement than tf.case , which is more like an if/elif/elif/else chain. The branch_fns parameter is either a dict from int to callables, or list
of ( int , callable) pairs, or simply a list of callables (in which case the
index is implicitly the key). The branch_index Tensor is used to select an
element in branch_fns with matching int key, falling back to default if none match, or max(keys) if no default is provided. The keys must form
a contiguous set from 0 to len(branch_fns) - 1 . tf.switch_case supports nested structures as implemented in tf.nest . All
callables must return the same (possibly nested) value structure of lists,
tuples, and/or named tuples. Example: Pseudocode: switch ( branch_index ) { // c-style switch case 0 : return 17 ; case 1 : return 31 ; default : return -1 ; } or branches = { 0 : lambda : 17 , 1 : lambda : 31 } branches . get ( branch_index , lambda : - 1 )() Expressions: def f1 (): return tf . constant ( 17 ) def f2 (): return tf . constant ( 31 ) def f3 (): return tf . constant ( - 1 ) r = tf . switch_case ( branch_index , branch_fns = { 0 : f1 , 1 : f2 }, default = f3 ) # Equivalent: tf.switch_case(branch_index, branch_fns={0: f1, 1: f2, 2: f3}) Args branch_index An int Tensor specifying which of branch_fns should be
executed. branch_fns A dict mapping int s to callables, or a list of ( int ,
callable) pairs, or simply a list of callables (in which case the index
serves as the key). Each callable must return a matching structure of
tensors. default Optional callable that returns a structure of tensors. name A name for this operation (optional). Returns The tensors returned by the callable identified by branch_index , or those
returned by default if no key matches and default was provided, or those
returned by the max-keyed branch_fn if no default is provided. Raises TypeError If branch_fns is not a list/dictionary. TypeError If branch_fns is a list but does not contain 2-tuples or
callables. TypeError If fns[i] is not callable for any i, or default is not
callable.


Page: https://www.tensorflow.org/api_docs/python/tf/math/tan
Computes tan of x element-wise. View aliases Main aliases tf.tan Compat aliases for migration See Migration guide for
more details. tf.compat.v1.tan tf . math . tan ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Given an input tensor, this function computes tangent of every
  element in the tensor. Input range is (-inf, inf) and
  output range is (-inf, inf) . If input lies outside the boundary, nan is returned. x = tf . constant ([ - float ( "inf" ), - 9 , - 0.5 , 1 , 1.2 , 200 , 10000 , float ( "inf" )]) tf . math . tan ( x ) == > [ nan 0.45231566 - 0.5463025 1.5574077 2.572152 - 1.7925274 0.32097113 nan ] Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/math/tanh
Computes hyperbolic tangent of x element-wise. View aliases Main aliases tf.nn.tanh , tf.tanh Compat aliases for migration See Migration guide for
more details. tf.compat.v1.tanh tf . math . tanh ( x : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Used in the notebooks Used in the guide Better performance with tf.function Given an input tensor, this function computes hyperbolic tangent of every
  element in the tensor. Input range is [-inf, inf] and
  output range is [-1,1] . x = tf . constant ([ - float ( "inf" ), - 5 , - 0.5 , 1 , 1.2 , 2 , 3 , float ( "inf" )]) tf . math . tanh ( x ) < tf . Tensor : shape = ( 8 ,), dtype = float32 , numpy = array ([ - 1.0 , - 0.99990916 , - 0.46211717 , 0.7615942 , 0.8336547 , 0.9640276 , 0.9950547 , 1.0 ], dtype = float32 ) > Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , complex64 , complex128 . name A name for the operation (optional). Returns A Tensor . Has the same type as x . If x is a SparseTensor , returns SparseTensor(x.indices, tf.math.tanh(x.values, ...), x.dense_shape)


Page: https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add
Adds sparse updates to an existing tensor according to indices . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.tensor_scatter_add , tf.compat.v1.tensor_scatter_nd_add tf . tensor_scatter_nd_add ( tensor : Annotated [ Any , TV_TensorScatterAdd_T ], indices : Annotated [ Any , TV_TensorScatterAdd_Tindices ], updates : Annotated [ Any , TV_TensorScatterAdd_T ], name = None ) -> Annotated [ Any , TV_TensorScatterAdd_T ] Used in the notebooks Used in the guide Extension types Introduction to tensor slicing This operation creates a new tensor by adding sparse updates to the passed
in tensor .
This operation is very similar to tf.compat.v1.scatter_nd_add , except that the
updates are added onto an existing tensor (as opposed to a variable). If the
memory for the existing tensor cannot be re-used, a copy is made and updated. indices is an integer tensor containing indices into a new tensor of shape tensor.shape .  The last dimension of indices can be at most the rank of tensor.shape : indices . shape [ - 1 ] < = tensor . shape . rank The last dimension of indices corresponds to indices into elements
(if indices.shape[-1] = tensor.shape.rank ) or slices
(if indices.shape[-1] < tensor.shape.rank ) along dimension indices.shape[-1] of tensor.shape . updates is a tensor with shape indices . shape [: - 1 ] + tensor . shape [ indices . shape [ - 1 ]:] The simplest form of tensor_scatter_nd_add is to add individual elements to a
tensor by index. For example, say we want to add 4 elements in a rank-1
tensor with 8 elements. In Python, this scatter add operation would look like this: indices = tf . constant ([[ 4 ], [ 3 ], [ 1 ], [ 7 ]]) updates = tf . constant ([ 9 , 10 , 11 , 12 ]) tensor = tf . ones ([ 8 ], dtype = tf . int32 ) updated = tf . tensor_scatter_nd_add ( tensor , indices , updates ) updated < tf . Tensor : shape = ( 8 ,), dtype = int32 , numpy = array ([ 1 , 12 , 1 , 11 , 10 , 1 , 1 , 13 ], dtype = int32 ) > We can also, insert entire slices of a higher rank tensor all at once. For
example, if we wanted to insert two slices in the first dimension of a
rank-3 tensor with two matrices of new values. In Python, this scatter add operation would look like this: indices = tf . constant ([[ 0 ], [ 2 ]]) updates = tf . constant ([[[ 5 , 5 , 5 , 5 ], [ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ]], [[ 5 , 5 , 5 , 5 ], [ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ]]]) tensor = tf . ones ([ 4 , 4 , 4 ], dtype = tf . int32 ) updated = tf . tensor_scatter_nd_add ( tensor , indices , updates ) updated < tf . Tensor : shape = ( 4 , 4 , 4 ), dtype = int32 , numpy = array ([[[ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ], [ 9 , 9 , 9 , 9 ]], [[ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ]], [[ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ], [ 9 , 9 , 9 , 9 ]], [[ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ]]], dtype = int32 ) > Note: on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, the index is ignored. Args tensor A Tensor . Tensor to copy/update. indices A Tensor . Must be one of the following types: int32 , int64 .
Index tensor. updates A Tensor . Must have the same type as tensor .
Updates to scatter into output. name A name for the operation (optional). Returns A Tensor . Has the same type as tensor .


Page: https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_max
Apply a sparse update to a tensor taking the element-wise maximum. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.tensor_scatter_nd_max tf . tensor_scatter_nd_max ( tensor : Annotated [ Any , TV_TensorScatterMax_T ], indices : Annotated [ Any , TV_TensorScatterMax_Tindices ], updates : Annotated [ Any , TV_TensorScatterMax_T ], name = None ) -> Annotated [ Any , TV_TensorScatterMax_T ] Used in the notebooks Used in the guide Introduction to tensor slicing Returns a new tensor copied from tensor whose values are element-wise maximum between
tensor and updates according to the indices. tensor = [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] indices = [[ 1 ], [ 4 ], [ 5 ]] updates = [ 1 , - 1 , 1 ] tf . tensor_scatter_nd_max ( tensor , indices , updates ) . numpy () array ([ 0 , 1 , 0 , 0 , 0 , 1 , 0 , 0 ], dtype = int32 ) Refer to tf.tensor_scatter_nd_update for more details. Args tensor A Tensor . Tensor to update. indices A Tensor . Must be one of the following types: int32 , int64 .
Index tensor. updates A Tensor . Must have the same type as tensor .
Updates to scatter into output. name A name for the operation (optional). Returns A Tensor . Has the same type as tensor .


Page: https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_min
View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.tensor_scatter_nd_min tf . tensor_scatter_nd_min ( tensor : Annotated [ Any , TV_TensorScatterMin_T ], indices : Annotated [ Any , TV_TensorScatterMin_Tindices ], updates : Annotated [ Any , TV_TensorScatterMin_T ], name = None ) -> Annotated [ Any , TV_TensorScatterMin_T ] Used in the notebooks Used in the guide Introduction to tensor slicing Args tensor A Tensor . Tensor to update. indices A Tensor . Must be one of the following types: int32 , int64 .
Index tensor. updates A Tensor . Must have the same type as tensor .
Updates to scatter into output. name A name for the operation (optional). Returns A Tensor . Has the same type as tensor .


Page: https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_sub
Subtracts sparse updates from an existing tensor according to indices . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.tensor_scatter_nd_sub , tf.compat.v1.tensor_scatter_sub tf . tensor_scatter_nd_sub ( tensor : Annotated [ Any , TV_TensorScatterSub_T ], indices : Annotated [ Any , TV_TensorScatterSub_Tindices ], updates : Annotated [ Any , TV_TensorScatterSub_T ], name = None ) -> Annotated [ Any , TV_TensorScatterSub_T ] Used in the notebooks Used in the guide Introduction to tensor slicing This operation creates a new tensor by subtracting sparse updates from the
passed in tensor .
This operation is very similar to tf.scatter_nd_sub , except that the updates
are subtracted from an existing tensor (as opposed to a variable). If the memory
for the existing tensor cannot be re-used, a copy is made and updated. indices is an integer tensor containing indices into a new tensor of shape shape .  The last dimension of indices can be at most the rank of shape : indices . shape [ - 1 ] < = shape . rank The last dimension of indices corresponds to indices into elements
(if indices.shape[-1] = shape.rank ) or slices
(if indices.shape[-1] < shape.rank ) along dimension indices.shape[-1] of shape . updates is a tensor with shape indices . shape [: - 1 ] + shape [ indices . shape [ - 1 ]:] The simplest form of tensor_scatter_sub is to subtract individual elements
from a tensor by index. For example, say we want to insert 4 scattered elements
in a rank-1 tensor with 8 elements. In Python, this scatter subtract operation would look like this: indices = tf . constant ([[ 4 ], [ 3 ], [ 1 ], [ 7 ]]) updates = tf . constant ([ 9 , 10 , 11 , 12 ]) tensor = tf . ones ([ 8 ], dtype = tf . int32 ) updated = tf . tensor_scatter_nd_sub ( tensor , indices , updates ) print ( updated ) The resulting tensor would look like this: [ 1 , - 10 , 1 , - 9 , - 8 , 1 , 1 , - 11 ] We can also, insert entire slices of a higher rank tensor all at once. For
example, if we wanted to insert two slices in the first dimension of a
rank-3 tensor with two matrices of new values. In Python, this scatter add operation would look like this: indices = tf . constant ([[ 0 ], [ 2 ]]) updates = tf . constant ([[[ 5 , 5 , 5 , 5 ], [ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ]], [[ 5 , 5 , 5 , 5 ], [ 6 , 6 , 6 , 6 ], [ 7 , 7 , 7 , 7 ], [ 8 , 8 , 8 , 8 ]]]) tensor = tf . ones ([ 4 , 4 , 4 ], dtype = tf . int32 ) updated = tf . tensor_scatter_nd_sub ( tensor , indices , updates ) print ( updated ) The resulting tensor would look like this: [[[ - 4 , - 4 , - 4 , - 4 ], [ - 5 , - 5 , - 5 , - 5 ], [ - 6 , - 6 , - 6 , - 6 ], [ - 7 , - 7 , - 7 , - 7 ]], [[ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ]], [[ - 4 , - 4 , - 4 , - 4 ], [ - 5 , - 5 , - 5 , - 5 ], [ - 6 , - 6 , - 6 , - 6 ], [ - 7 , - 7 , - 7 , - 7 ]], [[ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 ]]] Note that on CPU, if an out of bound index is found, an error is returned.
On GPU, if an out of bound index is found, the index is ignored. Args tensor A Tensor . Tensor to copy/update. indices A Tensor . Must be one of the following types: int32 , int64 .
Index tensor. updates A Tensor . Must have the same type as tensor .
Updates to scatter into output. name A name for the operation (optional). Returns A Tensor . Has the same type as tensor .


Page: https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update
View source on GitHub Scatter updates into an existing tensor according to indices . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.tensor_scatter_nd_update , tf.compat.v1.tensor_scatter_update tf . tensor_scatter_nd_update ( tensor , indices , updates , name = None ) This operation creates a new tensor by applying sparse updates to the
input tensor . This is similar to an index assignment. # Not implemented: tensors cannot be updated inplace. tensor [ indices ] = updates If an out of bound index is found on CPU, an error is returned. Warning: There are some GPU specific semantics for this operation. If an out of bound index is found, the index is ignored. The order in which updates are applied is nondeterministic, so the output
will be nondeterministic if indices contains duplicates. This operation is very similar to tf.scatter_nd , except that the updates are
scattered onto an existing tensor (as opposed to a zero-tensor). If the memory
for the existing tensor cannot be re-used, a copy is made and updated. In general: indices is an integer tensor - the indices to update in tensor . indices has at least two axes, the last axis is the depth of the
index vectors. For each index vector in indices there is a corresponding entry in updates . If the length of the index vectors matches the rank of the tensor , then
the index vectors each point to scalars in tensor and each update is a
scalar. If the length of the index vectors is less than the rank of tensor , then
the index vectors each point to the slices of tensor and shape of the updates
must match that slice. Overall this leads to the following shape constraints: assert tf . rank ( indices ) > = 2 index_depth = indices . shape [ - 1 ] batch_shape = indices . shape [: - 1 ] assert index_depth < = tf . rank ( tensor ) outer_shape = tensor . shape [: index_depth ] inner_shape = tensor . shape [ index_depth :] assert updates . shape == batch_shape + inner_shape Typical usage is often much simpler than this general form, and it
can be better understood starting with simple examples: Scalar updates The simplest usage inserts scalar elements into a tensor by index.
In this case, the index_depth must equal the rank of the
input tensor , slice each column of indices is an index into an axis of the
input tensor . In this simplest case the shape constraints are: num_updates , index_depth = indices . shape . as_list () assert updates . shape == [ num_updates ] assert index_depth == tf . rank ( tensor ) ` For example, to insert 4 scattered elements in a rank-1 tensor with
8 elements. This scatter operation would look like this: tensor = [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] # tf.rank(tensor) == 1 indices = [[ 1 ], [ 3 ], [ 4 ], [ 7 ]] # num_updates == 4, index_depth == 1 updates = [ 9 , 10 , 11 , 12 ] # num_updates == 4 print ( tf . tensor_scatter_nd_update ( tensor , indices , updates )) tf . Tensor ([ 0 9 0 10 11 0 0 12 ], shape = ( 8 ,), dtype = int32 ) The length (first axis) of updates must equal the length of the indices : num_updates . This is the number of updates being inserted. Each scalar
update is inserted into tensor at the indexed location. For a higher rank input tensor scalar updates can be inserted by using an index_depth that matches tf.rank(tensor) : tensor = [[ 1 , 1 ], [ 1 , 1 ], [ 1 , 1 ]] # tf.rank(tensor) == 2 indices = [[ 0 , 1 ], [ 2 , 0 ]] # num_updates == 2, index_depth == 2 updates = [ 5 , 10 ] # num_updates == 2 print ( tf . tensor_scatter_nd_update ( tensor , indices , updates )) tf . Tensor ( [[ 1 5 ] [ 1 1 ] [ 10 1 ]], shape = ( 3 , 2 ), dtype = int32 ) Slice updates When the input tensor has more than one axis scatter can be used to update
entire slices. In this case it's helpful to think of the input tensor as being a two level
array-of-arrays. The shape of this two level array is split into the outer_shape and the inner_shape . indices indexes into the outer level of the input tensor ( outer_shape ).
and replaces the sub-array at that location with the corresponding item from
the updates list. The shape of each update is inner_shape . When updating a list of slices the shape constraints are: num_updates , index_depth = indices . shape . as_list () outer_shape = tensor . shape [: index_depth ] inner_shape = tensor . shape [ index_depth :] assert updates . shape == [ num_updates , inner_shape ] For example, to update rows of a (6, 3) tensor : tensor = tf . zeros ([ 6 , 3 ], dtype = tf . int32 ) Use an index depth of one. indices = tf . constant ([[ 2 ], [ 4 ]]) # num_updates == 2, index_depth == 1 num_updates , index_depth = indices . shape . as_list () The outer_shape is 6 , the inner shape is 3 : outer_shape = tensor . shape [: index_depth ] inner_shape = tensor . shape [ index_depth :] 2 rows are being indexed so 2 updates must be supplied.
Each update must be shaped to match the inner_shape . # num_updates == 2, inner_shape==3 updates = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) Altogether this gives: tf . tensor_scatter_nd_update ( tensor , indices , updates ) . numpy () array ([[ 0 , 0 , 0 ], [ 0 , 0 , 0 ], [ 1 , 2 , 3 ], [ 0 , 0 , 0 ], [ 4 , 5 , 6 ], [ 0 , 0 , 0 ]], dtype = int32 ) More slice update examples A tensor representing a batch of uniformly sized video clips naturally has 5
axes: [batch_size, time, width, height, channels] . For example: batch_size , time , width , height , channels = 13 , 11 , 7 , 5 , 3 video_batch = tf . zeros ([ batch_size , time , width , height , channels ]) To replace a selection of video clips: Use an index_depth of 1 (indexing the outer_shape : [batch_size] ) Provide updates each with a shape matching the inner_shape : [time, width, height, channels] . To replace the first two clips with ones: indices = [[ 0 ],[ 1 ]] new_clips = tf . ones ([ 2 , time , width , height , channels ]) tf . tensor_scatter_nd_update ( video_batch , indices , new_clips ) To replace a selection of frames in the videos: indices must have an index_depth of 2 for the outer_shape : [batch_size, time] . updates must be shaped like a list of images.  Each update must have a
shape, matching the inner_shape : [width, height, channels] . To replace the first frame of the first three video clips: indices = [[ 0 , 0 ], [ 1 , 0 ], [ 2 , 0 ]] # num_updates=3, index_depth=2 new_images = tf . ones ([ # num_updates=3, inner_shape=(width, height, channels) 3 , width , height , channels ]) tf . tensor_scatter_nd_update ( video_batch , indices , new_images ) Folded indices In simple cases it's convenient to think of indices and updates as
lists, but this is not a strict requirement. Instead of a flat num_updates ,
the indices and updates can be folded into a batch_shape . This batch_shape is all axes of the indices , except for the innermost index_depth axis. index_depth = indices . shape [ - 1 ] batch_shape = indices . shape [: - 1 ] Note: The one exception is that the batch_shape cannot be [] . You can't
update a single index by passing indices with shape [index_depth] . updates must have a matching batch_shape (the axes before inner_shape ). assert updates . shape == batch_shape + inner_shape Note: The result is equivalent to flattening the batch_shape axes of indices and updates . This generalization just avoids the need
for reshapes when it is more natural to construct "folded" indices and
updates. With this generalization the full shape constraints are: assert tf . rank ( indices ) > = 2 index_depth = indices . shape [ - 1 ] batch_shape = indices . shape [: - 1 ] assert index_depth < = tf . rank ( tensor ) outer_shape = tensor . shape [: index_depth ] inner_shape = tensor . shape [ index_depth :] assert updates . shape == batch_shape + inner_shape For example, to draw an X on a (5,5) matrix start with these indices: tensor = tf . zeros ([ 5 , 5 ]) indices = tf . constant ([ [[ 0 , 0 ], [ 1 , 1 ], [ 2 , 2 ], [ 3 , 3 ], [ 4 , 4 ]], [[ 0 , 4 ], [ 1 , 3 ], [ 2 , 2 ], [ 3 , 1 ], [ 4 , 0 ]], ]) indices . shape . as_list () # batch_shape == [2, 5], index_depth == 2 [ 2 , 5 , 2 ] Here the indices do not have a shape of [num_updates, index_depth] , but a
shape of batch_shape+[index_depth] . Since the index_depth is equal to the rank of tensor : outer_shape is (5,5) inner_shape is () - each update is scalar updates.shape is batch_shape + inner_shape == (5,2) + () updates = [ [ 1 , 1 , 1 , 1 , 1 ], [ 1 , 1 , 1 , 1 , 1 ], ] Putting this together gives: tf . tensor_scatter_nd_update ( tensor , indices , updates ) . numpy () array ([[ 1. , 0. , 0. , 0. , 1. ], [ 0. , 1. , 0. , 1. , 0. ], [ 0. , 0. , 1. , 0. , 0. ], [ 0. , 1. , 0. , 1. , 0. ], [ 1. , 0. , 0. , 0. , 1. ]], dtype = float32 ) Args tensor Tensor to copy/update. indices Indices to update. updates Updates to apply at the indices. name Optional name for the operation. Returns A new tensor with the given shape and updates applied according to the
indices.


Page: https://www.tensorflow.org/api_docs/python/tf/tensordot
View source on GitHub Tensor contraction of a and b along specified axes and outer product. View aliases Main aliases tf.linalg.tensordot Compat aliases for migration See Migration guide for
more details. tf.compat.v1.linalg.tensordot , tf.compat.v1.tensordot tf . tensordot ( a , b , axes , name = None ) Tensordot (also known as tensor contraction) sums the product of elements
from a and b over the indices specified by axes . This operation corresponds to numpy.tensordot(a, b, axes) . Example 1: When a and b are matrices (order 2), the case axes=1 is equivalent to matrix multiplication. Example 2: When a and b are matrices (order 2), the case axes = [[1], [0]] is equivalent to matrix multiplication. Example 3: When a and b are matrices (order 2), the case axes=0 gives
the outer product, a tensor of order 4. Example 4: Suppose that \(a_{ijk}\) and \(b_{lmn}\) represent two
tensors of order 3. Then, contract(a, b, [[0], [2]]) is the order 4 tensor
\(c_{jklm}\) whose entry
corresponding to the indices \((j,k,l,m)\) is given by: \( c_{jklm} = \sum_i a_{ijk} b_{lmi} \). In general, order(c) = order(a) + order(b) - 2*len(axes[0]) . Args a Tensor of type float32 or float64 . b Tensor with the same type as a . axes Either a scalar N , or a list or an int32 Tensor of shape [2, k].
If axes is a scalar, sum over the last N axes of a and the first N axes of
b in order. If axes is a list or Tensor the first and second row contain
the set of unique integers specifying axes along which the contraction is
computed, for a and b , respectively. The number of axes for a and b must be equal. If axes=0 , computes the outer product between a and b . name A name for the operation (optional). Returns A Tensor with the same type as a . Raises ValueError If the shapes of a , b , and axes are incompatible. IndexError If the values in axes exceed the rank of the corresponding
tensor.


Page: https://www.tensorflow.org/api_docs/python/tf/tile
Constructs a tensor by tiling a given tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.manip.tile , tf.compat.v1.tile tf . tile ( input : Annotated [ Any , TV_Tile_T ], multiples : Annotated [ Any , TV_Tile_Tmultiples ], name = None ) -> Annotated [ Any , TV_Tile_T ] Used in the notebooks Used in the guide Used in the tutorials Better performance with the tf.data API Ragged tensors Understanding masking & padding Time series forecasting Calculate gradients Quantum data Parametrized Quantum Circuits for Reinforcement Learning A Tutorial on Multi-Armed Bandits with Per-Arm Features This operation creates a new tensor by replicating input multiples times.
The output tensor's i'th dimension has input.dims(i) * multiples[i] elements,
and the values of input are replicated multiples[i] times along the 'i'th
dimension. For example, tiling [a b c d] by [2] produces [a b c d a b c d] . a = tf . constant ([[ 1 , 2 , 3 ],[ 4 , 5 , 6 ]], tf . int32 ) b = tf . constant ([ 1 , 2 ], tf . int32 ) tf . tile ( a , b ) < tf . Tensor : shape = ( 2 , 6 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 , 1 , 2 , 3 ], [ 4 , 5 , 6 , 4 , 5 , 6 ]], dtype = int32 ) > c = tf . constant ([ 2 , 1 ], tf . int32 ) tf . tile ( a , c ) < tf . Tensor : shape = ( 4 , 3 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], dtype = int32 ) > d = tf . constant ([ 2 , 2 ], tf . int32 ) tf . tile ( a , d ) < tf . Tensor : shape = ( 4 , 6 ), dtype = int32 , numpy = array ([[ 1 , 2 , 3 , 1 , 2 , 3 ], [ 4 , 5 , 6 , 4 , 5 , 6 ], [ 1 , 2 , 3 , 1 , 2 , 3 ], [ 4 , 5 , 6 , 4 , 5 , 6 ]], dtype = int32 ) > Args input A Tensor . Can be of any rank. multiples A Tensor . Must be one of the following types: int32 , int64 .
1-D. Length must be the same as the number of dimensions in input name A name for the operation (optional). Returns A Tensor . Has the same type as input .


Page: https://www.tensorflow.org/api_docs/python/tf/timestamp
Provides the time since epoch in seconds. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.timestamp tf . timestamp ( name = None ) -> Annotated [ Any , _atypes . Float64 ] Used in the notebooks Used in the tutorials Random noise generation in TFF Returns the timestamp as a float64 for seconds since the Unix epoch. Common usages include: Logging Providing a random number seed Debugging graph execution Generating timing information, mainly through comparison of timestamps Note: In graph mode, the timestamp is computed when the op is executed,
not when it is added to the graph.  In eager mode, the timestamp is computed
when the op is eagerly executed. Args name A name for the operation (optional). Returns A Tensor of type float64 .


Page: https://www.tensorflow.org/api_docs/python/tf/transpose
View source on GitHub Transposes a , where a is a Tensor. tf . transpose ( a , perm = None , conjugate = False , name = 'transpose' ) Used in the notebooks Used in the guide Used in the tutorials Matrix approximation with Core APIs Effective Tensorflow 2 Better performance with tf.function Advanced automatic differentiation TensorFlow basics Scalable model compression Time series forecasting Client-efficient large-model federated learning via `federated_select` and sparse aggregation Quantum data Neural machine translation with a Transformer and Keras Permutes the dimensions according to the value of perm . The returned tensor's dimension i will correspond to the input dimension perm[i] . If perm is not given, it is set to (n-1...0), where n is the rank
of the input tensor. Hence, by default, this operation performs a regular
matrix transpose on 2-D input Tensors. If conjugate is True and a.dtype is either complex64 or complex128 then the values of a are conjugated and transposed. For example: x = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) tf . transpose ( x ) < tf . Tensor : shape = ( 3 , 2 ), dtype = int32 , numpy = array ([[ 1 , 4 ], [ 2 , 5 ], [ 3 , 6 ]], dtype = int32 ) > Equivalently, you could call tf.transpose(x, perm=[1, 0]) . If x is complex, setting conjugate=True gives the conjugate transpose: x = tf . constant ([[ 1 + 1 j , 2 + 2 j , 3 + 3 j ], [ 4 + 4 j , 5 + 5 j , 6 + 6 j ]]) tf . transpose ( x , conjugate = True ) < tf . Tensor : shape = ( 3 , 2 ), dtype = complex128 , numpy = array ([[ 1. - 1. j , 4. - 4. j ], [ 2. - 2. j , 5. - 5. j ], [ 3. - 3. j , 6. - 6. j ]]) > 'perm' is more useful for n-dimensional tensors where n > 2: x = tf . constant ([[[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]], [[ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]]) As above, simply calling tf.transpose will default to perm=[2,1,0] . To take the transpose of the matrices in dimension-0 (such as when you are
transposing matrices where 0 is the batch dimension), you would set perm=[0,2,1] . tf . transpose ( x , perm = [ 0 , 2 , 1 ]) < tf . Tensor : shape = ( 2 , 3 , 2 ), dtype = int32 , numpy = array ([[[ 1 , 4 ], [ 2 , 5 ], [ 3 , 6 ]], [[ 7 , 10 ], [ 8 , 11 ], [ 9 , 12 ]]], dtype = int32 ) > Note: This has a shorthand linalg.matrix_transpose ): Args a A Tensor . perm A permutation of the dimensions of a .  This should be a vector. conjugate Optional bool. Setting it to True is mathematically equivalent
to tf.math.conj(tf.transpose(input)). name A name for the operation (optional). Returns A transposed Tensor . numpy compatibility In numpy transposes are memory-efficient constant time operations as they
simply return a new view of the same data with adjusted strides . TensorFlow does not support strides, so transpose returns a new tensor with
the items permuted.


Page: https://www.tensorflow.org/api_docs/python/tf/math/truediv
View source on GitHub Divides x / y elementwise (using Python 3 division operator semantics). View aliases Main aliases tf.truediv Compat aliases for migration See Migration guide for
more details. tf.compat.v1.math.truediv , tf.compat.v1.truediv tf . math . truediv ( x , y , name = None ) Note: Prefer using the Tensor operator or tf.divide which obey Python
division operator semantics. This function forces Python 3 division operator semantics where all integer
arguments are cast to floating types first.   This op is generated by normal x / y division in Python 3 and in Python 2.7 with from __future__ import division .  If you want integer division that rounds
down, use x // y or tf.math.floordiv . x and y must have the same numeric type.  If the inputs are floating
point, the output will have the same type.  If the inputs are integral, the
inputs are cast to float32 for int8 and int16 and float64 for int32 and int64 (matching the behavior of Numpy). Args x Tensor numerator of numeric type. y Tensor denominator of numeric type. name A name for the operation (optional). Returns x / y evaluated in floating point. Raises TypeError If x and y have different dtypes.


Page: https://www.tensorflow.org/api_docs/python/tf/truncatediv
Returns x / y element-wise, rounded towards zero. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.truncatediv tf . truncatediv ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] Truncation designates that negative numbers will round fractional quantities
toward zero. I.e. -7 / 5 = -1. This matches C semantics but it is different
than Python semantics. See FloorDiv for a division function that matches
Python Semantics. Note: truncatediv supports broadcasting. More about broadcasting here Args x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , uint8 , int8 , uint16 , int16 , int32 , uint32 , uint64 , int64 , complex64 , complex128 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/truncatemod
Returns element-wise remainder of division. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.truncatemod tf . truncatemod ( x : Annotated [ Any , tf . raw_ops . Any ], y : Annotated [ Any , tf . raw_ops . Any ], name = None ) -> Annotated [ Any , tf . raw_ops . Any ] This emulates C semantics in that the result here is consistent with a truncating divide. E.g. truncate(x / y) *
y + truncate_mod(x, y) = x . Note: truncatemod supports broadcasting. More about broadcasting here Args x A Tensor . Must be one of the following types: int32 , int64 , bfloat16 , half , float32 , float64 . y A Tensor . Must have the same type as x . name A name for the operation (optional). Returns A Tensor . Has the same type as x .


Page: https://www.tensorflow.org/api_docs/python/tf/tuple
View source on GitHub Groups tensors together. tf . tuple ( tensors , control_inputs = None , name = None ) The returned tensors have the same value as the input tensors, but they
are computed only after all the input tensors have been computed. Note: In TensorFlow 2 with eager and/or Autograph, you should not require
this method, as ops execute in the expected order thanks to automatic control
dependencies. Only use tf.tuple when working with v1 tf.Graph code. See also tf.group and tf.control_dependencies . Example: with tf . Graph () . as_default (): with tf . compat . v1 . Session () as sess : v = tf . Variable ( 0.0 ) a = tf . constant ( 1.0 ) sess . run ( tf . compat . v1 . global_variables_initializer ()) for i in range ( 5 ): update_op = v . assign_add ( 1.0 ) b = a + v res_b = sess . run ( b ) res_v = sess . run ( v ) print ( res_v ) 0.0 0.0 0.0 0.0 0.0 with tf . Graph () . as_default (): with tf . compat . v1 . Session () as sess : v = tf . Variable ( 0.0 ) a = tf . constant ( 1.0 ) sess . run ( tf . compat . v1 . global_variables_initializer ()) for i in range ( 5 ): update_op = v . assign_add ( 1.0 ) calc = [ a + v ] # `tf.tuple` ensures `update_op` is run before `b` b = tf . tuple ( calc , [ tf . group ( update_op )]) res_b = sess . run ( b ) res_v = sess . run ( v ) print ( res_v ) 1.0 2.0 3.0 4.0 5.0 Args tensors A list of Tensor s or IndexedSlices , some entries can be None . control_inputs List of additional ops to finish before returning. name (optional) A name to use as a name_scope for the operation. Returns Same as tensors . Raises ValueError If tensors does not contain any Tensor or IndexedSlices . TypeError If control_inputs is not a list of Operation or Tensor objects.


Page: https://www.tensorflow.org/api_docs/python/tf/type_spec_from_value
View source on GitHub Returns a tf.TypeSpec that represents the given value . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.type_spec_from_value tf . type_spec_from_value ( value ) -> tf . TypeSpec Used in the notebooks Used in the guide Extension types Examples >>> tf . type_spec_from_value ( tf . constant ([ 1 , 2 , 3 ])) TensorSpec ( shape = ( 3 ,), dtype = tf . int32 , name = None ) >>> tf . type_spec_from_value ( np . array ([ 4.0 , 5.0 ], np . float64 )) TensorSpec ( shape = ( 2 ,), dtype = tf . float64 , name = None ) >>> tf . type_spec_from_value ( tf . ragged . constant ([[ 1 , 2 ], [ 3 , 4 , 5 ]])) RaggedTensorSpec ( TensorShape ([ 2 , None ]), tf . int32 , 1 , tf . int64 ) example_input = tf . ragged . constant ([[ 1 , 2 ], [ 3 ]]) @tf . function ( input_signature = [ tf . type_spec_from_value ( example_input )]) def f ( x ): return tf . reduce_sum ( x , axis = 1 ) Args value A value that can be accepted or returned by TensorFlow APIs. Accepted
types for value include tf.Tensor , any value that can be converted to tf.Tensor using tf.convert_to_tensor , and any subclass of CompositeTensor (such as tf.RaggedTensor ). Returns A TypeSpec that is compatible with value . Raises TypeError If a TypeSpec cannot be built for value , because its type
is not supported.


Page: https://www.tensorflow.org/api_docs/python/tf/unique
View source on GitHub Finds unique elements in a 1-D tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.unique tf . unique ( x , out_idx = tf . dtypes . int32 , name = None ) Used in the notebooks Used in the tutorials MoViNet for streaming action recognition Client-efficient large-model federated learning via `federated_select` and sparse aggregation This operation returns a tensor y containing all of the unique elements of x sorted in the same order that they occur in x ; x does not need to be sorted.
This operation also returns a tensor idx the same size as x that contains
the index of each value of x in the unique output y . In other words: y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1] Examples: # tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8] y , idx = unique ( x ) y == > [ 1 , 2 , 4 , 7 , 8 ] idx == > [ 0 , 0 , 1 , 2 , 2 , 2 , 3 , 4 , 4 ] # tensor 'x' is [4, 5, 1, 2, 3, 3, 4, 5] y , idx = unique ( x ) y == > [ 4 , 5 , 1 , 2 , 3 ] idx == > [ 0 , 1 , 2 , 3 , 4 , 4 , 0 , 1 ] Args x A Tensor . 1-D. out_idx An optional tf.DType from: tf.int32, tf.int64 . Defaults to tf.int32 . name A name for the operation (optional). Returns A tuple of Tensor objects (y, idx). y A Tensor . Has the same type as x . idx A Tensor of type out_idx .


Page: https://www.tensorflow.org/api_docs/python/tf/unique_with_counts
View source on GitHub Finds unique elements in a 1-D tensor. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.unique_with_counts tf . unique_with_counts ( x , out_idx = tf . dtypes . int32 , name = None ) This operation returns a tensor y containing all of the unique elements of x sorted in the same order that they occur in x . This operation also returns a
tensor idx the same size as x that contains the index of each value of x in the unique output y . Finally, it returns a third tensor count that
contains the count of each element of y in x . In other words: y[idx[i]] = x[i] for i in [0, 1,...,rank(x) - 1] For example: # tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8] y , idx , count = unique_with_counts ( x ) y == > [ 1 , 2 , 4 , 7 , 8 ] idx == > [ 0 , 0 , 1 , 2 , 2 , 2 , 3 , 4 , 4 ] count == > [ 2 , 1 , 3 , 1 , 2 ] Args x A Tensor . 1-D. out_idx An optional tf.DType from: tf.int32, tf.int64 . Defaults to tf.int32 . name A name for the operation (optional). Returns A tuple of Tensor objects (y, idx, count). y A Tensor . Has the same type as x . idx A Tensor of type out_idx . count A Tensor of type out_idx .


Page: https://www.tensorflow.org/api_docs/python/tf/unravel_index
Converts an array of flat indices into a tuple of coordinate arrays. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.unravel_index tf . unravel_index ( indices : Annotated [ Any , TV_UnravelIndex_Tidx ], dims : Annotated [ Any , TV_UnravelIndex_Tidx ], name = None ) -> Annotated [ Any , TV_UnravelIndex_Tidx ] Example: y = tf . unravel_index ( indices = [ 2 , 5 , 7 ], dims = [ 3 , 3 ]) # 'dims' represent a hypothetical (3, 3) tensor of indices: # [[0, 1, *2*], #  [3, 4, *5*], #  [6, *7*, 8]] # For each entry from 'indices', this operation returns # its coordinates (marked with '*'), such as # 2 ==> (0, 2) # 5 ==> (1, 2) # 7 ==> (2, 1) y == > [[ 0 , 1 , 2 ], [ 2 , 2 , 1 ]] Args indices A Tensor . Must be one of the following types: int32 , int64 .
An 0-D or 1-D int Tensor whose elements are indices into the
flattened version of an array of dimensions dims. dims A Tensor . Must have the same type as indices .
An 1-D int Tensor. The shape of the array to use for unraveling
indices. name A name for the operation (optional). Returns A Tensor . Has the same type as indices . numpy compatibility Equivalent to np.unravel_index


Page: https://www.tensorflow.org/api_docs/python/tf/unstack
View source on GitHub Unpacks the given dimension of a rank- R tensor into rank- (R-1) tensors. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.unstack tf . unstack ( value , num = None , axis = 0 , name = 'unstack' ) Used in the notebooks Used in the tutorials Scalable model compression Unpacks tensors from value by chipping it along the axis dimension. x = tf . reshape ( tf . range ( 12 ), ( 3 , 4 )) p , q , r = tf . unstack ( x ) p . shape . as_list () [ 4 ] i , j , k , l = tf . unstack ( x , axis = 1 ) i . shape . as_list () [ 3 ] This is the opposite of stack. x = tf . stack ([ i , j , k , l ], axis = 1 ) More generally if you have a tensor of shape (A, B, C, D) : A , B , C , D = [ 2 , 3 , 4 , 5 ] t = tf . random . normal ( shape = [ A , B , C , D ]) The number of tensor returned is equal to the length of the target axis : axis = 2 items = tf . unstack ( t , axis = axis ) len ( items ) == t . shape [ axis ] True The shape of each result tensor is equal to the shape of the input tensor,
with the target axis removed. items [ 0 ] . shape . as_list () # [A, B, D] [ 2 , 3 , 5 ] The value of each tensor items[i] is equal to the slice of input across axis at index i : for i in range ( len ( items )): slice = t [:,:, i ,:] assert tf . reduce_all ( slice == items [ i ]) Python iterable unpacking With eager execution you can unstack the 0th axis of a tensor using python's
iterable unpacking: t = tf . constant ([ 1 , 2 , 3 ]) a , b , c = t unstack is still necessary because Iterable unpacking doesn't work in
a @tf.function : Symbolic tensors are not iterable. You need to use tf.unstack here: @tf . function def bad ( t ): a , b , c = t return a bad ( t ) Traceback ( most recent call last ): OperatorNotAllowedInGraphError : ... @tf . function def good ( t ): a , b , c = tf . unstack ( t ) return a good ( t ) . numpy () 1 Unknown shapes Eager tensors have concrete values, so their shape is always known.
Inside a tf.function the symbolic tensors may have unknown shapes.
If the length of axis is unknown tf.unstack will fail because it cannot
handle an unknown number of tensors: @tf . function ( input_signature = [ tf . TensorSpec ([ None ], tf . float32 )]) def bad ( t ): tensors = tf . unstack ( t ) return tensors [ 0 ] bad ( tf . constant ([ 1.0 , 2.0 , 3.0 ])) Traceback ( most recent call last ): ValueError : Cannot infer argument ` num ` from shape ( None ,) If you know the axis length you can pass it as the num argument. But this
must be a constant value. If you actually need a variable number of tensors in a single tf.function trace, you will need to use exlicit loops and a tf.TensorArray instead. Args value A rank R > 0 Tensor to be unstacked. num An int . The length of the dimension axis . Automatically inferred if None (the default). axis An int . The axis to unstack along. Defaults to the first dimension.
Negative values wrap around, so the valid range is [-R, R) . name A name for the operation (optional). Returns The list of Tensor objects unstacked from value . Raises ValueError If axis is out of the range [-R, R) . ValueError If num is unspecified and cannot be inferred. InvalidArgumentError If num does not match the shape of value .


Page: https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope
View source on GitHub Scope which defines a variable creation function to be used by variable(). @tf_contextlib . contextmanager tf . variable_creator_scope ( variable_creator ) Used in the notebooks Used in the guide Validating correctness & numerical equivalence variable_creator is expected to be a function with the following signature: def variable_creator ( next_creator , ** kwargs ) The creator is supposed to eventually call the next_creator to create a
variable if it does want to create a variable and not call Variable or
ResourceVariable directly. This helps make creators composable. A creator may
choose to create multiple variables, return already existing variables, or
simply register that a variable was created and defer to the next creators in
line. Creators can also modify the keyword arguments seen by the next
creators. Custom getters in the variable scope will eventually resolve down to these
custom creators when they do create variables. The valid keyword arguments in kwds are: initial_value: A Tensor , or Python object convertible to a Tensor ,
  which is the initial value for the Variable. The initial value must have
  a shape specified unless validate_shape is set to False. Can also be a
  callable with no argument that returns the initial value when called. In
  that case, dtype must be specified. (Note that initializer functions
  from init_ops.py must first be bound to a shape before being used here.) trainable: If True , the default, GradientTapes automatically watch
  uses of this Variable. validate_shape: If False , allows the variable to be initialized with a
  value of unknown shape. If True , the default, the shape of initial_value must be known. caching_device: Optional device string describing where the Variable
  should be cached for reading.  Defaults to the Variable's device.
  If not None , caches on another device.  Typical use is to cache
  on the device where the Ops using the Variable reside, to deduplicate
  copying through Switch and other conditional statements. name: Optional name for the variable. Defaults to 'Variable' and gets
  uniquified automatically.
dtype: If set, initial_value will be converted to the given type.
  If None , either the datatype will be kept (if initial_value is
  a Tensor), or convert_to_tensor will decide. constraint: A constraint function to be applied to the variable after
  updates by some algorithms. synchronization: Indicates when a distributed a variable will be
  aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses
  when to synchronize. aggregation: Indicates how a distributed variable will be aggregated.
  Accepted values are constants defined in the class tf.VariableAggregation . This set may grow over time, so it's important the signature of creators is as
mentioned above. Args variable_creator the passed creator Yields A scope in which the creator is active


Page: https://www.tensorflow.org/api_docs/python/tf/vectorized_map
View source on GitHub Parallel map on the list of tensors unpacked from elems on dimension 0. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.vectorized_map tf . vectorized_map ( fn , elems , fallback_to_while_loop = True , warn = True ) Used in the notebooks Used in the guide NumPy API on TensorFlow This method works similar to tf.map_fn but is optimized to run much faster,
possibly with a much larger memory footprint. The speedups are obtained by
vectorization (see Auto-Vectorizing TensorFlow Graphs: Jacobians,
Auto-Batching and Beyond ). The idea
behind vectorization is to semantically launch all the invocations of fn in
parallel and fuse corresponding operations across all these invocations. This
fusion is done statically at graph generation time and the generated code is
often similar in performance to a manually fused version. Because tf.vectorized_map fully parallelizes the batch, this method will
generally be significantly faster than using tf.map_fn , especially in eager
mode. However this is an experimental feature and currently has a lot of
limitations: There should be no data dependency between the different semantic
invocations of fn , i.e. it should be safe to map the elements of the
inputs in any order. Stateful kernels may mostly not be supported since these often imply a
data dependency. We do support a limited set of such stateful kernels
though (like RandomFoo, Variable operations like reads, etc). fn has limited support for control flow operations. fn should return nested structure of Tensors or Operations. However
if an Operation is returned, it should have zero outputs. The shape and dtype of any intermediate or output tensors in the
computation of fn should not depend on the input to fn . Examples: def outer_product ( a ): return tf . tensordot ( a , a , 0 ) batch_size = 100 a = tf . ones (( batch_size , 32 , 32 )) c = tf . vectorized_map ( outer_product , a ) assert c . shape == ( batch_size , 32 , 32 , 32 , 32 ) # Computing per-example gradients batch_size = 10 num_features = 32 layer = tf . keras . layers . Dense ( 1 ) def model_fn ( arg ): with tf . GradientTape () as g : inp , label = arg inp = tf . expand_dims ( inp , 0 ) label = tf . expand_dims ( label , 0 ) prediction = layer ( inp ) loss = tf . nn . l2_loss ( label - prediction ) return g . gradient ( loss , ( layer . kernel , layer . bias )) inputs = tf . random . uniform ([ batch_size , num_features ]) labels = tf . random . uniform ([ batch_size , 1 ]) per_example_gradients = tf . vectorized_map ( model_fn , ( inputs , labels )) assert per_example_gradients [ 0 ] . shape == ( batch_size , num_features , 1 ) assert per_example_gradients [ 1 ] . shape == ( batch_size , 1 ) Args fn The callable to be performed. It accepts one argument, which will have
the same (possibly nested) structure as elems , and returns a possibly
nested structure of Tensors and Operations, which may be different than
the structure of elems . elems A tensor or (possibly nested) sequence of tensors, each of which will
be unpacked along their first dimension. The nested sequence of the
resulting slices will be mapped over by fn . The first dimensions of all
elements must broadcast to a consistent value; equivalently, each
element tensor must have first dimension of either B or 1 , for some
common batch size B >= 1 . fallback_to_while_loop If true, on failing to vectorize an operation,
the unsupported op is wrapped in a tf.while_loop to execute the map
iterations. Note that this fallback only happens for unsupported ops and
other parts of fn are still vectorized. If false, on encountering an
unsupported op, a ValueError is thrown. Note that the fallbacks can result
in slowdowns since vectorization often yields speedup of one to two orders
of magnitude. warn If set to false , this will supress any warnings due to operation
conversions in the provided fn falling back to while loops. Returns A tensor or (possibly nested) sequence of tensors. Each tensor packs the
results of applying fn to tensors unpacked from elems along the first
dimension, from first to last. Although they are less common as user-visible inputs and outputs, note that
tensors of type tf.variant which represent tensor lists (for example from tf.raw_ops.TensorListFromTensor ) are vectorized by stacking the list
contents rather than the variant itself, and so the container tensor will
have a scalar shape when returned rather than the usual stacked shape. This
improves the performance of control flow gradient vectorization. Raises ValueError If vectorization fails and fallback_to_while_loop is False.


Page: https://www.tensorflow.org/api_docs/python/tf/where
View source on GitHub Returns the indices of non-zero elements, or multiplexes x and y . View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.where_v2 tf . where ( condition , x = None , y = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Extension types Better performance with tf.function Unicode strings Transfer learning and fine-tuning Integrated gradients Substantial Undocumented Infection Facilitates the Rapid Dissemination of Novel Coronavirus (SARS-CoV2) TFP Release Notes notebook (0.11.0) Bayesian Switchpoint Analysis This operation has two modes: Return the indices of non-zero elements - When only condition is provided the result is an int64 tensor where each row is
the index of a non-zero element of condition . The result's shape
is [tf.math.count_nonzero(condition), tf.rank(condition)] . Multiplex x and y - When both x and y are provided the
result has the shape of x , y , and condition broadcast together. The
result is taken from x where condition is non-zero
or y where condition is zero. 1. Return the indices of non-zero elements Note: In this mode condition can have a dtype of bool or any numeric
dtype. If x and y are not provided (both are None): tf.where will return the indices of condition that are non-zero,
in the form of a 2-D tensor with shape [n, d] , where n is the number of
non-zero elements in condition ( tf.count_nonzero(condition) ), and d is
the number of axes of condition ( tf.rank(condition) ). Indices are output in row-major order. The condition can have a dtype of tf.bool , or any numeric dtype . Here condition is a 1-axis bool tensor with 2 True values. The result
has a shape of [2,1] tf . where ([ True , False , False , True ]) . numpy () array ([[ 0 ], [ 3 ]]) Here condition is a 2-axis integer tensor, with 3 non-zero values. The
result has a shape of [3, 2] . tf . where ([[ 1 , 0 , 0 ], [ 1 , 0 , 1 ]]) . numpy () array ([[ 0 , 0 ], [ 1 , 0 ], [ 1 , 2 ]]) Here condition is a 3-axis float tensor, with 5 non-zero values. The output
shape is [5, 3] . float_tensor = [[[ 0.1 , 0 ], [ 0 , 2.2 ], [ 3.5 , 1e6 ]], [[ 0 , 0 ], [ 0 , 0 ], [ 99 , 0 ]]] tf . where ( float_tensor ) . numpy () array ([[ 0 , 0 , 0 ], [ 0 , 1 , 1 ], [ 0 , 2 , 0 ], [ 0 , 2 , 1 ], [ 1 , 2 , 0 ]]) These indices are the same that tf.sparse.SparseTensor would use to
represent the condition tensor: sparse = tf . sparse . from_dense ( float_tensor ) sparse . indices . numpy () array ([[ 0 , 0 , 0 ], [ 0 , 1 , 1 ], [ 0 , 2 , 0 ], [ 0 , 2 , 1 ], [ 1 , 2 , 0 ]]) A complex number is considered non-zero if either the real or imaginary
component is non-zero: tf . where ([ complex ( 0. ), complex ( 1. ), 0 + 1 j , 1 + 1 j ]) . numpy () array ([[ 1 ], [ 2 ], [ 3 ]]) 2. Multiplex x and y Note: In this mode condition must have a dtype of bool . If x and y are also provided (both have non-None values) the condition tensor acts as a mask that chooses whether the corresponding
element / row in the output should be taken from x (if the element in condition is True ) or y (if it is False ). The shape of the result is formed by broadcasting together the shapes of condition , x , and y . When all three inputs have the same size, each is handled element-wise. tf . where ([ True , False , False , True ], [ 1 , 2 , 3 , 4 ], [ 100 , 200 , 300 , 400 ]) . numpy () array ([ 1 , 200 , 300 , 4 ], dtype = int32 ) There are two main rules for broadcasting: If a tensor has fewer axes than the others, length-1 axes are added to the
left of the shape. Axes with length-1 are streched to match the coresponding axes of the other
tensors. A length-1 vector is streched to match the other vectors: tf . where ([ True , False , False , True ], [ 1 , 2 , 3 , 4 ], [ 100 ]) . numpy () array ([ 1 , 100 , 100 , 4 ], dtype = int32 ) A scalar is expanded to match the other arguments: tf . where ([[ True , False ], [ False , True ]], [[ 1 , 2 ], [ 3 , 4 ]], 100 ) . numpy () array ([[ 1 , 100 ], [ 100 , 4 ]], dtype = int32 ) tf . where ([[ True , False ], [ False , True ]], 1 , 100 ) . numpy () array ([[ 1 , 100 ], [ 100 , 1 ]], dtype = int32 ) A scalar condition returns the complete x or y tensor, with
broadcasting applied. tf . where ( True , [ 1 , 2 , 3 , 4 ], 100 ) . numpy () array ([ 1 , 2 , 3 , 4 ], dtype = int32 ) tf . where ( False , [ 1 , 2 , 3 , 4 ], 100 ) . numpy () array ([ 100 , 100 , 100 , 100 ], dtype = int32 ) For a non-trivial example of broadcasting, here condition has a shape of [3] , x has a shape of [3,3] , and y has a shape of [3,1] .
Broadcasting first expands the shape of condition to [1,3] . The final
broadcast shape is [3,3] . condition will select columns from x and y .
Since y only has one column, all columns from y will be identical. tf . where ([ True , False , True ], x = [[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]], y = [[ 100 ], [ 200 ], [ 300 ]] ) . numpy () array ([[ 1 , 100 , 3 ], [ 4 , 200 , 6 ], [ 7 , 300 , 9 ]], dtype = int32 ) Note that if the gradient of either branch of the tf.where generates
a NaN , then the gradient of the entire tf.where will be NaN . This is
because the gradient calculation for tf.where combines the two branches, for
performance reasons. A workaround is to use an inner tf.where to ensure the function has
no asymptote, and to avoid computing a value whose gradient is NaN by
replacing dangerous inputs with safe inputs. Instead of this, x = tf . constant ( 0. , dtype = tf . float32 ) with tf . GradientTape () as tape : tape . watch ( x ) y = tf . where ( x < 1. , 0. , 1. / x ) print ( tape . gradient ( y , x )) tf . Tensor ( nan , shape = (), dtype = float32 ) Although, the 1. / x values are never used, its gradient is a NaN when x = 0 . Instead, we should guard that with another tf.where x = tf . constant ( 0. , dtype = tf . float32 ) with tf . GradientTape () as tape : tape . watch ( x ) safe_x = tf . where ( tf . equal ( x , 0. ), 1. , x ) y = tf . where ( x < 1. , 0. , 1. / safe_x ) print ( tape . gradient ( y , x )) tf . Tensor ( 0.0 , shape = (), dtype = float32 ) See also: tf.sparse - The indices returned by the first form of tf.where can be
useful in tf.sparse.SparseTensor objects. tf.gather_nd , tf.scatter_nd , and related ops - Given the
list of indices returned from tf.where the scatter and gather family
of ops can be used fetch values or insert values at those indices. tf.strings.length - tf.string is not an allowed dtype for the condition . Use the string length instead. Args condition A tf.Tensor of dtype bool, or any numeric dtype. condition must have dtype bool when x and y are provided. x If provided, a Tensor which is of the same type as y , and has a shape
broadcastable with condition and y . y If provided, a Tensor which is of the same type as x , and has a shape
broadcastable with condition and x . name A name of the operation (optional). Returns If x and y are provided:
  A Tensor with the same type as x and y , and shape that
  is broadcast from condition , x , and y .
Otherwise, a Tensor with shape [tf.math.count_nonzero(condition),
tf.rank(condition)] . Raises ValueError When exactly one of x or y is non-None, or the shapes
are not all broadcastable.


Page: https://www.tensorflow.org/api_docs/python/tf/while_loop
View source on GitHub Repeat body while the condition cond is true. (deprecated argument values) tf . while_loop ( cond , body , loop_vars , shape_invariants = None , parallel_iterations = 10 , back_prop = True , swap_memory = False , maximum_iterations = None , name = None ) Used in the notebooks Used in the guide Used in the tutorials Extension types Modeling COVID-19 spread in Europe and the effect of interventions Linear Mixed Effects Models Deprecated: SOME ARGUMENT VALUES ARE DEPRECATED: (back_prop=False) . They will be removed in a future version.
Instructions for updating:
back_prop=False is deprecated. Consider using tf.stop_gradient instead.
Instead of:
results = tf.while_loop(c, b, vars, back_prop=False)
Use:
results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars)) Note: This op is automatically used in a tf.function to convert Python for-
and while- loops when the loop variable is a tf.Tensor , unless autograph=False is explicitly specified in tf.function args. For example,
the following are equivalent: @tf . function def sumSquare ( n ): i , result = tf . constant ( 0 ), tf . constant ( 0 ) while i < n : # AutoGraph converts while-loop to tf.while_loop(). result += i * i i += 1 return result sumSquare ( 10 ) . numpy () 285 @tf . function def sumSquare2 ( n ): i , result = tf . constant ( 0 ), tf . constant ( 0 ) c = lambda i , _ : tf . less ( i , n ) b = lambda i , result : ( i + 1 , result + i * i ) return tf . while_loop ( c , b , [ i , result ])[ 1 ] sumSquare2 ( 10 ) . numpy () 285 For more information, see tf.function and AutoGraph guide . cond is a callable returning a boolean scalar tensor. body is a callable
returning a (possibly nested) tuple, namedtuple or list of tensors of the same
arity (length and structure) and types as loop_vars . loop_vars is a
(possibly nested) tuple, namedtuple or list of tensors that is passed to both cond and body . cond and body both take as many arguments as there are loop_vars . In addition to regular Tensors or IndexedSlices, the body may accept and
return TensorArray objects.  The flows of the TensorArray objects will
be appropriately forwarded between loops and during gradient calculations. Note that while_loop calls cond and body exactly once (inside the
call to while_loop , and not at all during Session.run() ). while_loop stitches together the graph fragments created during the cond and body calls with some additional graph nodes to create the graph flow that
repeats body until cond returns false. For correctness, tf.while_loop() strictly enforces shape invariants for
the loop variables. A shape invariant is a (possibly partial) shape that
is unchanged across the iterations of the loop. An error will be raised
if the shape of a loop variable after an iteration is determined to be more
general than or incompatible with its shape invariant. For example, a shape
of [11, None] is more general than a shape of [11, 17] , and [11, 21] is
not compatible with [11, 17] . By default (if the argument shape_invariants is not specified), it is assumed that the initial shape of each tensor in loop_vars is the same in every iteration. The shape_invariants argument
allows the caller to specify a less specific shape invariant for each loop
variable, which is needed if the shape varies between iterations. The tf.Tensor.set_shape function may also be used in the body function to indicate that
the output loop variable has a particular shape. The shape invariant for
SparseTensor and IndexedSlices are treated specially as follows: a) If a loop variable is a SparseTensor, the shape invariant must be TensorShape([r]) where r is the rank of the dense tensor represented
by the sparse tensor. It means the shapes of the three tensors of the
SparseTensor are ([None], [None, r], [r]) . NOTE: The shape invariant here
is the shape of the SparseTensor.dense_shape property. It must be the shape of
a vector. b) If a loop variable is an IndexedSlices, the shape invariant must be
a shape invariant of the values tensor of the IndexedSlices. It means
the shapes of the three tensors of the IndexedSlices are (shape, [shape[0]],
[shape.ndims]) . while_loop implements non-strict semantics, enabling multiple iterations
to run in parallel. The maximum number of parallel iterations can be
controlled by parallel_iterations , which gives users some control over
memory consumption and execution order. For correct programs, while_loop should return the same result for any parallel_iterations > 0 . For training, TensorFlow stores the tensors that are produced in the
forward inference and are needed in back propagation. These tensors are a
main source of memory consumption and often cause OOM errors when training
on GPUs. When the flag swap_memory is true, we swap out these tensors from
GPU to CPU. This for example allows us to train RNN models with very long
sequences and large batches. Args cond A callable that represents the termination condition of the loop. body A callable that represents the loop body. loop_vars A (possibly nested) tuple, namedtuple or list of numpy array, Tensor , and TensorArray objects. shape_invariants The shape invariants for the loop variables. parallel_iterations The number of iterations allowed to run in parallel. It
must be a positive integer. back_prop (optional) Deprecated. False disables support for back
propagation. Prefer using tf.stop_gradient instead. swap_memory Whether GPU-CPU memory swap is enabled for this loop. maximum_iterations Optional maximum number of iterations of the while loop
to run.  If provided, the cond output is AND-ed with an additional
condition ensuring the number of iterations executed is no greater than maximum_iterations . name Optional name prefix for the returned tensors. Returns The output tensors for the loop variables after the loop. The return value
has the same structure as loop_vars . Raises TypeError if cond or body is not callable. ValueError if loop_vars is empty. Example: i = tf . constant ( 0 ) c = lambda i : tf . less ( i , 10 ) b = lambda i : ( tf . add ( i , 1 ), ) r = tf . while_loop ( c , b , [ i ])[ 0 ] r . numpy () 10 Example with nesting and a namedtuple: import collections Pair = collections . namedtuple ( 'Pair' , 'j, k' ) ijk_0 = ( tf . constant ( 0 ), Pair ( tf . constant ( 1 ), tf . constant ( 2 ))) c = lambda i , p : i < 10 b = lambda i , p : ( i + 1 , Pair (( p . j + p . k ), ( p . j - p . k ))) ijk_final = tf . while_loop ( c , b , ijk_0 )[ 1 ] ijk_final [ 0 ] . numpy (), ijk_final [ 1 ] . numpy () ( 32 , 64 ) Example using shape_invariants: i0 = tf . constant ( 0 ) m0 = tf . ones ([ 2 , 2 ]) c = lambda i , m : i < 10 b = lambda i , m : [ i + 1 , tf . concat ([ m , m ], axis = 0 )] tf . while_loop ( c , b , loop_vars = [ i0 , m0 ], shape_invariants = [ i0 . get_shape (), tf . TensorShape ([ None , 2 ])])[ 1 ] < tf . Tensor : shape = ( 2048 , 2 ), dtype = float32 , numpy =... > Example which demonstrates non-strict semantics: In the following
example, the final value of counter does not depend on x . So
the while_loop can increment the counter parallel to updates of x .
However, because the loop counter at one loop iteration depends
on the value at the previous iteration, the loop counter itself cannot
be incremented in parallel. Hence if we just want the final value of the
counter (which we print on the line print(sess.run(i)) ), then x will never be incremented, but the counter will be updated on a
single thread. Conversely, if we want the value of the output (which we
print on the line print(sess.run(out).shape) ), then the counter may be
incremented on its own thread, while x can be incremented in
parallel on a separate thread. In the extreme case, it is conceivable
that the thread incrementing the counter runs until completion before x is incremented even a single time. The only thing that can never
happen is that the thread updating x can never get ahead of the
counter thread because the thread incrementing x depends on the value
of the counter. with tf . compat . v1 . Session () as sess : n = 10 c = lambda i , x : i < n b = lambda i , x : ( tf . compat . v1 . Print ( i + 1 , [ i ], "Updating i based on i == " ), # Let x depend on i tf . compat . v1 . Print ( x + i , [ i ], "Updating x based on i == " )) # Make x to be a big matrix so its updating thread would run slowly x = tf . zeros ([ 1000 , 100 ], dtype = tf . int32 ) counter = tf . constant ( 0 ) counter_out , x_out = tf . while_loop ( c , b , ( counter , x )) # The following line may increment the counter and x in parallel. # The counter thread may get ahead of the x thread, but not the # other way around. For example, the log may contain these messages: # ...   # Updating i based on i == [9]
...   # Updating x based on i == [3]
...   # ...   # meaning that the counter(i) thread is on iteration 9,
...   # while the x thread is on iteration 3.
...   print(sess.run(x_out).shape)
(1000, 100)


Page: https://www.tensorflow.org/api_docs/python/tf/zeros
View source on GitHub Creates a tensor with all elements set to zero. View aliases Compat aliases for migration See Migration guide for
more details. tf.compat.v1.zeros tf . zeros ( shape , dtype = tf . dtypes . float32 , name = None , layout = None ) Used in the notebooks Used in the guide Used in the tutorials Introduction to modules, layers, and models Multilayer perceptrons for digit recognition with Core APIs Better performance with the tf.data API Training checkpoints Optimizers with Core APIs Custom layers Learned data compression Using DTensors with Keras Integrated gradients Multilevel Modeling Primer in TensorFlow Probability See also tf.zeros_like , tf.ones , tf.fill , tf.eye . This operation returns a tensor of type dtype with shape shape and
all elements set to zero. tf . zeros ([ 3 , 4 ], tf . int32 ) < tf . Tensor : shape = ( 3 , 4 ), dtype = int32 , numpy = array ([[ 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 ], [ 0 , 0 , 0 , 0 ]], dtype = int32 ) > Args shape A list of integers, a tuple of integers, or a 1-D Tensor of
type int32 . dtype The DType of an element in the resulting Tensor . name Optional string. A name for the operation. layout Optional, tf.experimental.dtensor.Layout . If provided, the result
is a DTensor with the
provided layout. Returns A Tensor with all elements set to zero.


Page: https://www.tensorflow.org/api_docs/python/tf/zeros_like
View source on GitHub Creates a tensor with all elements set to zero. tf . zeros_like ( input , dtype = None , name = None , layout = None ) Used in the notebooks Used in the guide Used in the tutorials Automatically rewrite TF 1.x and compat.v1 API symbols CycleGAN Deep Convolutional Generative Adversarial Network DeepDream pix2pix: Image-to-image translation with a conditional GAN Integrated gradients See also tf.zeros . Given a single tensor or array-like object ( input ), this operation returns
a tensor of the same type and shape as input with all elements set to zero.
Optionally, you can use dtype to specify a new type for the returned tensor. Note that the layout of the input tensor is not preserved if the op
is used inside tf.function. To obtain a tensor with the same layout as the
input, chain the returned value to a dtensor.relayout_like . Examples >>> tensor = tf . constant ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) >>> tf . zeros_like ( tensor ) < tf . Tensor : shape = ( 2 , 3 ), dtype = int32 , numpy = array ([[ 0 , 0 , 0 ], [ 0 , 0 , 0 ]], dtype = int32 ) > tf . zeros_like ( tensor , dtype = tf . float32 ) < tf . Tensor : shape = ( 2 , 3 ), dtype = float32 , numpy = array ([[ 0. , 0. , 0. ], [ 0. , 0. , 0. ]], dtype = float32 ) > tf . zeros_like ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) < tf . Tensor : shape = ( 2 , 3 ), dtype = int32 , numpy = array ([[ 0 , 0 , 0 ], [ 0 , 0 , 0 ]], dtype = int32 ) > Args input A Tensor or array-like object. dtype A type for the returned Tensor . Must be float16 , float32 , float64 , int8 , uint8 , int16 , uint16 , int32 , int64 , complex64 , complex128 , bool or string (optional). name A name for the operation (optional). layout Optional, tf.experimental.dtensor.Layout . If provided, the result
is a DTensor with the
provided layout. Returns A Tensor with all elements set to zero.
